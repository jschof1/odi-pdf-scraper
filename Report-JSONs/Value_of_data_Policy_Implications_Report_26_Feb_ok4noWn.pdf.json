{"numpages":49,"numrender":49,"info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"Microsoft Word - Value of data report - typeset.docx","Keywords":"","Creator":"Word","Producer":"Mac OS X 10.13.6 Quartz PDFContext","CreationDate":"D:20200219132943Z00'00'","ModDate":"D:20200219132943Z00'00'"},"metadata":null,"text":"\n\n \n \n  \n \n \n \n \n \n \n  \n \n \n \n   \n \n \n \n \n \n \n \n \n \n \n  \nMain report \nPOLICY IMPLICATIONS \n\n \nREPORT  \nThe Value of Data  \nPolicy Implications \n \nCONTENTS \nIntroduction 1 \nThe economic lens:  The distinctive economic characteristics of data 4 \nThe information lens: Subject, context and use 8 \nQuality 10 \nSensitivity and personal data 10 \nInteroperability and linkability 10 \nExcludability 11 \nAccessibility 11 \nThe current UK legal framework 14 \nIntellectual property rights and licensing 14 \nIntellectual property rights in public sector information 15 \nData protection rights 16 \nMarket-based methods of data valuation 17 \nExisting non-market estimates 21 \nCreating value through open and shared data 23 \nInstitutions for the data economy 27 \nData Infrastructure 29 \nData Trusts 31 \nOther data sharing models 32 \nPolicy issues and recommendations 37 \nNote on Methods 43 \nAdvisory group members 44 \nInterviewees 44 \n \n \n  \n\n \nPublished: February 2020 \n \nAUTHORS \n \nDiane Coyle                Lawrence Kay \nStephanie Diepeveen               Jeni Tennison \nJulia Wdowin \nBennett Institute, University of Cambridge              Open Data Institute \n \n \n \nPublication from the Bennett Institute for Public Policy, Cambridge in partnership with  \nthe Open Data Institute. bennettinstitute.cam.ac.uk / theodi.org \n \n \n\n \nThe Value of Data / policy implications - 1 \n \nIntroduction \n \nThere is a lively debate about the value of data, but the creation of value from data of different \nkinds,  its capture by  different  entities,  and  its distribution,  need  to  be  better  understood.  This \nmatters for effective policy as well as business opportunities, in order to ensure that society as a \nwhole  gains  from  the  data-fuelled  changes  in  the  economy.  Yet  at  present  there  is  too  little \ndistinction in the debate between different types and uses of data, and the private and public \nvalue these could create, even though the number of data transactions is growing significantly.  \nThis  report  proposes  an  approach  to  understanding  questions  of  value,  and  the  policy \nimplications, based on the economic characteristics of data. Its aim is to contribute to a shared \nunderstanding of the value of this newly pervasive intangible asset. By ‘value’, we are referring \nto  the  economic  concept  of  ‘social  welfare’:  the  broad  economic well-being  of  all  of  society, \nincluding the profitability of businesses, the incomes and needs of individuals, and non-monetary \nbenefits such as convenience or health. \nThis definition encompasses the value exchanges that are taking place involving public sector \norganisations  in  areas  such  as  transport  and  health,  with  commercial  deals  on  various  terms \ninvolving patient or passenger data. We set out some key issues and principles for data policy \nand regulation. The ultimate aim is to ensure that there is as much creation of value as possible \nfrom data (in terms of social welfare), shared widely in society. The focus here is therefore on \neconomic  value  broadly  understood  from  the  perspective  of  society,  not  solely  on  commercial \npotential. This lens highlights some potential policy trade-offs. \nPolicy  interest  in  data  has  two  dimensions.  First,  government  has  to  make  policy  decisions \ninvolving the value of data to the economy as a whole. These include decisions by government \nto invest in maintaining datasets, choices about the terms on which publicly-created data will be \nmade available, and decisions to regulate more broadly concerning data access. The UK Treasury \nrecently published a discussion paper pointing to the economic potential of data, but also the \nchallenges around unlocking that potential.\n1\n The European Commission’s Joint Research Centre \nnoted the large array of policy questions, concluding that there were no easy answers to them \n(Duch-Brown et al, 2017).\n2\n A greater understanding of the value of data would help identify where \nthe benefits of greater investment in and sharing of data are worth the costs. Given the public \ngood characteristics of data, it seems likely that there is considerable untapped value in enabling \ngreater provision, access to, and joining up of data sets. \nSecond,  even  where  it  is  recognised  that  greater  sharing  of  data  brings  benefits,  such  as  in \ntransport  or  health,  those  making  decisions  about  providing  data  they  have  created  need  to \nunderstand the economic transaction. Although it may prove difficult or impossible to establish \nspecific monetary valuations of certain data sets, a clearer sense of the social value is needed \n                                                   \n1\n HM Treasury, “The Economic Value of Data: A discussion paper,” (2018). \n2\n N. Duch-Brown, B. Martens and F. Mueller-Langer, “The economics of ownership, access and trade in \ndigital data,” European Commission JRC Digital Economy Working Paper 2017-01 (2017). \n\n \nThe Value of Data / policy implications - 2 \n \nurgently, as many transactions involving access to public sector data are in fact already occurring. \nCommercial firms are eager to gain access to data held by public bodies. Yet even though these \ncompanies  may  develop  useful  and  commercially  successful  new  services,  there  is  a  risk  that \nmuch  potential  value  will  be  concentrated  in  a  small  number  of  hands,  or  that  citizens  and \ntaxpayers  will  not  receive  a  fair  return  from  private  companies  using  publicly  provided  data. \nThese are pressing issues: following recommendations from the Hall & Pesenti AI Review,\n3\n the \nOffice  for  AI  has  been  investigating  implementation  of  data trusts,  which  will  require  an \nunderstanding  of  how  to  distribute  value  from  users  to  contributors.  The  question  of  market \npower  based  on  data  aggregation  was  one  of  the  considerations  for  the  Furman  Review  of \ncompetition  in  digital  markets,  which  concluded  opportunities  for  innovation  and  growth  are \nbeing  limited  by  a  lack  of  access  to  data.\n4\n There  is  considerable  debate  about  possible \nmechanisms  for  paying  people  for  personal  data.  At  the  same  time,  there  are  trade-offs, \nparticularly the need to ensure adequate incentives for investment in data, and the risks involved \nin storing and using data, and protecting privacy. A better understanding of data value will inform \nthese  discussions  and  help  to  shape  appropriate  regulation  and  governance,  in  a  context  of \nsignificant distrust of the uses to which individuals’ data may be put by both public and private \nsector entities.  \nWhile a growing number of studies have investigated the value of data (reported in our separate \nliterature review), most treat data as homogeneous. However, the value of different types of data \ncan  be  very  different:  for  example,  it  may  be  reference  data, streaming  data,  historical  data, \nstatistical data or sensitive individual data; it may have different levels of completeness, accuracy \nor representativeness; it may depreciate more or less rapidly; it may be unique or commonplace; \nits marginal value may differ depending on context and use; and so on. This paper starts from \nthe basic economics of data in order to develop a more nuanced understanding of how to value \nit using the two lenses of economic characteristics and informational content.  \nMany  of  the  available  empirical  studies  use  market  valuations  or  transactions  as  the  basis  for \nestimating   the   value   of   data.   By   definition,   these   valuations   exclude   externalities   and \ncomplementarities. In effect, considering data markets as a basis for policy leaves public value \non the table – for instance, the additional value that could be derived from enhanced access to \nenable additional uses; or the additional value from being able to combine different data sets. \nThe (social welfare) value left untapped by failing to enable these non-market opportunities is \nof significant policy relevance. \nOur approach in this report has drawn on a series of expert interviews, extensive discussions with \nour advisory group (see p39), feedback from conference presentations, our own analysis, and a \nreview of the existing literature. We have also prepared a summary. The report touches on a wide \n                                                   \n3\n W. Hall and J. Pesenti, “Growing the artificial intelligence industry in the UK,” Department for Digital, \nCulture, Media & Sport and Department for Business (DCMS), Energy & Industrial Strategy (BEIS). Part of the \nIndustrial Strategy UK and the Commonwealth, (2017). \n4\n HM Treasury, “Unlocking digital competition Report of the Digital Competition Expert Panel,” (2019). \nAccessed at: \nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/7855\n47/unlocking_digital_competition_furman_review_web.pdf.  \n\n \nThe Value of Data / policy implications - 3 \n \nrange of challenging issues and trade-offs, and considerable work remains in order to flesh out \nthe policy implications of the data landscape. With this caution, we conclude that it is possible \nto generate more society-wide economic value from data if the right set of policies can be put in \nplace; but if the data economy is ‘left to the market’ there will be worse outcomes in terms of \nsocial  welfare  because  there  is  a  wedge  between  private  and  social  incentives  due  to  the \nunderlying economic characteristics of data.  \nWe set out some recommendations in the final section of the report. The key points concern the \nneed  to  consider  in  detail,  in  different  contexts,  the  access  rights  different  organisations  or \nindividuals  have  to  certain  data,  and  establishing  a  trustworthy  institutional  framework  for \nmanaging, monitoring and enforcing the terms of access. Asymmetries of information mean that \ncontracts  for  data  use  are  incomplete,  and  the  regulatory  framework  should  recognise  this, \nparticularly that schemes for sharing data in a regulated way change the returns on investment \nin collecting and cleaning data, and investing in complementary skills and assets. There are also \nsome unavoidable trade-offs that will require policy choices. \n  \n\n \nThe Value of Data / policy implications - 4 \n \nThe economic lens:  \nThe distinctive economic characteristics of data \n \nThere  are  several  existing  taxonomies  of  data  aiming  to  delineate  characteristics  relevant  to \nvaluation. Some are presented in the table below.  \nBy characteristics By origin By usage By feature \nOECD 2013\n5\n OECD 2013\n6\n Sweden National Board of \nTrade 2014\n7\n \nNguyen & Paczos 2018\n8\n \n•Sensitivity \n•Subject \n•Purposes \n•Context \n•Identifiability \n•In/directly collected \n•Provided \n•Observed \n•Derived \n•Inferred \n•Corporate \n•B2C \n•Human resources \n•B2B \n•Technical \n•Public or private \n•Proprietary or \nopen/public domain \n•Personal or not \n•User created/machine \ngenerated/administrative \n•Actively or passively \ncreated \n \nRather than these taxonomies, when thinking about the value of data in the broad sense in which \nwe  mean  it,  two  lenses  need  to  be  turned  on  data:  one  is  the  economic  lens;  the  other  is  the \ninformation lens.  \nThe economic lens is the first one we apply to consider the value of data. Data is an intangible \nasset with distinctive economic characteristics, which do not map onto these taxonomies.  \nMost importantly, data is non-rival:  unlike  many  conventional  goods  (such  as  apples)  or  assets \n(such as machine tools), many people can use the same data at the same time without it being \nused  up.  It  is,  technically  speaking,  either  a public  good,  or  a club  good when  access  to  it  is \nexcluded by technical and/or legal means. Data is therefore shared to varying degrees, or its use \nis  licensed.  It  is not best  thought  of  as  owned  or  exchanged.  Our  interviewees  agreed  that \npersonal  ‘ownership’  is  an  inappropriate  concept  for  data  (and  that  characterising  data  as  ‘the \nnew  oil’  is  similarly  misleading).  Excludability  determines whether  or  not  data  is  a  club  good \n                                                   \n5\n OECD, \"Exploring the Economics of Personal Data: A Survey of Methodologies for Measuring Monetary \nValue,\" OECD Digital Economy Papers no. 220 (Paris: OECD Publishing, 2013). \n6\n Ibid. \n7\n The National Board of Trade, “No Transfer, No Trade: The Importance of Cross-Border Data Transfers for \nCompanies Based in Sweden” (The National Board of Trade, 2014): 8. \n8\n D. Nguyen and M. Paczos, “Measuring the economic value of data and data flows,” presentation at OECD \nWorking Party on Measurement and Analysis of the Digital Economy, Paris, 7 May 2019. \n\n \nThe Value of Data / policy implications - 5 \n \nrather than a pure public good; different types of data have different excludability characteristics, \ndiscussed below.  \nData  often  involves externalities. These  are  often  positive,  such  as  additional  data  improving \npredictive accuracy, or enhancing the information content of other data.\n9\n  In these cases,  data \noften gains its value from being combined with other data. For example, one person’s health data \ngains  much  of  its  value  from  comparison  with  aggregate  statistics,  such  as  the  distribution  of \ncholesterol levels in the population or average blood pressure, and other data based on research \nabout how these are linked to health outcomes. There may also be negative externalities, notably \nthe compromising of individual privacy.\n10\n  \nAlthough there is a strict legal definition of personal data in the EU under GDPR, which means it \nhas  to  be  treated  differently  from  other  types  of  data  (implying  different  costs  and  risks),  our \ninterviewees  by  and  large  considered  it  would  be  impossible  to  define  with  any  precision  an \neconomically   meaningful   category   of   ‘personal   data’.   For   although   individuals   provide \nconsiderable amounts of data about themselves, which may be sensitive or private, the valuable \ninformation content often lies in aggregation or in comparison with data provided by others. The \ninformation is co-produced by individuals and by companies, with individuals creating (positive \nor  negative)  informational  externalities  for  each  other.\n11\n Consequently,  focusing  questions  of \nvalue on the data provided by individuals overlooks the allocation of value created thanks to the \nexternalities and complementarities.  \nAlong  with  this  public  good  character  of  data  (in  the  technical  sense  of  non-rivalry),  the \nexternalities  mean  that  market  mechanisms  are  unlikely  to  deliver  socially  optimal  outcomes, \nproducing  too  little  and/or  charging  too  much  where  there  are  positive  externalities,  and  vice \nversa.  We  are  probably  in  a  situation  where  both  are  true:  there  is  over-production  and \ncommercial use of some types of data raising privacy concerns; and also under-production and \nuse in contexts where the commercial opportunities may not be so obvious (or may be limited to \nthe aggregation of data across consumers or across activities occurring within individual firms) \nbut the potential public value is large. Together, the non-rivalry and externalities mean there is \na  wedge  between  the  private  value  of  data  and  public  value  (social  welfare  in  economists’ \nlanguage).   When   there   are   positive   externalities,   and information   content   comes   from \naggregation, too little data will be provided for use, from the perspective of society, as it can be \ndifficult  for  whoever  incurs  the  cost  to  capture  the  benefits  of  it.  In  either  case,  market \ntransactions alone will not lead to the best social outcomes; a strategy of public investment and \nregulation is essential.  \n                                                   \n9\n C. Jones and C. Tonetti, “Nonrivalry and the Economics of Data,” Stanford Business School Working Paper \n(August 2019). Accessed at: https://www.gsb.stanford.edu/faculty-research/working-papers/nonrivalry-\neconomics-data. \n10\n D. Acemoglu et al., “Too Much Data,” MIT Working Paper (September 2019). Accessed at: \nhttps://economics.mit.edu/files/17760. \n11\n See also D. Bergemann and A. Bonatti, “The Economics of Social Data: An Introduction,” Cowles \nFoundation Discussion Paper 2171R (September 2019). \n\n \nThe Value of Data / policy implications - 6 \n \nThe distribution of value is also affected. The identity of the beneficiaries of insights from data \ninfluences its total potential value, how that value is likely to be distributed, and the likelihood \nof  investment  in  that  data.  For  example,  data  about  purchasing  decisions  may  be  valuable  to \nadvertising agencies; it could either boost their profits or reduce costs for advertisers. Data about \ndisease contagion may be valuable to public health professionals; it may increase the impact of \nmoney spent on public health measures and reduce sickness and death. Data about purchasing \ndecisions might attract a high market price because of the direct economic benefit to advertising \nagencies, while that about disease does not. \n \nAggregated value may often be greater than the sum of individual values, but sometimes there \ncan  be increasing  returns to  gathering  more  data,  and  sometimes diminishing  returns.  This  is \ndetermined by context and use. Sometimes data is needed to build a predictive statistical model. \nSo,  at  some  point,  after  a  period  of  increasing  returns,  diminishing  returns  to  additional  data \npoints set in. Nevertheless, data holders with market power may continue to accumulate data as \na means of cementing their economic rents and using data as a barrier to entry. In other cases, \nsuch as certain mapping and traffic applications, granular real time data are useful and more data \npoints  will  continue  to  add  value.  Data  can  also depreciate,  losing  value,  at  different  rates \ndepending  on  context  and  use - slowly  for  population  health  data,  much  faster  for  data \nconcerning real time traffic flows supporting in-car navigation systems. So, the volume of data, \nits granularity, and its speed - as well as its accuracy - will all have implications for value but in \nvarying ways depending on context. \nAs  well  as  the context,  discussed  further  below,  the consequences of  data  use  affect  value. \nAdditional data will be more valuable in highly consequential situations. Contrast the potential \nlife and death consequences of autonomous vehicles with the consequences of badly targeted \nadvertising. The use values differ widely.  \nOften  individual  sources  of  data  will  have  considerable option  value,  or  in other  words  might \nbecome valuable if new questions, not yet thought of, can be answered in future. The consensus \namong our interviewees was that many of the organisations accumulating data have been doing \nso  because  of  potential  rather  than  actual  uses.  The  EU’s  GDPR  legislation  rules  out  the \naccumulation of individuals’ data for other than specified reasons but this may rule out potential \nfor  innovation;  significant  innovations  usually  derive  from  new  questions  rather  than  new \nanswers to old questions. While the legislation does not formally require individuals’ permission \nto be sought for any new use of their data, many companies are currently taking an ultra-cautious \napproach. It may take some time for GDPR to be fully understood and tested. \nData also involves costs and risks; it can be a liability as well as an asset.  \nInvestment in the collection and cleaning of data often has a high up-front cost and low marginal \ncost (like other digital or intangible goods and assets). Up-front costs might involve investment \nin  hardware  (such  as  sensors),  software,  data  modelling  or  standardisation,  and  in  developing \nprocesses for collecting and maintaining data. The return on the investment will depend on the \nuse of the data. This use value is often likely to exceed the marginal cost, particularly when the \ncollection of data is highly automated, for example generated through the delivery of a digital \nservice or from sensors. Secure storage of data - necessary when data is sensitive - involves costs, \nand while storage has become relatively cheap, the risk of data breaches has increased. There \nare  additionally  reputational  and  financial  risks  (including  fines)  associated  with  security \n\n \nThe Value of Data / policy implications - 7 \n \nbreaches or data misuse. When data collection is laborious, for example involving surveying of \npeople,  organisations  or  the  physical  environment  to  create  maps  or  registers,  the  costs  of \nmaintaining data can also be high.  \nFinally, capturing the value from data will often need specific capabilities (e.g. data science and \nanalytical  skills,  management  know-how)  or complementary  investments (e.g.  software,  other \ncapital equipment). Our interviewees consistently indicated that a lack of capabilities is a major \nbarrier to capturing the potential value from data use. \nTogether, these considerations imply the following issues and potential valuation methods: \nCharacteristic Issues Evaluation \nDiminishing/increasing marginal \nreturns? \nHow granular is the necessary data? \nHow much data is needed for \nprediction models?  \n \n \nIs the holder using data \naccumulation as a source of market \npower? \nAccuracy of predictive models \n \nInnovations and quality \nimprovements in services \n \nMonopoly rents - profitability, \nabsence of new entry \nExternalities Does additional data, or \naggregation, sharing/open data, or \njoining different data sources add \ninformation? Does it compromise \nprivacy? \nInnovations and quality \nimprovements in services \n \nContingent valuation methods \nOptionality Does gathering more information \nprovide scope for future process or \nquality improvements or \ninnovation? \nReal options methods \nConsequences Are decisions made using the data \nhighly consequential? \nValue at Risk methods \nCosts What costs need to be covered - \ndata acquisition, data cleaning, \nstorage, skills and capabilities, \ngovernance? \n \nWhat are the contingent costs - \nsecurity breaches, loss of sensitive \ninformation, reputational damage, \nfines? \n \n \n \n \n \nHarm to identified individuals (eg if \nlater defrauded), loss of commercial \nconfidentiality \n \nRisk assessments \n\n \nThe Value of Data / policy implications - 8 \n \nThe information lens: Subject, context and use \n \nThe second lens to apply to data value concerns its information content - illustrated by the classic \ninformation pyramid, Figure 1, below: information enables people, firms and government officials \nto make better decisions, depending on their objectives. Context matters because the value of \ndata is not related in any simple way to its volume (records, bits etc).  \nFigure 1: The Information Pyramid \n \nThe  subject  or  information  content  of  data  determines  how  useful  it  will  be.  A  number  of \ncharacteristics shape this. \nSome of these reflect use value. Data can be about people (such as demographics, behaviours, \nand relationships), about organisations (such as their types, activities and business relationships), \nabout  the  natural  environment,  built  environment  or  manufactured  objects.  It  can  be  used  to \nmake  decisions  that  affect  us  economically - such  as  about  purchases  or  investments;  our \nenvironment - such as our energy and transport use; or our lives - such as our health, education \nor engagement with society.  \nThe generality of data determines how many decisions the data is useful for. Some data might be \nonly  valuable  for  a  few  purposes  and  other  data  useful  in  a  range  of  different  scenarios.  For \nexample,  labelled  retinal  scans  might  only  be  useful  for  creating  diagnostic  systems  for  eye \ndiseases whereas geospatial data might be used for things as varied as navigation, understanding \nthe density of services offered to different communities, or predicting the impact of floods. The \ngranularity of data affects its generality: the more detailed and granular data is, the more purposes \nit can be put to because it can be filtered, aggregated and combined in different ways to reveal \ndifferent insights. \nThe geospatial coverage of data limits its utility. Data about a city is largely useful only within \nthat city, for example. Data with a wide geospatial coverage can have limited value at a more \nlocal level unless it has high geospatial granularity that allows it to be filtered to provide a more \nlocal  view.  While  data  about  people  from  one place  can  inform  decisions  about  other  places, \n\n \nThe Value of Data / policy implications - 9 \n \ndifferences in laws, cultures, and demographics mean that even data that does not seem to be \ntied to a particular area can have limited cross-locale relevance. \nThe temporal coverage of data also determine how it can be used. Data can be: \n● Forecasts that predict what will happen in the future \n● From the present or recent past \n● Part of the historical record \n● A backcast that estimates what happened in the past \n \nData can be used by people and organisations taking different kinds of actions: \n● Planners - acting  to  affect  or  prepare  for  the  short/medium/long  term  future  eg  city \nplanners, children choosing schools/subjects \n● Operators - acting to deal with the present, eg. doctors in A&E, commuters deciding what \nroute to take home \n● Historians - acting to respond to something in the past eg. police investigating a crime, \ntax collectors. \n \nThe  utility  of  different  temporal  characteristics  for  these  different  people  are  shown  in  the \nfollowing table: \n \n Planners Operators Historians \nForecasts Most valuable Near future potentially \nvaluable if it helps \nprioritise \nNot valuable \nCurrent/recent past Valuable to feed into \nprediction engines \nMost valuable Valuable only in so far as \nit provides an anchor for \nunderstanding what \nhappened in the past \nHistorical record Valuable to create and \nvalidate prediction \nengines \nOnly valuable in so far as \ninforms current decision \nmaking \nMost valuable \nBackcasts Valuable when historical \nrecord is missing, to \nsupplement existing data \nin the generation of \nprediction engines \nNot valuable Potentially valuable if it \nsupplements or supports \nthe historical record \n \n\n \nThe Value of Data / policy implications - 10 \n \nQuality \nQuality is frequently cited as an important characteristic of data. Our interviewees emphasised \nthe quality of data as a key challenge. In particular, this is a growing challenge with the use of \nbig data sets, where the quality issues can be significant .\n12\n It is important to note that the quality \nneeded depends on what the data is being used for. Higher quality data reduces uncertainty or \nreduces the risk that decisions based on it are incorrect; knowledge of known issues with quality \ncan help with assessing that risk. Data quality is typically described in terms of characteristics \nsuch as completeness, accuracy, and timeliness: \n● Completeness is an assessment of what proportion of reality a dataset represents. This can \ninclude  its  spatial  and  temporal  extent  as  well  as  being  influenced  by  sampling  and \nbiases in data collection. \n● Accuracy is  an  assessment  of  the  correctness  of  the  information  made  available  in  the \ndataset.  Accuracy  can  be  influenced  by  the  method  of  data  collection,  with  more \nautomated mechanisms being more accurate. \n● Timeliness is an assessment of the delay between the time period the data is about (its \ntemporal extent) and when it is available. Timeliness is particularly important for data \nbeing  used  in  an  operational  context,  and  for  data  that  relates  to  the  recent  past  and \nforecasts of the near future. \n \nThe  lower  the  completeness,  accuracy  and  timeliness  of  data,  then  in  general the  greater  the \nmarginal returns on additional data being incorporated into the dataset. \nSensitivity and personal data \nData can be sensitive for a number of reasons, including revealing information about individuals \nor  organisations,  or  about  physical  assets  that  might  be  susceptible  to  attacks  or  disruption. \nSensitive data will normally have restrictions on access to protect the people, organisations or \nthings  it  is  about.  However,  sensitive  data  is  usually  thought  to  be  vital  for  personalising  or \ncustomising a product or service. The requirement to protect sensitive data means that collecting \nand storing it will have additional costs.  \nInteroperability and linkability \nAs discussed above, the value of data frequently arises from it being brought together with other \ndata. There are two characteristics that relate to the ease of combining datasets: \n● Interoperability relates to the use of data standards when representing data, which means \nthat data relating to the same things can be easily brought together. \n● Linkability relates  to  the  use  of  standard  identifiers  within  the  dataset  that  enables  a \nrecord in one dataset to be connected to additional data in another dataset. \n                                                   \n12\n See for example Maurizio Vichi and David Hand, “Trusted Smart Statistics: the challenge of extracting \nusable aggregate information from new data sources.” Statistical Journal of the IAOS: vol. 35, no. 4, pp. \n605-613, 2019  \n\n \nThe Value of Data / policy implications - 11 \n \nExcludability \nOnce collected and accessible, data is non-rival. However, as noted earlier some types of data \ncan be naturally excludable while others are not. Categories for data based on its excludability \ninclude: \n● Environmental  data is  data  collected  about  the  environment.  Anyone  can  see  data  that \narises from the environment, so anyone (with the right sensors) can collect it. It is hard \nto exclude environmental data from those who can afford to collect it and is becoming \nharder with the growing use of sensors and satellites. Examples include geospatial data, \nrainfall,  satellite  data,  air  pollution, roadworks,  and  data  that  is  public  on  the  internet \nsuch as tweets or LinkedIn profiles. \n● Administrative  data is  data  that  is  collected  as  people  interact  with  public  or  private \nservices. Unless it is explicitly shared, this kind of data is only naturally visible to those \nproviding  the  service.  Examples  include  tax  returns  to  tax  offices,  shopping  carts  to \nretailers, patient records to healthcare providers. \n● Planned data is data about planned activities. This information is invisible except to those \ndoing  the planning  and  is  therefore  very  easy  to  exclude.  Examples  include  budgets, \nroadwork schedules, or bus timetables. \n● Predicted data is data about what might happen in the future. This may be data anyone \ncan  create,  if  they  have  access  to  enough  data  to  create reasonable  predictions  and \ncapability  to  create  predictive  models.  Examples  include  voting  outcomes,  weather \nforecasts, or stockmarket predictions. \n● Historic data is data about historic events. This data is only accessible to those who were \nthere  or  who  recorded  it,  although  it  may  be  reconstituted  through  backcasting  in  a \nsimilar  way  to  predicted  data.  Examples  include  an  individual  browser  history,  historic \nmembers of parliament, actual bus times. \n \nAccessibility \nThe useful distinctions above do not, however, capture the role of access to data in unlocking its \nwider value to economies and societies. To move toward an alternative approach, we begin by \ndescribing in more detail the Data Spectrum. Data, although inherently non-rival, can be closed, \nshared, or open:  \nIf access to data is restricted, its uses are limited to what that organisation can do with \nit. \nIf it is shared with a select group of people - a club good - its uses and analysis can be \nwider, perhaps creating more value. \nIf data is shared openly - a public good - anyone can use it.  \nThe creation of wider social value through greater openness is limited by the sensitivity of the \ndata - sharing more widely can cause harm to the people, organisations or environment the data \ndescribes - and by incentives to invest, as firms might be reluctant to spend money on collecting \nand controlling the data if others are going to capture the benefits.  \n\n \nThe Value of Data / policy implications - 12 \n \nThe Data Spectrum (Figure 2 below) shows some of the access conditions determining whether \ndata  is  a  private,  shared,  or  public  good.  Access  conditions  can  be  determined  by  technology, \nlicensing  or  terms  and  conditions,  and  regulation.  For  example,  an  authentication  process  for \nparties  wanting  to  access  data  on  a  medical  research  project  restricts  availability,  which \nsafeguards sensitive data and perhaps raises value creation potential by enhancing incentives for \ninvestment  in  long-term  research.  Equally,  a  lack  of  interoperable  standards  or  restrictive \nlicensing will reduce potential investment and value creation. \n \nFigure 2: The Data Spectrum \n \n \nAccess  conditions  can  be  determined  by  how  many  overlapping  use  rights  might  apply  to  the \ndata.  If  data  is  collected  about  someone  who  has  a  high  degree  of  control  over  it,  they  have \nsomething akin to a unilateral property right and may be able to demand payment for others to \nuse it. Where data is collected about many people at once, and they have rights to control sharing \nor use of that data, there will need to be a process of negotiating claims to control before it can \nbe used.  \n\n \nThe Value of Data / policy implications - 13 \n \nData about individuals can be found at both ends of the Data Spectrum, and rights and access \naffect its value.\n13\n Information  about  the  consumption,  movement,  and  work  habits  of  a  person \ncan  be  valuable  to  advertisers,  particularly  when  similar  data  is  collected  about  lots  of  other \npeople.  But  the  risk  of  harm  from  sensitive  insights  about  that  behaviour  being  released \ninappropriately has long motivated data protection laws around the world and hence barriers to \naccess.\n14\n \nIt is also important to note that the spectrum from open to closed is distinct from the contrast \nbetween  free  and  purchased.  The  financing  of  access  to  data  is  an  important  but  different \nquestion. \n \n  \n                                                   \n13\n Open Data Institute, “Anonymisation and open data: An introduction to managing the risk of re-\nidentification” (2019). Accessed at: \nhttps://docs.google.com/document/d/1CoXniaTnQL_4ZyQuji9_MA_YCEElQjx4z1SEdB08c2M. \n14\n  Although the EU’s Digital Single Market strategy has distinguished non-personal data – such as on \ntransport timetables – from personal data in an attempt to simplify transactions. European Commission, \n“Free flow of non-personal data” (2019). Accessed at: https://ec.europa.eu/digital-single-market/en/free-\nflow-non-personal-data; Eur-Lex, “Regulation (EU) 2018/1807 of the European Parliament and of the \nCouncil of 14 November 2018 on a framework for the free flow of non-personal data in the European \nUnion (2019). Accessed at: https://eur-lex.europa.eu/legal-\ncontent/EN/TXT/?qid=1546942605408&uri=CELEX:32018R1807. \n\n \nThe Value of Data / policy implications - 14 \n \nThe current UK legal framework  \n \nThe trading of data, and the distribution of value arising from it, are founded on legal rights. This \nsection describes the basics of the current rights framework in the UK. \n \nIntellectual property rights and licensing \nWhen an organisation or individual creates an intangible asset, such as documents, code or data, \nthey automatically have intellectual property rights in that asset. The most important category \nwhen  it  comes  to  data  are copyright - rights  over  assets  generated  through  creativity - and \ndatabase  rights - rights  over  datasets  arising  when  significant  effort  is  invested  in  creating or \nmaintaining that data. There are no intellectual property rights in plain facts; these only arise \nwhen  facts  are  arranged  into  databases.  Unlike  copyright, sui  generis database  rights  are  only \ndefined in a few countries, mostly in the European Union, and they exist in the UK by virtue of \nThe Copyright and Rights in Databases Regulations 1997.\n15\n These have a term of 15 years, but as \nthis is extended when substantial alterations are made to the database, there is essentially no \ntermination date for database rights in data constantly kept up to date. \nThese  intellectual  property  rights  limit  what  other  people  can  do  without  explicit  permission \nfrom the rights holder. Permission to use data and other content can be explicitly provided either \nby  licensing  it  or by  dedicating  it  to  the  public  domain,  which  means  waiving  IP  rights  in  the \nasset. \nLicences to use data can be generated on a case-by-case basis, through negotiation between the \nrights holder and the licensee. However, more typically a rights holder will have a fixed licence \nthat it applies to particular data. There are also standard licences, most notably open licences, \nwhich are adopted by multiple licensors. Standard licences reduce the transaction and legal costs \ninvolved in setting up an arrangement to use data. \nThis legal framework thus provides for the holders of database rights to charge for: \n● The  IP  rights  themselves,  i.e.  to  transfer  both  a  database  and  the  rights  to  use  that \ndatabase and determine who else uses it, to someone else; \n● A one-off licence to use a particular version of a dataset; \n● A  continuous  or  recurring  licence,  where  the  licensee  will  pay  a  subscription  to  retain \naccess to up-to-date data. \n \nLicences  can  and  frequently  do  contain  clauses  that  limit  what  licensees  can  do with  data,  in \nparticular to protect the licensor from losing revenue if licensees make the data available to third \nparties. Licences may limit the ability of the licensee to sub-license, may allow this only in return \nfor  additional  royalties,  and  may  claim additional  rights  over  data  derived  from  the  originally \nlicensed data. \n                                                   \n15\n See: http://www.legislation.gov.uk/uksi/1997/3032/made \n\n \nThe Value of Data / policy implications - 15 \n \n \nCopyright  and  database  rights  are  private  rights;  defending  them  requires  the  holders  of \nintellectual property rights to take legal action themselves. Protecting IP entails both detection \nand enforcement costs. Organisations that use, or anticipate they may be accused of using, third-\nparty IP can take insurance against legal action. \nIntellectual property rights in public sector information \nThe intellectual property rights in public sector information (PSI) - that is information generated \nby a public body in the course of delivering on its public task - is held by the Crown. Those rights \nare administered by the Queen’s Printer, within The National Archives, through the Government \nLicensing Framework.\n16\n Under this framework, most PSI that does not contain personal data is \nlicensed with the Open Government Licence (OGL). Public bodies can only license data differently \nif they are given a delegation of authority allowing them to do so. \nThe Reuse of Public Sector Information Regulations 2015\n17\n constrains the ways in which PSI can \nbe licensed, in particular ensuring that no exclusive licences are granted (which would prevent \nthe public body from granting a licence to other reusers) and ensuring no one is given preferential \nterms.  \nAccess  to  some  data  created  by  public  bodies  can  also  be  requested  through  the  Freedom  of \nInformation Act 2000\n18\n and the Freedom of Information (Scotland) Act 2002.\n19\n The Freedom of \nInformation Act 2000 was amended by the Protection of Freedoms Act 2012 to ensure that public \nbodies  provide  clarity  about  the  licensing  of  any  data  requested  through  the  act.  The \nEnvironmental  Information  Regulations  2004\n20\n also  provides  a  mechanism  for  accessing \nenvironmental information, including from some private sector bodies such as water companies \nwho are delivering a public service. \nThese rights and responsibilities are regulated and enforced by the Information Commissioner's \nOffice (ICO). \n                                                   \n16\n Government Licensing Framework. Accessed at: http://www.nationalarchives.gov.uk/information-\nmanagement/re-using-public-sector-information/uk-government-licensing-framework/. \n17\n See: https://www.legislation.gov.uk/id/uksi/2015/1415 \n18\n See: https://www.legislation.gov.uk/id/ukpga/2000/36 \n19\n See: https://www.legislation.gov.uk/id/asp/2002/13 \n20\n See: https://www.legislation.gov.uk/id/uksi/2004/3391 \n\n \nThe Value of Data / policy implications - 16 \n \nData protection rights \nWhile they are alive, people have a set of rights over data that is collected about them.\n21\n These \nrights are enshrined in the Data Protection Act 2018,\n22\n which maps the General Data Protection \nRegulations (GDPR) into UK law. \nData protection law limits what data organisations can collect about people (particularly when \nit is more sensitive, special category data) and how it can be used. Personal data can be \nlawfully collected, processed and shared by obtaining consent or having a contract with data \nsubjects (those identified within the data). However, consent is not the only lawful basis for \nprocessing personal data: data can also be used where necessary to fulfil legal obligations, save \npeople’s lives, by public bodies to deliver their public task, and for other “legitimate interests” \nof the organisation or third parties, which can include for wider public good, as long as this is \nbalanced against any privacy impact on the data subjects. \nData protection law frequently requires organisations that gather personal data to keep track of \nwhen people have provided consent, or restricted the use of data. It also requires organisations \nto enable people to access, correct and delete data held about them as well as to provide \ninformation about how data is used within the organisation. These requirements can add costs \nto organisations that hold or process personal data. \nWhile data protection law does not explicitly prevent organisations from selling personal data \n(as long as they have a legal basis for doing so, such as consent), any buyer of such data needs \nto also have a lawful basis for processing that data themselves. Since these buyers tend not to \nhave a direct relationship with data subjects, they often rely on the basis in law of “legitimate \ninterests” to process data. However, the Information Commissioner’s Office’s Update report on \nadtech and real time bidding indicates “legitimate interests” is currently being frequently \nmisused and misunderstood, particularly as it cannot be used as the basis for processing special \ncategory data.\n23\n For organisations for whom behaving lawfully is important or the threat of \naction by ICO is an effective deterrent, GDPR should limit the sharing and trading of personal \ndata. \n  \n                                                   \n21\n Information Commissioner’s Office, “Guide to Data Protection.” Accessed at: https://ico.org.uk/for-\norganisations/guide-to-data-protection/. \n22\n See: https://www.legislation.gov.uk/id/ukpga/2018/12 \n23\n Update report into adtech and real time bidding, Information Commissioner’s Office. Accessed at: \nhttps://ico.org.uk/media/about-the-ico/documents/2615156/adtech-real-time-bidding-report-201906.pdf \n\n \nThe Value of Data / policy implications - 17 \n \nMarket-based methods of data valuation \n \nAlthough the economic characteristics of data mean it is unlikely that market-based transactions \ngive a complete picture of the value or potential value of data, a growing number of studies and \napproaches  use  market  prices  to  estimate  value.  These  can  be  divided  into  broad  categories: \nstockmarket valuations, and income-based or cost-based approaches.  \nOne approach is to compare the stockmarket valuations of companies that are and are not data-\nintensive. For example, a report by PwC finds that stockmarket valuations of data-driven firms \nwithin  the  same  industry  tend  to  be  higher  than  those  of  their  peers,  and  furthermore,  that \ncompanies  with  data  analytics  capabilities  are  twice  as  likely  to  end  up  in  the  top  quartile  of \nperformance  within  their  industries.\n24\n There  are  some  striking  examples  of  how  effective  data \nuse  can  affect  corporate  performance.  For  instance,  BP  has  a  10-year  $1.2bn  contract  with \nPalantir to integrate data across its businesses. One early result is a digital model of BP’s entire \nproduction system which can optimise the oil’s most efficient route, using data to speed the flow \nand increase production by 30,000 barrels a day.\n25\n  \nAn alternative approach, taken in a number of recent papers and reports, is income-based, using  \n“an estimate of future cash flows to be derived from the asset.”\n26\n The data value chain (Figure 3) \nvisualises this approach. \n \nFigure 3: Data value chain \n \nMawer, 2015 \n \n                                                   \n24\n PwC, “Putting a value on data” (2019). Accessed on 9 October 2019 at:  https://www.pwc.co.uk/data-\nanalytics/documents/putting-value-on-data.pdf. \n25\n A. Raval, “BP’s Bernard Looney takes oil major into energy transition,” Financial Times, 6 October 2019.  \n26\n C. Mawer, “Valuing data is hard,” Silicon Valley Data science blog post (2015). Accessed at: \nhttps://www.svds.com/valuing-data-is-hard/. See also C. Corrado, “Data as an Asset,” presentation at \nEMAEE 2019 Conference on the Economics, Governance and Management of AI, Robots and Digital \nTransformation, 2019; and M. Savona, “The Value of Data: Towards a Framework to Redistribute It,” SPRU \nWorking Paper 2019-21 (October 2019).  \n\n \nThe Value of Data / policy implications - 18 \n \nLi et al. (2019)\n27\n consider data value in the context of value chains for several different business \nmodels, such as e-commerce marketplaces, search, or matching platforms. Many of these involve \na  direct  monetary  benefit  to  the  company  accumulating  data,  an  indirect  monetary  benefit  to \nsuppliers  and  advertisers  who  subsequently  make  more  sales,  and  a  non-monetary  benefit  to \nfinal users who get free online services (as well as perhaps more choice or lower prices). However, \nthe authors observe that the income-based approach is limited because a data-driven business \nmodel,  embedded  in  the  organisation’s  capabilities,  can  create  additional  value  beyond  that \ngenerated by the chain of transactions. Thus, the big data-driven platforms effectively capture \nmuch of the social value of the data they have accumulated. For example, in Amazon’s case, the \nplatform is able to take advantage of the feedback loops its business model creates (Figure 4). \n \nFigure 4: Value creation in e-commerce platform \n \n \nLi et al (2019) \n \nResearch into users’ contingent valuation (willingness to pay/willingness to accept) suggests they \nplace  a  high value  on  the  free  online  services  they  can  access  in  return  for  their  provision  of \n                                                   \n27\n Li,\tW.C.Y.,\tNirei,\tM.,\t&\tYamana,\tK.\t2019.\t“Value\tof\tData:\tThere’s\tNo\tSuch\tThing\tas\ta\tFree\tLunch\tin\t\nthe\tDigital\tEconomy”,\tRIETI\tdiscussion\tpaper\t19-E-022\tand\tBEA\tworking\tpaper,\tFebruary\t2019. \n\n \nThe Value of Data / policy implications - 19 \n \ndata.\n28\n However, the character of the exchange (and market power/profitability of the large data-\naccumulating  companies)  has  led  to  some  debate  about  whether  or  not  individuals  should  be \npaid for data they provide .\n29\n This model has emerged in the case of some health data start-ups, \nwhich  act  as  platforms  matching  data  from  patients  who  sign  up  with  interested  pharma \ncompanies.\n30\n Any  such  payments  to  individuals  would  be  small, compared  to  the  externalities \ncreated by aggregating data - which is another way of restating the limitations of market-based \n(monetary) transactions as a basis for valuing data. \nThe  assessment  of  future  income  or  profits  can  change  substantially  as  new  opportunities \nemerge.  Technological  innovations  enable  new  ways  of  using  data  that  can  revalue  it.  For \ninstance, Arrieta Ibarra et al (2017) comment on the growing and future importance of machine \nlearning for data valuation, creating new potential uses and services and therefore new future \nincome  streams.\n31\n The  value  of  any  asset  depends  on  an  estimate  of  future  returns,  so  this \ncharacteristic is not unique to data assets; however, the barriers to new uses of data may be lower \nthan in the case of other assets. Market prices at any moment in time are unlikely to include the \nfull option value of the data.  \nAnother limitation to market-based methods of valuation is that there are not many ‘thick’ data \nmarkets with a sufficient number of buyers and sellers to ensure that the transaction prices are \nclosely  related  to  fundamental  economic  value.  Monetary  transactions  do  take  place,  with  an \nactive  landscape  of  data  broking  companies  selling  data  about  individuals  for  marketing \npurposes, and indeed a market in illegal transactions for stolen data. There are thousands of data \nbrokers offering different types of data on individuals or companies for sale. However, as these \ndata  markets  are  complicated,  non-transparent  and  increasingly  concentrated,  the  prices  of \ntransactions in them do not seem to be a sound basis for valuation.\n32\n A market study by the UK \nCompetition and Markets Authority expressed concern about whether consumers are getting a \nfair deal in the data-driven online advertising market.\n33\n Data brokers do not post prices, and there \nis a wide range for estimates of the value of personal data to businesses involved in advertising-\nbased  models  or  digital  marketing.  Estimates  based  on  prices  posted  on  the  dark  web,  where \n                                                   \n28\n E.g. E. Brynjolfsson et al., “Using massive online choice experiments to measure changes in well-\nbeing,” PNAS 116, no. 15 (2019): 7250-7255. Although note that algorithms may manipulate users’ \nbehaviour, see Morton, F.S., Bouvier, P., Ezrachi, A., Juliien, B., Kimmelman, G., Melamed, A.D., & \nMorgenster, J. 2019. Report on the Study of Digital Platforms, Chicago Stigler Center for the Study of the \nEconomy and the State, May. \n29\n I. Arrieta Ibarra et al. “Should We Treat Data as Labor? Moving Beyond 'Free'.” Moving Beyond 'Free' \n(December 27, 2017), American Economic Association Papers & Proceedings 1, no. 1 (2017). \n30\n S. Neville, “Patients take control of their medical data,” Financial Times, 23 April 2019. \n31\n I. Arrieta Ibarra et al. “Should We Treat Data as Labor? Moving Beyond 'Free'.” Moving Beyond 'Free' \n(December 27, 2017), American Economic Association Papers & Proceedings 1, no. 1 (2017). \n32\n Federal Trade Commission, “Data Brokers - A Call for Transparency and Accountability,” (2014). \n33\n See: https://www.gov.uk/cma-cases/online-platforms-and-digital-advertising-market-study#interim-\nreport \n\n \nThe Value of Data / policy implications - 20 \n \nhacked data is sold, range from around £1 for some retail website login details to around £200 \nfor bank or PayPal details.\n34\n  \nThe  alternative  cost-based  approach  is  used  in  estimating  the  aggregate  value  of  data  to  the \neconomy in the national accounts, as there are relatively few market sales of datasets, with most \nbeing generated within the business in the process of providing other goods and services. The \nfigures currently used in the national accounts are defined to reflect the costs to businesses - \nmainly labour costs - of preparing data in a useful format, but not of purchasing or producing the \nunderlying data in the first place. There is a debate among statisticians now about whether this \napproach is too limited, given the explosion of data gathering and use. One approach would be \nto  treat  firms’  creation  of  digitized  data  as  investment  in  an  asset,  which  would  increase  GDP \ncompared with the current treatment, and in turn would continue to be measured in terms of the \ncost  of  creating  the  data  (although  a  limitation  is  that  value  can  clearly  exceed  cost).  An \nalternative is to see data as generated by households and provided as a barter with businesses \nin  return  for  free  services;  this  approach  would  specifically  apply  to  advertising-funded  social \nmedia and search companies so its dependence on a particular business model is a drawback. \nOrganisations can also use a cost-based approach to valuing the data that they collect, either by \nlooking at the investment they put into collecting it, or by assessing the cost of replacing that \ndata  with  something  equivalent.  The  latter  approach  implies  that  unique  data  is  priceless, \nwhereas  the  value  of  data  from  things  that  can  be  observed  in  the  environment  (e.g.  satellite \nimagery) is diminishing as the number of alternative sources increases. \nSome of the research on market valuations illustrates the distinction between private and public \nvalue. For example, one study found that monitoring drivers prompted them to drive more safely, \nwhile  the  monitoring  data  enabled  their  insurance  company  to  make  a  higher  profit  while \nreducing  the  premium  charged  to  safer  drivers.  However,  requiring  the  insurance  company  to \nshare the data would have reduced its incentive to invest in the monitoring and data collection \nscheme.\n35\n \nThis  highlights  the  broader  point  that  the  regulatory  environment  changes  market  valuations. \nThe market transactions currently observed are not capturing a fundamental reality; rather, the \nmarket value of data is endogenous, depending on policy choices. There are likely to be many \ntrade-offs  between  creating  private incentives  to  invest  in  collecting  and  using  data,  and \ncapturing social benefits. The value of data will depend on the societal trade-offs, analogous to \nthose  in  the  domain  of  intellectual  property  where  there  is  a  trade-off  between  the  private \nincentive to  invest  in  innovation  created  by  the  temporary  monopoly  provided  by  patent  or \ncopyright, and the social benefit of ensuring wide access to innovations as quickly as possible. \nThe heated debate about the economics of intellectual property in the digital economy suggests \nthat the policy choices will be no easier in the case of data. \n \n                                                   \n34\n A. Williams, “How much is your data worth to hackers?” Moneywise, 21 March 2018. Accessed at: \nhttps://www.moneywise.co.uk/news/2018-03-21%E2%80%8C%E2%80%8C/how-much-your-data-worth-\nhackers \n35\n Y. Jin and S. Vasserman, “Buying Data from Consumers,” NBER Working Paper (2019).  \n\n \nThe Value of Data / policy implications - 21 \n \nExisting non-market estimates \n \nMarket valuations thus provide useful information but do not capture the full social value of data. \nA number of studies (in addition to those cited above concerning the value of ‘free’ services) have \nprovided  estimates  of  data  value  going  beyond  market  transaction  values,  mostly  using \ncontingent valuation methods.  \nA 2013 study of the impact of opening up Landsat data using this methodology estimated a value \nof  $2bn/year,  based  on  surveying  different  groups  of  users  to  estimate  the  average  monetary \nbenefit to each group.\n36\n This did not include the additional value to additional users provided \nwith services based on Landsat data. \nIn a 2017 study of TfL’s open data approach, Deloitte evaluated the cost savings and incremental \nvalue to three groups – passengers and other road users, the London economy as measured by \njob creation and commercial use of the data by firms, and TfL itself – generated from TfL’s £1m \na  year  investment  in  publishing  open  data.\n37\n For  passengers,  they  estimated  £70m-£90m/year \ncost savings through less time being wasted in adjusting routes in light of new information; they \nalso highlighted value arising from increased use of the public transport network particularly by \nthose with accessibility needs (£5.1m/year) and healthier lifestyles due to increased cycling and \nwalking. For the London economy, value arose from new companies using open data, amounting \nto £14m a year in GVA and the generation of 500 direct and 230 indirect additional jobs. For TfL, \nvalue  was  reflected  in  £1m  costs  saved  from  customer  support  services  they  would  otherwise \nneed to provide directly. \nA  2019  report  on  the  value  of  Companies  House  Data  included  a  valuation for  intermediaries \nsuch as credit reference agencies who use Companies House data as an input to their own data \nproducts and services as well as people and organisations who access the information directly \nfrom Companies House.\n38\n Intermediaries attributed £23m/year of their revenues and £5m/year \nof their costs to their use of Companies House data. They did not attempt to quantify the impact \nof this data being absent, but described costs associated with removing functionality from their \nproducts and services or collecting relevant data from businesses directly themselves. \nWhen 2,416 individuals were asked how much they valued their data privacy, Angela Winegar \nand Cass Sunstein found that willingness to pay for privacy was low (an average $5 a month) but \nwillingness  to  accept  loss  of  privacy  was  a  more  substantial  $80  a  month.\n39\n Both  figures  are \n                                                   \n36\n H. M. Miller et al. “Users, uses, and value of Landsat satellite imagery—Results from the 2012 survey of \nusers,” U.S. Geological Survey Open-File Report 2013–1269, 51 (2013). Accessed at: \nhttps://doi.org/10.3133/ofr20131269 \n37\n Deloitte, “Assessing the value of TfL’s open data and digital partnerships” (2017). Accessed at: \nhttp://content.tfl.gov.uk/deloitte-report-tfl-open-data.pdf. \n38\n UK Government (BEIS), “Companies House data: valuing the user benefits.” (2019) Accessed at: \nhttps://www.gov.uk/government/publications/companies-house-data-valuing-the-user-benefits. \n39\n A. G. Winegar and C. Sunstein, \"How Much Is Data Privacy Worth? A Preliminary Investigation,” Journal \nof Consumer Policy 42, no. 3 (September 2019): 425-440. Advocates of data privacy rights challenge the \n\n \nThe Value of Data / policy implications - 22 \n \nhigher than the amount indicated by calculations such as Facebook’s average profit per active \nuser (about $10)\n40\n or - a different type of benchmark - the amount per individual implied by fines \nfor data breaches (around $5 per person affected implied by Equifax’s 2019 fine of some $800m \nfrom  the  US  FTC,  £0.005  per  affected  user  implied  by  the  UK  ICO’s  2019  £500,000  fine  on \nFacebook\n41\n).  \nWhile the different approaches each have limitations, all these studies highlight that valuation \nof data in a wider societal context needs to consider different groups: \n1. The  costs  and  benefits  (or  risks  and  options)  to  data  stewards  of  collecting,  using  and \nsharing data; \n2. The  costs  and  benefits  to  intermediaries  with  whom  data  is  shared,  and  the  wider \neconomic impact of the activity of those organisations (for example in providing jobs). \nEach of the above studies highlights that products, services and entire businesses can be \nbrought into being due to data being available to them. For intermediaries, attributes of \ndata, such as quality and interoperability, are important for reducing costs and risks, as \nare aspects of their relationship with the data steward, such as receiving notifications of \nchanges; \n3. The  costs  and  benefits  to  end  users  or  consumers  who  use  the  products  and  services \nprovided by intermediaries. \n \n  \n                                                   \nvery notion of a price for privacy, reflecting the broader debate about the validity of contingent valuation \nmethods that seek to put monetary values on intrinsic goods. \n40\n Based on 2018 net income of $2.2bn and 2018 monthly active users of 2.2bn \nhttps://investor.fb.com/investor-news/press-release-details/2019/Facebook-Reports-Fourth-Quarter-and-\nFull-Year-2018-Results/default.aspx  \n41\n https://ico.org.uk/about-the-ico/news-and-events/news-and-blogs/2018/10/facebook-issued-with-\nmaximum-500-000-fine/ and 38m affected users; the figure is £0.50 if the minimum affected number of \nUK users only is used. \n\n \nThe Value of Data / policy implications - 23 \n \nCreating value through open and shared data \n \nA dataset holds information which needs to be analysed before it can be used in a product or \nservice to meet demand in a given context. Making data closed, shared or open means changing \nthe range of people who might analyse the digitally stored information, be able to turn it into a \nproduct, or use it in different contexts.    \nThe  public  good  character  of  data  and  the  prevalence  of  positive  externalities  create  a \npresumption that more open access to certain types of data will increase social welfare. However, \nthere are several trade-offs to consider. \nFirst, there is a trade-off due to the need to incentivise investment and innovation, and to cover \nongoing  costs  of  maintaining  data  securely.  This  is  similar  to  the  well-known  trade-off  in \nintellectual property, where patent or copyright protection restricting access is needed to create \nan incentive for investment in discovery and innovation to occur in the first place, but at the same \ntime limits the potential social welfare benefits of a new service or product. \nThis trade-off is most direct for organisations whose purpose and business model centres on data \ncollection and maintenance as opposed to those generating exhaust data as a consequence of \ntheir activities. In the former case, the cost of generating data has to be met. In the latter case, \ndecision-making about the investment is driven by the benefits of providing the data-generating \nservice. \nHowever, the need to provide more access to data can also disincentivise investment in products \nand services that use data. Exclusive access to data can enable firms to gain a market advantage \nfor  the  services  they  offer,  such  as  providing  a  more  personalised  service  to  their  customers. \nThere are also competitive considerations: if that data is also available to other companies to \nprovide  an  improved  service,  then  that  private  advantage  is  lost  although  social  benefits  will \nlikely  be  enhanced.  Companies  may  also  rightly  worry  about  privacy,  data  breaches  or \nunsanctioned uses if third parties have access to data. \nSo, for most organisations, providing access to certain types of data (individual or sensitive) is an \nadditional cost which may be difficult to meet. They are therefore likely to underinvest in the \nprovision of such data, if they do not charge for it. This may be particularly the case for public \nsector organisations with limited budgets for this purpose or alternatively if they are required to \nrecoup some of their costs from charges. \nA second trade-off applies to data that is personally or commercially sensitive. Individuals will \nwant  to  limit  access  to  certain  types  of  identifiable  information  about  themselves.  Companies \nwill  not  want  to  share  data  that  will  help  their  rivals.  So,  increasing  access  to  such  data  has \nattractions   but   also   involves   complexities,   including   avoiding   negative   privacy-intruding \nexternalities.  \nA  final trade-off  concerns  the  requirement  for  an  evidence  base  before  investing  in  data  or \nproviding access to data. The option value of data - the fact that it is hard to predict how data \nmight  be  used  by  other  organisations  or  in  the  future  as  technologies  change  and  other  data \nbecomes  available - means  there  is  inherent  uncertainty  when  making  cost/benefit  trade-offs \n\n \nThe Value of Data / policy implications - 24 \n \naround  investments  in  improving  the  quality  of  or  access  to  data.  Even  the  work  needed  to \nunderstand the potential realisable value of data in order to determine whether investing in it is \nworthwhile is complex and costly in and of itself.  \nMoves  toward  open government data  have  been  motivated  by  the  publishing  of  public  sector \ninformation  as  a  public  good.  That  motivation  comes  from  expectations  of  the  value  that  it \ncreates,  in  terms  of  transparency  and  accountability  of  democratic  institutions,  and  the \nstimulation of innovation and economic growth.  Allowing citizens to have more access to the \ninformation that the government holds can give them the opportunity to make more informed \ndecisions, while also analysing the data with skills that might not be available, or useful, to the \ngovernment. The government also collects substantial administrative data whose wider use could \nalso enable better decisions and improved services. As with any open data, government data can \nbe combined with private and shared data.  \nEstimates for the value of open data as a percentage of GDP have ranged from 0.08 percent to \n7.19  percent,  derived  from  different  mixtures  of  sectors,  countries,  types  of  data,  potential \nbenefits, and other factors (Figure 5).\n42\n A recent OECD report cites a range of 1 per cent to 2.5 \npercent of GDP.\n43\n The range of estimates may be partly caused by the lack of research into the \neffects  of  open  data in  comparison  with  properly  delineated  counterfactuals,  as  noted  by  the \n2018 Open Data Barometer report.\n44\n   \n \n \n \n \n \n \n \n \n \n                                                   \n42\n European Data Portal, “Analytical Report 9: The Economic Benefits of Open Data” (2017): 17. Accessed \nat: \nhttps://www.europeandataportal.eu/sites/default/files/analytical_report_n9_economic_benefits_of_open_\ndata.pdf. \n43\n OECD, Enhancing Access to and Sharing of Data: Reconciling Risks and Benefits for Data Re-use across \nSocieties (Paris: OECD Publishing, 2019).   \n44\n Open Data Barometer, “Open Data Barometer: Leaders Edition, from Promise to Progress,” (2018). \nAccessed at: https://opendatabarometer.org/?_year=2017&indicator=ODB. \n\n \nThe Value of Data / policy implications - 25 \n \nFigure 5: the value of Open Data as measured by different studies\n45\n \n \n \nThe debate about the value of open public sector data has led to consideration of which datasets \nmight be of most value.\n46\n The European Commission sees the value of public sector information \nas  determined  by  its  potential  to  create  economic  and  other  benefits;  the  potential  for  the \ncreation  of  innovative  services;  how  many  people  can  use  it;  the  scope  for  revenue;  the \npossibilities for re-combination; and the effects on public undertakings.\n47\n These suggest six types \nof  government  data  that  have  the  most  value:  geospatial,  earth  observation  and  environment, \nmeteorological, statistics, companies, and transport.\n48\n     \n                                                   \n45\n European Data Portal, “Analytical Report 9: The Economic Benefits of Open Data” (2017): 18. Accessed \nat:  \nhttps://www.europeandataportal.eu/sites/default/files/analytical_report_n9_economic_benefits_of_open_\ndata.pdf; also see P. Kuzev (ed). Open Data The Benefits (2016), Accessed at:  \nhttps://www.kas.de/einzeltitel/-/content/open-data.-the-benefits1. \n46\n Open Knowledge Foundation, “What data counts in Europe? Towards a public debate on Europe’s high \nvalue data and the PSI Directive” (2019). Accessed at:’ \nhttps://blog.okfn.org/2019/01/16/what-data-counts-in-europe-towards-a-public-debate-on-europes-\nhigh-value-data-and-the-psi-directive/. \n47\n Ibid. \n48\n Ibid. \n\n \nThe Value of Data / policy implications - 26 \n \nHowever, creating public sector information and making it openly available is costly. Although \nthe amount spent on official statistics and other public data is low in per capita terms - and, as \nnoted above, there is almost certainly too little provision - governments often consider requiring \npayments for publicly-held data to help cover the costs. What’s more, EU legislation has often \nbeen interpreted to  require that all users are charged the same amount, from individuals to big \ncorporations, even though the economic calculation in terms of marginal cost and benefit faced \nby different types of user differ greatly. Yet requiring payments for public sector data can impede \nits  use  and  hence  the  value  that  can  be  derived  from  it;  research  for  the  Open  Data  Institute \nfound  that  making  the  most  useful  public  datasets  open  would  create  0.5  percent  more  GDP \ngrowth per year for the British economy than making users pay for access to the data.\n49\n A number \nof our interviewees thought the government should not generally charge for public sector data \nbut were also concerned that it should capture some of the value created by the use of data to \ngenerate new services or products. \nPrivate sector organisations can also open the data that they hold.\n50\n In the development of artificial \nintelligence,  firms  can  adopt  business  models  that  make  their  algorithms  and  the  data  they \ncontrol more or less open, affecting how easy it is for them to collaborate with others and limiting \nthe costs they face in managing large datasets.\n51\n However, many large datasets are held by the \nprivate sector and are far less open than public data. At present there are relatively few incentives \nor legal requirements for private sector companies to share their data (although in the UK the \nDigital Economy Act provides a legal basis to mandate some limited sharing with the Office for \nNational Statistics). A mixture of regulation and institutional innovation is likely to be needed to \nenable greater provision of data by the private sector for public benefit  - discussed further below. \nThere is significant potential for shared or open data to promote competition and innovation in \nthe economy, in contrast to the hoarding of data by digital companies with considerable market \npower, as recently noted by the Digital Competition Expert Panel.\n52\n  \n  \n                                                   \n49\n Open Data Institute, “Research: The economic value of open versus paid data” (2016). Accessed at: \n https://theodi.org/article/research-the-economic-value-of-open-versus-paid-data/. \n50\n See for example https://blog.google/technology/research/open-source-and-open-data/  \n51\n Open Data Institute, “The role of data in AI business models” (2018). Accessed at: \nhttps://docs.google.com/document/d/14g0p6KSyH1r1J_PrykJIXUX-\nrdeP1B4CLIffAyFPOnk/edit#heading=h.rcydy9gttjg4. \n52\n HM Treasury, “Unlocking digital competition Report of the Digital Competition Expert Panel” (2019). \nAccessed at:  \nhttps://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/7855\n47/unlocking_digital_competition_furman_review_web.pdf.  \n\n \nThe Value of Data / policy implications - 27 \n \nInstitutions for the data economy \n \nThere  is  a  vast  literature  on  the  appropriate  institutional  framework  for  provision  of non-rival \ngoods: what norms, regulations and laws, and what mix of market, collective and government \ndecisions  about  production  and  allocation  will  maximise  social  welfare?  These  questions  are \nhighly relevant to policy choices aiming to get the best out of the data economy. The amount of \ndata is rapidly growing as digitization  makes  it  possible  to  turn  many  goods  and  services  into \ndata records, and as behaviour is changing significantly shifting activities online. There are very \nmany data sets, collected in different ways by different public and private sector organisations, \nwith access restricted in varying degrees including by lack of interoperable technical standards. \nThe number of potential uses probably far exceeds the actual usage of data to create valuable \ngoods and services. The governance challenge is both to prevent misuse of sensitive data - where \nmuch of the public policy focus has been to date - and to realise more of the potential from data, \nensuring the benefits are widely shared.  \nWhile this will involve traditional government regulation, discussed below, there is already some \ninstitutional innovation and experimentation with regard to access to data. Two principles are \nfundamental.  First,  in  order  to  increase  the  economic  value  of  data  to  society,  the  design \nchallenges  concern  establishing  terms  of  shared  access  that  enable  more  use,  and  capture \npositive externalities while limiting negative ones.  \nSecondly,  the  trustworthiness  of  the  institutions  is  of  paramount  importance  as  they  will  be \ndetermining  who  can  access  what  data  in  accordance  with  the  social  and  legal  ‘permissions’ \ngiven. In other words, the governance and processes matter. As O’Neill has argued, the real or \nperceived crisis of trust in many societies reflects suspicion of authority.\n53\n In the case of data, the \nsuspicion  can  seem  well  warranted  by  frequent  security  breaches,  stories  of  manipulation  or \nabuses  such  as  Facebook/Cambridge  Analytica.  Informed  consent,  especially  consent  given  to \nlong  and  obscure  terms  and  conditions  online,  is  inadequate  as  a  basis  of  trust.  Instead, \ntrustworthy  institutions  subject  to  intelligent  forms  of  accountability  (rather  than  the  target-\nbased or tick box versions found in some institutions) are needed. As Benedict Evans has pointed \nout, it is possible to discern an emerging societal consensus about who should be able to do what \nwith  different  data:  “Different  entities  have  permission  for  different  things.”\n54\n Is  it  the \nsupermarket, a video streaming app, or the police? Do we trust an organisation with certain data \nonly as long as it is not too easy for them to use at speed or at scale, or too easy to join up with \nother data?  \nAlthough data is in its economic characteristics almost the opposite of a ‘commons’ (which refers \nto resources such as fish or grazing land that are rival in consumption), Elinor Ostrom’s framework \nfor the management of shared resources also offers some useful insights for data regulation and \n                                                   \n53\n O. O’Neill, “A Question of Trust,” BBC Radio 4 (2002). Accessed at: www.bbc.co.uk/radio4/reith2002/.  \n54\n B. Evans, “Face Recognition and AI” (2019). Accessed at: https://www.ben-\nevans.com/benedictevans/2019/9/6/face-recognition. \n\n \nThe Value of Data / policy implications - 28 \n \ngovernance.\n55\n Her work considered contexts where people need to reach agreement about rules \nof access to a resource when some individuals will have to sacrifice private benefit for the greater \ncommon good. Just as a farmer upstream could benefit from not sharing water for irrigation with \nthose downstream but will enable higher crop yields as a whole if they do participate, the holder \nof data may sacrifice some private economic rents by sharing but will unlock potentially much \nlarger benefits for others.  \nShe identified the conditions determining the way different goods are produced and allocated, \nincluding - as well as the characteristics of the good itself - the prevailing social norms and trust, \nthe costs and benefits of different outcomes for different people, the information available and \nthe  technical  or  practical  conditions.  She  also  established  the  general  design  principles  for \ncollective  self-managing  institutions - set  out  here  with  implications  for  the  data  economy. \nWithin a data economy we need to consider three groups of people or organisations involved in \ncollaboration  around  data:  those  organisations  which  collect  and  share  data  (stewards);  those \nthat use data from those organisations; and those who are the subjects of data (i.e. the data is \nabout  them  or  is used  in  ways  that  affect  them).  These  groups  have  different  rights  and \ncapabilities,  and  there  will  be  asymmetries  of  information  between  them.  The  table  sets  out \nOstrom’s design principles and their data economy parallels: \n \n| Ostrom’s principles        | Data economy parallel \nThere are clear boundaries and rules about who is \nentitled to what \nClarity on the rights of different entities to control, \naccess, use and share data \nMonitoring actions is feasible Transparency and auditability of how data is being \ncollected, used and shared \nThere are mechanisms for resolving conflicts Regulators who can enforce both mandating and \nlimiting access to data \nIndividual responsibilities and benefits broadly balance Transparency and better understanding of both rights \nand how value from data returns to people and \norganisations \nUsers themselves are responsible for monitoring and \nenforcement \nTransparency and contractual terms to enable \nmonitoring and auditing of data use and sharing; in a \ndata economy this may require agents who can act on \nbehalf of data subjects \nSanctions for abuse are possible and graduated, getting \nprogressively tougher \nEnforcement of a range of consequences for the misuse \nof data, ranging from the withdrawal of access \npermissions to fines and other penalties \n                                                   \n55\n E. Ostrom, Governing the Commons: The Evolution of Institutions for Collective Action (Cambridge: \nCambridge University Press, 1990). \n\n \nThe Value of Data / policy implications - 29 \n \nDecisions are legitimated by the participation of users For individuals, consent and opt outs need to be \ninformed and viable (which requires competitive \nalternative services). \n \nOrganisations need to engage actively with \ncommunities for example through representative data \ngovernance bodies and public participation exercises. \nDecisions are also legitimated by government \nrecognition  \nA comprehensive data strategy and \ninstitutional/regulatory framework \n \nThese principles are useful for assessing the new types of institution or regulatory framework \nthat  will  be  needed  to  govern  access  to  and  use  of  data.  They  speak  to  the  asymmetries  of \ninformation and incomplete contracts characterising the data economy. Economic regulation in \nother domains is built on the extensive institutional economics literature, and the same analytical \ntools  need  to  be  brought  to  bear  here  in  designing  data  access  regulation - including  the \nmandatory data access schemes under consideration in some jurisdictions as well as voluntary \nsharing arrangements. \n \nData Infrastructure \nThese institutional and regulatory questions need to encompass the whole of data infrastructure. \nData infrastructure consists of: \n● data assets such as datasets, identifiers and registers \n● the standards and technologies used to curate and provide access to those data assets \n● the guidance and policies that inform the use and management of data assets and the \ndata infrastructure itself \n● the organisations that govern the data infrastructure \n● the communities involved in contributing to or maintaining it, and those who are affected \nby decisions that are made using it. \n \nSchemes for sharing data previously kept closed for commercial or sensitivity reasons are starting \nto become more common, aiming to create a club good for the parties involved.  \nHowever, data sharing may be limited by a coordination problem: “[D]ata producers only have an \nincentive to make data available if they think there are enough users, and users need available \ndata to get value from it. But data producers don’t know how many potential users there are, and \n\n \nThe Value of Data / policy implications - 30 \n \nusers don’t know the amount, variety or quality of data that is available. This mutual uncertainty \nimpedes data sharing.”\n56\n  \nOrganisations considering sharing data with others face a number of other considerations:  \n● if the data in question contains personal information, there is a risk that a partner sharing \nit will mistakenly disclose it and incur regulatory and reputational costs for all involved;  \n● the data could reveal insights into the workings of the firm and its intellectual property, \nor provide an advantage to commercial rivals;  \n● if  the  future  use  of  the  data  being  shared  is  unpredictable,  whether  because  of  the \ninformation it holds or as a result of its use being subject to novel technology, it is hard \nto  determine  whether  a  partner  will  invest  sufficiently  before  the  fact,  or  exit  the \narrangement at an undesirable time later on.  \n \nThese questions are examples of the classic problems of asymmetric or incomplete information, \nprincipal-agent misalignments, and the difficulty of designing contracts under uncertainty.  \nDespite the barriers, a number of initiatives sharing data are under way. These schemes have the \npotential  to  change  the  incentives  for  governing  data  access.  Schemes  for  shared  data  in  a \nbounded  space  may  change  the  returns  on  investment  in  the  collection  and  cleaning  of  data, \ncomplementary skills and assets. \nOvercoming the barriers may also require international collaboration over data infrastructure.\n57\n \nCountry surveys have revealed hesitation towards sharing sensitive data about citizens across \nborders – more than 75 percent of surveyed internet users in Indonesia, China, India, Poland \nand Mexico support the domestic storage of personal data – but accessing frontier skills for \ncreating value from data often involves working with organisations in artificial intelligence \nhubs, like Berlin, Shanghai, and Toronto.\n58\n To develop trustworthy data sharing relationships \nacross borders, national regulators could benefit from exchanging and testing ideas - the OECD \nhas recommended that they should do so early in the process of developing domestic \nregulations - while companies can learn through facilitated market entry schemes, like the \nfintech bridges run by the Financial Conduct Authority.\n59\n    \n                                                   \n56\n London Economics, “Independent assessment of the Open Data Institute’s work on data trusts and on \nthe concept of data trusts,” Report to the Open Data Institute, 2019. Accessed at: https://theodi.org/wp-\ncontent/uploads/2019/04/Datatrusts-economicfunction.pdf. \n57\n Open Data Institute, “What are the links between data infrastructure and trade?” (2019). Access at: \nhttps://docs.google.com/document/d/1NWhM50Vp_xpV4k8LmOr49VdURrRGIKqu2aVZXz1Gv5c/edit \n58\n CIGI and Ipsos, “83% of Global Internet Users Believe Affordable Access to the Internet Should be a \nBasic Human Right” (2014). Accessed at: \nhttps://www.cigionline.org/sites/default/files/documents/internet-survey-2014-factum.pdf. A. Goldfarb \nand D. Trefler, “AI and International Trade” (2017). Accessed at \nhttps://www.nber.org/chapters/c14012.pdf. \n59\n Financial Conduct Authority, “UK establishes FinTech Bridge with the Republic of Korea” (2016). \nAccessed at: https://www.fca.org.uk/news/press-releases/uk-establishes-fintech-bridge-republic-korea. \n\n \nThe Value of Data / policy implications - 31 \n \nData Trusts \nOne such approach to forming a data institution, data trusts, are being developed and trialled in \nseveral  countries.  Schemes  such  as  data  trusts  involve  making  more  complete  the  contracts \nbetween parties that have asymmetric data holdings or technology skills. \nA data trust can take a number of different forms - such as the legal trust, contractual, corporate, \npublic, and community trust models. Arguments can be made for a plurality of approaches.\n60\n They \nhave a number of aims in common:  \n● To enable data to be shared; \n● To deliver public benefits as well as benefit to those sharing the data; \n● To respect the interests of those with legal rights in the data; \n● And ensure the data is used ethically and in accordance with the rules established by the \ndata trust; \n● To ensure that whoever holds data subject to the trust rules does so safely and securely, \nand that data is dealt with appropriately (for example by deletion) if the data trust ends; \n● To manage individual rights and interests collectively (including any sharing of benefits \nreceived by the data trust); \n● To set standard rules to govern all data sharing; \n● To act as custodian/steward making decisions on behalf of data providers/ data users;  \n● And to be able to evolve to have new purposes, governance and working methods. \n \nTrustees  of  a  data  trust  may  need  to  have  powers  strong  enough  to  discourage  misuse  of  the \ndata, in line with Ostrom’s principles.\n61\n Data trusts may be able to reduce transaction costs and \nincrease efficiency, by allowing one data sharing agreement between partners rather than their \nhaving to negotiate several. They may be able to set conditions for the quality of data provided \nby  members,  perhaps  reducing  information  asymmetries.\n62\n Data  trusts  may  also  be  a  way  to \ncompensate for ‘missing markets’.\n63\n  \n                                                   \nOECD, “International Regulatory Co-operation: Addressing Global Challenges,” (2013). Accessed at: \nhttps://www.oecd.org/gov/regulatory-policy/international-regulatory-co-operation-9789264200463-\nen.htm    \n60\n S. Delacroix and N. Lawrence, “Bottom up Data Trusts: Disturbing the ‘One Size Fits All’ Approach to \nData Governance,” forthcoming in International Data Privacy Law. \n61\n Register Dynamics, “Putting the Trust in Data Trusts” (2019). Accessed at: https://www.register-\ndynamics.co.uk/data-trusts/. \n62\n P. Masons, “Data trusts: legal and governance considerations” (2019). Accessed at: \nhttps://theodi.org/wp-content/uploads/2019/04/General-legal-report-on-data-trust.pdf. \n63\n London Economics, “Independent assessment of the Open Data Institute’s work on data trusts and on \nthe concept of data trusts,” Report to the Open Data Institute (2019). Accessed at: https://theodi.org/wp-\ncontent/uploads/2019/04/Datatrusts-economicfunction.pdf. \n\n \nThe Value of Data / policy implications - 32 \n \nThe  Open  Data  Institute  has  piloted  data  trusts  based  on  contractual  relationships  between \nparties for sharing energy and mobility data in London, data about the illegal wildlife trade, and \ndata about food waste; while Sidewalk Labs has used a data trust in its approach to the collection \nand use of data in an area of Toronto.\n64\n  \n \nOther data sharing models \nOther approaches have also been adopted, either directly sharing datasets, pooling data through \nportals, or establishing platforms as mediators between providers and users of data.  \nOne recent example is Databox, a multi-partner project funded by the EPSRC. It gives individuals \ncontrol  over  the  data  they  provide,  including  data  increasingly  being  generated  by  Internet  of \nThings  devices  such as  smart  thermostats  and  meters.\n65\n The  data  is  held  in  a  physical  device \ncontrolled  by  the  individual,  rather  than  in  the  cloud,  using  ‘containerisation’  technology. \nAccording to a Royal Academy of Engineering Report, “Consumers will be able to obtain insights \nfrom their own data, while commercial organisations will have access to a greater range of data \nsources  of  appropriate  type  or  granularity,  enabling  richer  and  more  accurate  analytics.”\n66\n The \nDatabox  mediates  access  to  the  source  of  data  but  does  not  hold  it.  Individuals  can  give \npermission to third party app developers to access specific data. When the developer has used \nthe data, the service can be provided to the individual without continuing to store data.  \nData sharing in the UK energy industry has been mandated by the Government as part of the roll-\nout of smart meters. The Data Communications Company manages the smart meter infrastructure \nincluding  data,  licensed  by  Ofgem.\n67\n The  in-home  meter  is  linked  to  the  telecommunications \nnetwork enabling consumer data to be shared with competing energy suppliers, energy network \noperators and other authorised parties, such as third party intermediaries that offer energy saving, \nswitching or load shifting services. Consumers are asked to authorise the use of their data. The \ninfrastructure  could  potentially  be  extended:  “Smart  metering  equipment  could  potentially  be \n                                                   \n64\n See Open Data Institute, “Greater London Authority and Royal Borough of Greenwich pilot: What \nhappened when we applied a data trust” (2019). Accessed at: \nhttps://theodi.org/?post_type=article&p=7891; Open Data Institute, “Illegal wildlife trade pilot: What \nhappened when we applied a data trust” (2019). Accessed at: \nhttps://theodi.org/?post_type=article&p=7890; Open Data Institute, “Food waste pilot: What happened \nwhen we applied a data trust” (2019). Accessed at: https://theodi.org/?post_type=article&p=7889; \nSidewalk Labs, “An Update on Data Governance for Sidewalk Toronto” (2018). Accessed at: \nhttps://www.sidewalklabs.com/blog/an-update-on-data-governance-for-sidewalk-toronto/. \n65\n Databox Project (2019), https://www.databoxproject.uk/.  \n66\n Royal Academy of Engineering, “Databox: allowing individuals to control how they share data with \nother parties.” Accessed on 20 January 2019 at: http://reports.raeng.org.uk/datasharing/case-study-1-\ndatabox/. \n67\n Data Communications Company, “What we do”, Accessed at: https://www.smartdcc.co.uk/.  \n\n \nThe Value of Data / policy implications - 33 \n \nused  to  collect  property  information,  such  as  temperature  or  humidity  measurements,  to  spot \nwhere there are health risks to vulnerable people.”\n68\n  \nAnother  example  of  data  sharing  by  private  companies  required  by  government  followed \nlegislation  (the  2017  Bus  Services  Act\n69\n)  mandating  bus  operators  to  share  information.  The \nDepartment  for  Transport  created  the  Bus  Open  Data  Portal  and  established  standards and \nformats. \nInstances of existing private data sharing models not mandated by legal or regulatory compliance \ninclude a DAFNI, a database and model repository for infrastructure providers; examples of ‘open \ninnovation’  platforms  such  as  APROCONE  in  aerospace  or  Goldcorp’s  then-startling  (in  2000) \nopening of its proprietary geological database to invite outsiders to help locate gold deposits; \nand Strava Metro, which provides GPS tracking data from the Strava fitness app free to individuals \nand  under  licence to  other  users.  In  these  examples,  the  incentives  for  data  sharing  vary,  but \nthere  are  clear  benefits  in  each  case  to  the  companies  sharing  data:  respectively,  lower  cost \nmonitoring  and  enhanced  resilience  of  infrastructure  assets,  design  improvements  along  the \nsupply chain, access to problem-solving resources, and building a reputation and customer base.  \nAn  alternative  approach  is  Tim  Berners-Lee’s  initiative  Solid,\n70\n which  centres  on  individuals \ncontrolling their own data, including terms of access and storage, in a decentralized model, in \nother  words  not  involving  any  centralizing  institutions.  Users  store  data  in  one  or  more  ‘pods’ \n(personal online data stores) hosted by an entity they can select, and they can permit different \norganisations to access data of different types. Solid’s focus is therefore on individuals owning \ndata they generate, and on safeguarding privacy. In other words, it is concerned with reducing \nnegative  data  externalities  from  loss  of  privacy;  to  capture  the  potential  social  value  from \nrealising  positive  externalities,  the  services  and  apps  using  data  need  to  accumulate  access \npermissions from individual users. \nAlthough  experience  over  time  of  using  models  of  sharing  may  enhance  trust  and  encourage \ngrowing participation, many shared data spaces - including most of the examples above -  have \nrequired regulatory intervention. If the benefits of sharing are asymmetric, if the costs of building \nand  maintaining  a  pool  or  platform  are  high,  if  there  are  concerns  about  loss  of  competitive \nadvantage, or fears of regulatory or legal breaches due to handling sensitive individual data, a \npolicy intervention will be required. Enabling the creation and capture of value from data, from \nnew business opportunities and economic growth to improvements in public services and non-\nmarket gains, will require new policy approaches. \n  \n                                                   \n68\n Royal Academy of Engineering, “Smart Meters: Data Sharing in the Energy Industry,” Accessed at: \nhttp://reports.raeng.org.uk/datasharing/case-study-7-smart-meters/  \n69\n See: https://www.gov.uk/government/publications/bus-services-act-2017-new-powers-and-\nopportunities \n70\n See: https://solid.mit.edu/  \n\n \nThe Value of Data / policy implications - 34 \n \nTRANSPORT \n \nThe  transport  sector  in  the  UK  illustrates  a  range  of  the  issues  discussed  here,  specifically  on \nopen and shared data, and models for public-private sector partnerships. These issues have had \ndifferent implications for value created and its distribution. \nSome public transportation and geospatial data is open and free. For instance, Highways England \nmakes important data freely available to developers via an API.\n71\n  The Geospatial Commission \nhas launched a Single Data Exploration Licence (although users may need to purchase some of \nthe data they identify).\n72\n   \nWith the public bus system,  it  has  been  a  question  of  enforcing  sharing  of  information.  In  the \nearly 2000s, the Department of Transport (DfT) required the use of company data to inform public \ntransit systems, improving services by providing users with more up to date information on buses’ \ntimetables,  routes  and  fares.  Bus  companies  had  to  share  access  to  their  real  time  operations, \nwhich also could allow for more effective monitoring of their performance. Transport Direct was \nset  up  as  a  distinct  entity  used  by  DfT  to  implement  the  opening  up  of  company  databases. \nOpening the data also created opportunities for other companies to create interfaces to inform \nthe public about their transport options in real time.  \nData does not have to be shared to improve outcomes for the public. For example, in response \nto a daily congestion charge on private hire vehicles in central London, Uber introduced a model \nin April 2019 that automatically adds £1 to every trip that passes through the congestion charge \nzone, regardless of time of day. At Heathrow airport, airport management has discussed the use \nof geo-fencing to regulate private vehicle use around Heathrow.\n73\n Here, Uber has agreed to place \nlimits on drivers to avoid congestion around Heathrow itself, using its internal dynamic pricing \nalgorithm. In these two instances, Uber utilises its ability to adjust demand by altering the cost \nto the user, responding to the user’s preferences. The end outcome on emissions and traffic is \nachieved through efforts within a private company rather than public sector regulation.  \nHowever, this latter case shows the capture of value from the use of data which is not shared. \nWith  the  London  congestion  charge,  depending  on  the  number  of  rides  and  timing,  Uber  can \ncollect the difference between its internal £1 congestion charge per ride and TfL’s £11.50 charge \nper day. The distribution of revenue between the private company and public sector is known to \nUber,  but  not  necessarily  to  TfL.  Similarly,  while  Uber  is  helping  Heathrow  Airport  manage \ncongestion, it alone is able to in effect implement a private congestion charge borne by drivers \nand passengers.  \n                                                   \n71\n See: http://webtris.highwaysengland.co.uk/api/swagger/ui/index  \n72\n See: https://www.gov.uk/government/news/geospatial-commission-making-geospatial-data-more-\naccessible  \n73\n Heathrow Press Release, “Heathrow Chairman: We can reduce freight emissions and still increase our \ncargo capacity,” 12 October 2017. Accessed at: \nhttps://mediacentre.heathrow.com/pressrelease/details/81/Corporate-operational-24/8878; G. Paton, \n“Heathrow crackdown to beat minicab congestion,” The Times, 25 June 2019. Accessed at: \nhttps://www.thetimes.co.uk/article/heathrow-crackdown-to-beat-minicab-congestion-fhzgfzlqk \n\n \nThe Value of Data / policy implications - 35 \n \nYet private companies rely on the public sector to maintain the roads and public transport. The \npublic  sector  remains  responsible  for  the  base  map  and  road  infrastructure.  Importantly,  the \nordnance survey in the UK owns the coordinates system upon which transport services map their \nactivities.  Licences  are  required  to  use  this  base  map,  enabling  information  on  the  location  of \nbuses and so forth can be plotted. Nonetheless, the distribution of value, as private firms use data \nto manage transport services or traffic, is not necessarily equally, or at all, shared with the public \nsector.  Service  improvements  through  the  use  of  data  require  negotiation  not  only  about  how \ninformation is shared but also how revenue is captured and distributed. \nIssues around context, value capture and sharing data will become more pressing as the use of \ndata for transport evolves. Autonomous vehicles illustrate the point made earlier that use affects \nthe  marginal  cost  and  benefit  of  collecting  more  data.  Autonomous  vehicles  require  a \nclassification model to identify and respond to different objects; the amount of data required for \nthis model will reach a point at which diminishing returns set in. On the other hand, autonomous \nvehicles  also  require  a  base  map  of  the  world  requiring  ever  more  accurate  and  detailed \ninformation. Second, they show that sharing all data might not be needed to create value. Rather, \nrunning  autonomous  vehicles  depends  on  access  to  specific  data  at  the  moment  when  it  is \nneeded.  Third,  they  also  reveal  some  of  the  challenges  around  interdependencies  and  the \ndistribution  of  value.  Autonomous  vehicles  will  rely  on  the  base  map  and  on  road  networks \nmaintained by the public sector. Regulation - to ensure that some of the cost of providing this \npart of the data infrastructure is recouped by the public sector - will affect the market price of \nuse of autonomous vehicles.  \n \n \nGENERATING VALUE FROM DATA IN HEALTH  \n \nThe use of data in the health sector in the UK highlights the debates around the potential costs \nand benefits of data use.  \nThere is a huge opportunity to use health sector data to improve social welfare. There are two \nprimary  areas  of  added  value:  1)  benefits  through  research  and  development;  and  2)  direct \nbenefits to clinical practice. These ends are often considered in relation to two supplementary \naims:  financial  benefits  to  health  providers,  and  commercial  benefits  to  the  private  sector. \nOutcomes can be seen as complementary and feeding into one another, or they can be seen in \ncompetition. In the UK, the National Health Service presents an important opportunity to harness \ndata to public welfare ends, providing a comprehensive, longitudinal patient dataset. Equally, it \nfaces key challenges to effectively realising value for the public.  \nThere are important challenges tied to the processes and context in which data is made useful, \nand  the  characteristics  of  the  data  itself.  Value  creation  often  lies  in  combining  datasets,  but \nthere are huge costs in linking up datasets, and transitioning the NHS to a consistent electronic \nsystem.  In  England,  there  are  more  than  200  clinical  commissioning  providers  and  150  trusts. \nSome  trusts  continue  to  rely  heavily  on  paper  records.  Further,  datasets  themselves  are  of \nvariable  quality,  differ  in  how  they  are  generated  and  recorded,  and  bring  important  concerns \nabout  sensitivity.  Access  rights,  often  couched  in  the  language  of  ownership,  are  central  in \nthinking  about  the  creation  of  value,  and  the  role  of  different  actors  in  processes,  including \n\n \nThe Value of Data / policy implications - 36 \n \npatients,  clinicians,  researchers,  trusts,  and  private  firms.  Some  argue  patients  must  have \n“meaningful agency” over data collected on them as individuals; value partially belongs to the \nindividuals  who  generate  it.\n74\n  Others  caveat  this.  Safeguarding  may  make  it  more  difficult  to \nintegrate sets of patient data, which could impact on the value created\n75\n. Additionally, data on \none individual has little value except in combination.  \nMaking use of data in the health sector tends to work through partnerships. Commercial partners \nplay a role in the ecosystem of drug development. For example, Health Data Research UK has a \nset  of  health  data  research  hubs  in  particular  disease  areas  that  involve  universities and \ncommercial  partners.  The  cost  and  procedural  requirements  for  cleaning,  anonymising  and \nlinking  health  data  mean  that  it  has  been  difficult  to  design  a  feasible  system  that  does  not \ninvolve commercial firms in investment and process. The leeway of commercial firms to direct \ndata  use,  however,  is  limited.  In  mid-2019,  a  decision  was  taken  that  commercial  agreements \ncould not grant any one organisation exclusive use of raw data held by NHS organisations.  \nThe role of partnerships also triggers debate over the distribution of value between private firms \nand the NHS. These debates are driven by the examples that have already occurred in practice. \nIn some, the price of data access has been tied to willingness to pay. Intermediary companies, \nwhich mediate access to clean and anonymise data after its approved release by a Trust, set a \nprice  for  access.  Others  argue  NHS  data  should  be  freely  available,  but  on  condition  that  the \nproducts  produced  through  it  are  accessible  back  to  the  NHS.  Our  interviews  emphasised  the \nimportance of the public getting the benefits of value created from their data in the domain of \nhealth in particular. \nCurrently, no single institution in the UK is responsible for governing how NHS data is used. The \nOffice of the Life Sciences supports research and industry development. NHSX and DHSC focus \non  patient  care.  NHSX  was  created  to  provide  overall  strategic  direction  for  efforts  to  digitise \nhealthcare. Finally, attention to trustworthy institutions, a wider finding in this report, will likely \nremain a key concern when it comes to balancing trade-offs around value creation, capture and \ndistribution,  particularly  recognising  the  role  of  the  individual  patient  and  private  sector \ncompanies the data and processes that generate value.  \n                                                   \n74\n For example, J. Powles\tand\tH.\tHodson,\t“Google\tDeepMind\tand\thealthcare\tin\tan\tage\tof\t\nalgorithms”,\tHealth\tand\ttechnology\t7,\tno.\t4\t(2017):\t351-367. \n75\n P. Groves, et al., \"The 'big data' revolution in healthcare: Accelerating value and innovation,\" McKinsey, \nCenter for US Health System Reform, Business Technology Office (2016). \n\n \nThe Value of Data / policy implications - 37 \n \nPolicy issues and recommendations \n \nThere  are  substantial  barriers  to  the  increased  provision  of  shared  data.  These  include  the \nchallenge of funding public goods with their cost structure of high initial but low marginal costs, \nand the trade off between wide availability of data and incentives to invest in its creation and \nprovision,  in  both  public  and  private  sector.  Furthermore,  the  benefits  created  by  additional \nprovision and sharing may be asymmetric, or costs may be imposed on the data holder in terms \nof  loss  of  commercial  advantage  or  additional  risks.  There  are  also  significant  concerns  about \nprivacy.  Finally,  regulation  and  the  design  of  an  appropriate  institutional  framework  needs  to \naddress significant asymmetries of information and principal-agent problems. \nYet the potential economic benefits to society as a whole - not just a handful of commercial firms \n- of further data sharing and use are large. The basic economic principles point to the scope for \ngains  from  additional  data  provision and  sharing  if  privacy  concerns  can  be  overcome,  and  a \ntrustworthy institutional and regulatory framework established. The possibility of demonstrable \nwidely-shared gains will be a precondition for trustworthiness.  \nWe have analysed the social welfare value of data seen through two lenses: its basic economic \ncharacteristics (pp4-7) and its contextual, informational content (pp8-13): \n  \nECONOMIC LENS INFORMATION LENS \nExcludability Subject \nExternalities (pos and neg) Generality \nIncreasing/decreasing returns Temporal coverage \nOption value Quality \nHigh fixed, low marginal costs Sensitivity \nComplementary investments Interoperability/linkability \n \nBoth lenses need to be applied to understand the value of data. They provide the questions to \nbe posed in thinking about the specifics of any given dataset.  \nOur  analysis  of  the  actual  and  potential  social  welfare - society-wide  economic  value - in  the \ndata economy underlines the following principles: \n \n\n \nThe Value of Data / policy implications - 38 \n \n Market transactions alone will not bring about the maximum social welfare from data, \ngiven  its  economic  characteristics  of  (positive  and  negative)  externalities  and  non-\nrivalry; \n  \n A more fruitful framing of the policy debate in order to generate increased social welfare \nfrom data, fairly shared, will be in terms of access rights and privacy protection, rather \nthan ownership of personal data; \n  \n Appropriate  institutional  and  regulatory  structures  will  be  vital  for  a  thriving  data \neconomy, regulating the permissions different types of entity have to access different \ntypes  of  data  and  monitoring  and  enforcing  compliance.  Work  on  the  principles  and \nstructures of data governance for the maximum social welfare is in its early days and \nmuch  more  thought  needs  to  be  given  to  the  specifics  of  regulatory  and  institutional \ndesign; \n  \n New, trustworthy institutional structures are needed to develop to enable access to data \nin  ways  that  make  possible  the  creation  of  both  commercial  and  wider  social  value, \nbuilding on a range of approaches and pilots currently under way;  \n  \n Policymakers should recognise that the legal and regulatory framework they establish \nwill affect both market and non-market values of data - the value of data is endogenous \nto the institutional framework; \n  \n Additional approaches to quantified economic valuations, incorporating social welfare \nbeyond  private,  market-based  valuations,  have  limitations  but  will  help  improve \nunderstanding of the transactions taking place, particularly involving publicly-held data \ntransactions  with  commercial  organisations.  In  domains  such  as  transport  and  health \nthere is currently no public confidence that the terms of the deals will benefit the public. \nIn addition to greater transparency, better understanding of data value is necessary; \n  \n There are significant policy trade-offs including: between creating adequate incentives \nto invest in creating and maintaining data and related services on the one hand and the \nsocial value of widely diffused use on the other; and, for public bodies, between short-\nterm financial gain from selling exclusive data access to the private sector, and long-\nterm economic and social gain from more open access; \n  \n Contracts for data use are incomplete, and the regulatory framework should recognise \nthis, particularly that schemes for sharing data in a regulated way change the returns on \ninvestment in collecting and cleaning data, and in complementary skills and assets. The \ninstitutional  and  regulatory  economics  literature  has  many  potential  lessons  for  data \nregulation. \n \n\n \nThe Value of Data / policy implications - 39 \n \nThe detailed work required to flesh out these principles is beyond the scope of this report. The \ntable below sets out some of the policy detail needing to be addressed: \n \nTrade-off between investment/innovation and \nopen/shared data \nAre there parallels with IP frameworks - patent pools - \nor is this too complex? Compulsory licensing or \nfranchising? Co-production rights? Is legal title to \n‘personal data’ sufficient for privacy or are there better \nways to protect privacy? \nLessons from regulatory economics literature. \nFinancing data provision Business models in the private sector; commercial \nmodels in the public sector. What charging mechanisms \nincentivise provision and also maximise social welfare? \nAre co-operative models relevant? \nEnabling competition & growth Codes of conduct applied to APIs (Application \nProgramming Interfaces); common technical standards \nneeded. \nWhat privately-held data sharing needs to be \nmandated? \nRegulatory thickets Clearer guidance on sharing sensitive data (by public \nand private sectors) - overcoming the fear of breach of \nGDPR, fines. \nModels for communicating data use and access rights \ne.g. is there a parallel with simplicity of Creative \nCommons licences? \nTerms of trade in public sector deals Should public agencies ever grant exclusive licences to \ndata? Data sharing as a licensing requirement e.g. for \nride shares, smart city data, autonomous vehicles. Time \nlimited licences. Are there lessons from spectrum \nauctions?  \nGreater transparency needed for trust. \nMandating data provision/sharing by the private or \npublic sector?  \nWhen is this needed? To what extent is Open Banking a \nmodel - for big tech companies? For NHS? Should \npublic sector reference data all be open? \nInstitutions Good models/metaphors? Trusts, pools, platforms, pods. \nWhat regulation/legislation is needed to establish a \ntrustworthy framework. What forms of accountability \nare needed in both public and private sectors?  \n \nThis  report  has  set  out  a  framework  for  thinking  about  how  to  increase  value - in  the  broad \neconomic sense of social welfare - in the data economy. Social welfare will be maximised by the \nability to use data involving positive externalities and new options (while minimising negative \n\n \nThe Value of Data / policy implications - 40 \n \nconsequences with regard to privacy), or in other words by identifying the potential for joining \nup data, creating new uses. The two lenses described here - economic characteristics (pp4-7) and \ninformation characteristics (pp8-13) - help identify which types of data may prove most valuable, \nand why - and also the potential risks. One of the key lessons from this report is that data is very \nfar from being homogeneous; seeing different data through our two lenses will help understand \nvariations in value. \nOne of the concerns about the data economy is that big incumbent companies might continue to \ncapture as private profit a large proportion of the value being created. They certainly have the \ngreatest capacity to undertake the investment and deploy the specialist skills needed. However, \npreventing them from using data to provide valued services would be counterproductive. A more \neffective way of bringing about a more even sharing of the economic welfare created by data use \nwould  be  the  direct  approach  of  using  competition  policy to  open  the  data-driven  markets  to \nother providers. Thinking about the potential social welfare arising from the use of data requires \nconsideration of the distributional issues, and while this is beyond our scope here, the lenses and \ntrade-offs we describe offer a way to begin to think about who may benefit from different types \nof  data,  and  about  the  potential  public  good  that  could  be  generated.  In  the  absence  of  an \nappropriate policy framework, the benefits are likely to be distributed unequally.  \nThis  requires  policies  addressing  the  challenges  described  in  this  report  in  a  systematic  way. \nConsiderable work is needed to fill out the details of the framework set out here. Four avenues \nfor future research and development work stand out.  \n \n One is attempting quantification, as sketched out here, in some specific data domains; \nmodels from financial economics may be useful, or further work on contingent valuation \napproaches. \n  \n A second is translating the economic and information lenses into a practical toolkit or \ndecision tree, particularly for the use of public sector organisations. \n  \n Third is the need to develop trustworthy institutional structures with public legitimacy, \nand to consider how this relates to the legal framework in different jurisdictions. \n  \n Finally, the challenges of regulatory and institutional design in a context of information \nasymmetries, principal-agent problems and pervasive externalities are a problem that \nthe  large  body  of  work  in  institutional  and  regulatory  economics  ought  to be  able  to \naddress. \n \nThese latter two in particular will help address the distributional consequences of investment in \ndata and its uses.  \nIn this report we have identified that there are substantial challenges to creating a thriving data \neconomy: \n\n \nThe Value of Data / policy implications - 41 \n \n● how to fund data as a public good, when it may need large up front investment \n● how to incentivise investment in data  \n● how the benefits that do arise from using data should be fairly distributed \n● how to compensate those who steward data for the costs and risks they take  \n● how to gain value from aggregated personal data while respecting people’s privacy \n● how to ensure data can be linked and combined to create positive externalities \n● how to keep options open for potential future uses of data \n \nMeeting these challenges calls for a strategic approach to data policymaking. \n \n Incentivise investment without disincentivising sharing \nThere is a trade-off between wider access to data and incentivising investment in the \ncreation  of  data  and  services.  Exclusive  access  can  give commercial  advantages. \nPolicymakers should re-examine existing legislation on intellectual property rights for \ndata and consider other approaches such as time-limited exclusive rights, patent pools \nor compulsory licensing. \n  \n \n Limit exclusive access to public sector data \nSelling exclusive data access to public sector data provides a short-term financial gain \nbut  more  open  access  will  usually  provide  greater  long-term  benefits.  Policymakers \nshould explore when exclusive access to public sector data is lawful and necessary. They \nshould build confidence that deals involving public sector data will benefit the public, \ndevelop financial models that enable different types of users to access the data, and be \ntransparent about commercial deals. \n  \n \n Use competition policy to distribute value \nBig incumbent companies currently capture a large proportion of the value of data as \nprivate  profit.  They  can  invest  in  collecting  and  using  data  and  in  specialist  skills. \nHowever,  preventing  them  from  using  data  to  provide  valued  services  would  be \ncounterproductive.  We  recommend  using  competition  policy  to  open  data-driven \nmarkets to other providers. \n  \n \n Explore mandating access to private sector data \nPolicymakers  should  examine  areas  where  mandating  access  to privately  held  data \ncould enable innovation, competition and growth in priority policy areas. Public bodies \nshould  explore  increasing  access  to  data  through  procurement  contracts  and  when \nissuing licences to operate services. Regulators should support and instigate initiatives \nthat standardise access to data, as they have with Open Banking. \n  \n\n \nThe Value of Data / policy implications - 42 \n \n Provide a trustworthy institutional and regulatory environment \nThe value data has is dependent on the environment in which it exists. Institutions are \nneeded  to regulate  who  has  access  to  data,  monitor  impact,  and  enforce  compliance \nwith   regulation,   technical   standards   and   codes   of   conduct.   The   Information \nCommissioner’s  Office,  sector-specific  regulators,  professional  bodies  and  industry \nassociations all have a role to play. New data institutions may also be needed to create \nwider commercial and social value from data. \n  \n \n Simplify data regulation and licensing \nComplex   and   overlapping   regulation   and   intricate   licensing   schemes   create \nuncertainties  that  hold  back organisations  from  using  and  sharing  data.  Existing \nregulation should be simplified, new regulation should be coherent, and clear guidance \nshould be provided. \n  \n \n Monitor impacts and iterate \nChanging  the  institutional  and  regulatory  environment  for  data  will  also  change  the \nreturn  on  investment  for  collecting  and  cleaning  data,  and  in  the  skills,  software  and \nother  resources  etc  that  help  organisations  make  the  most  of  data.  These  knock-on \neffects should be monitored. Experimentation in sectors or regions is useful for building \nevidence of what works. \n \nThe  precautionary  principle  is  often  applied  when  there  are  unknown  future  risks.  In \ndata  policy,  where  there  are  unknown  future  opportunities, we  would  argue  an \noptionality principle should also apply. As the UK develops its National Data Strategy \nand makes investments in data, the data economy and AI, it should create the conditions \nfor  greater  access,  sharing  and  use  of  data,  within  a  framework of  regulation  and \ntrustworthy institutions. \n \n \n \n \n \n  \n\n \nThe Value of Data / policy implications - 43 \n \nNote on Methods \n \nThis report is based on: \n13 semi-structured interviews, transcribed, and then coded using MaxQDA Qualitative Analysis \nSoftware, and a coding scheme based on the project design plus axial coding to highlight \nadditional relevant insights (p44); \nThree workshops with our advisory group (p44); \nFeedback on the interim report from discussants and conference participants in a session on \ndata at the American Social Science Associations annual conference in San Diego, January \n2020; \nA literature review, available on the Valuing Data project page. \nOur own analysis of the economic and information characteristics, drawing on large bodies of \nrelevant literature. \n \n  \n\n \nThe Value of Data / policy implications - 44 \n \nAdvisory group members \n \nAzeem Azhar, Exponential View \nJoshua Ballantyne, DCMS \nClaire Craig, Queens College, Oxford \nCatherine Dennison, Nuffield Foundation \nRay Eitel-Porter, Accenture \nJonathan Haskel, Bank of England MPC \nHerman Heyns, Anmut \nRichard Heys, ONS \nEd Humpherson, Office of Statistics Regulation \nFrank Kelly, University of Cambridge \nDavid Knight, DCMS \nRannia Leontaridi, BEIS \nWendy Li,  US Bureau of Economic Analysis \nStephen Lorimer, Greater London Authority Sergi Martorell, glass.ai \nDavid Nguyen, NIESR \nReema Patel, Ada Lovelace Institute \nCharles Price, HM Treasury \nMarshall Reinsdorf, IMF  \nChris Riley, Mozilla Foundation \nEric Salem, Office for Life Sciences \nInterviewees  \n \nAndrew Dilnot, Nuffield College, University of Oxford \nHerman Heyns, Anmut \nRichard Heys, ONS \nFrank Kelly, University of Cambridge \nDerek McAuley, University of Nottingham \nSergi Martorell, glass.ai \nRichard Mortier, University of Cambridge \nEric Salem, Office for Life Sciences \nTom Smith, Data Science Campus, ONS \nJohn Taysom, Privitar \nPaul Taylor, UCL Institute for Health Informatics \nPatrick Vallance, Government Chief Scientific Officer \nHal Varian, Google \n\n \nThe Value of Data / policy implications - 45 \n \n \nIn  addition  to  our  advisors  and  interviewees,  we  are  grateful  to  the  following  people  for  their \nhelpful  comments  and  discussions:  Vasco  Carvalho,  Jennifer  Cobbe,  Bill  Janeway,  Brian  Kahin, \nMichael Kenny, Louise Sheiner, and participants at a discussion hosted by Anmut.  \n \n \n \nWe are grateful to the Nuffield Foundation for funding this project, Valuing Data: Foundations \nfor Data Policy under grant number WEL/43956. \nThe Nuffield Foundation is an endowed charitable trust that aims to improve social well-being \nin  the  widest  sense.  It  funds  research  and  innovation  in  education  and  social  policy  and  also \nworks to build capacity in education, science and social science research. The Nuffield Foundation \nhas  funded  this  project,  but  the  views  expressed  are those  of  the  authors  and  not  necessarily \nthose of the Foundation. More information is available at www.nuffieldfoundation.org. \n \n \n  \n\n \n \nAbout the Bennett Institute for Public Policy \nThe Bennett Institute for Public Policy, established in 2018, conducts high-level academic and \npolicy research, as well as expanding the portfolio of public policy education and training \noffered at the University of Cambridge. The institute aims to become a world-leader in \nachieving successful and sustainable solutions to some of the most pressing problems of our \ntime. bennettinstitute.cam.ac.uk \nAbout the ODI \nThe Open Data Institute is an independent, non-profit, nonpartisan company headquartered in \nLondon. The ODI was co-founded in 2012 by the inventor of the web Sir Tim Berners-Lee and \nArtificial Intelligence expert Sir Nigel Shadbolt to advocate for the innovative use of data to \naffect positive change across the globe. The ODI works with companies and governments to \nbuild an open, trustworthy data ecosystem, where people can make better decisions using data \nand manage any harmful impacts. theodi.org \nAbout the Nuffield Foundation \nThe Nuffield Foundation is an independent charitable trust with a mission to advance social \nwell-being. It funds research that informs social policy, primarily in Education, Welfare, and \nJustice. It also funds student programmes that provide opportunities for young people to \ndevelop skills in quantitative and scientific methods. The Nuffield Foundation is the founder \nand co-funder of the Nuffield Council on Bioethics and the Ada Lovelace Institute. The \nFoundation has funded this project, but the views expressed are those of the authors and not \nnecessarily the Foundation. nuffieldfoundation.org \n \n \nThe Bennett Institute for Public Policy \nDepartment of Politics and International Studies \nAlison Richard Building \n7 West Road \nCambridge, CB3 9DT \n \n \n \n ","version":"1.10.100"}