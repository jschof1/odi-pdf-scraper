{"numpages":30,"numrender":30,"info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"Adobe InDesign CC 2014 (Macintosh)","Producer":"Adobe PDF Library 11.0","CreationDate":"D:20150121093738Z","ModDate":"D:20150121093739Z","Trapped":{"name":"False"}},"metadata":{"_metadata":{"xmp:createdate":"2015-01-21T09:37:38Z","xmp:metadatadate":"2015-01-21T09:37:39Z","xmp:modifydate":"2015-01-21T09:37:39Z","xmp:creatortool":"Adobe InDesign CC 2014 (Macintosh)","xmpmm:instanceid":"uuid:bc99249b-958f-024d-b99b-9cfb4e077f4e","xmpmm:originaldocumentid":"xmp.did:694c5aed-16f0-4c50-acb8-a13435c78d17","xmpmm:documentid":"xmp.id:2c10066d-215a-415b-926a-9ced53d75c82","xmpmm:renditionclass":"proof:pdf","xmpmm:derivedfrom":"xmp.iid:18616e78-244f-4d21-a939-f1c3e3091adfxmp.did:9790e060-0908-4a8d-aaff-73884c3544f9xmp.did:694c5aed-16f0-4c50-acb8-a13435c78d17default","xmpmm:history":"convertedfrom application/x-indesign to application/pdfAdobe InDesign CC 2014 (Macintosh)/2015-01-21T09:37:38Z","dc:format":"application/pdf","pdf:producer":"Adobe PDF Library 11.0","pdf:trapped":"False"}},"text":"\n\n\n\n\n\nBenchmarking open data automatically | Open Data Institute 20153\nAs open data becomes more widespread and useful, so does the need for effective ways \nto analyse it. \nBenchmarking open data means evaluating and ranking countries, organisations and \nprojects, based on how well they use open data in different ways. The process can \nimprove accountability and emphasise best practices among open data projects. It \nalso allows us to understand and communicate how best to use open data for solving \nproblems. Future research and benchmarking exercises will need to happen on a larger \nscale, at higher frequency and less cost to match the rising demands for evidence. \nThis paper explores individual dimensions of open data research, and assesses how \nfeasible it would be to conduct automated assessments of them. The four dimensions \nexamined are: open data’s context/environment, data, use, and impact. They are \ntaken from the Common Assessment Methods for Open Data (CAF),\n1\n  a  standardised  \nmethodology for rigorous open data analysis. The paper proposes a comprehensive set \nof ideal constructs and metrics that could be measured for benchmarking open data: \nfrom the existence of laws and licensing as a measure of context, to access to education \nas a measure of impact.\nRecognising that not all of these suggestions are feasible, the paper goes on to make \npractical recommendations for researchers, developers and policy-makers about how \nto put automated assessment of open data into practice:\n1. Introduce automated assessments of open data quality, e.g. on timeliness, where data \nand metadata are available.\n2. Integrate the automated use of global performance indicators, e.g. internet freedoms, \nto understand open data’s context and environment.\n3. When planning open data projects, consider how their design may allow for automated \nassessments from the outset.\nImproving automatic assessment methods for open data may increase its quality and \nreach, and therefore help to enhance its social, environmental and economic value \naround the world. For example, putting an emphasis on metadata may ensure that data \npublishers spend enough time on preparing the data before their release. This paper will \nhelp organisations apply benchmarking methods at larger scale, with lower cost and \nhigher frequency.\nExecutive summary\n\nBenchmarking open data automatically | Open Data Institute 20154\nThis paper is part of a series produced by the Open Data Institute, as part of the Partnership for \nOpen Data (POD), funded by the World Bank.\nWhat is open data?\nOpen data is data that is made available by governments, businesses and individuals for anyone \nto access, use and share. \nWhat is the Open Data Institute?\nThe Open Data Institute (ODI) is an independent, non-profit and non-partisan company based in \nLondon, UK. The ODI convenes world-class experts from industry, government and academia \nto collaborate, incubate, nurture and explore new ideas to promote innovation with open data. \nIt was founded by Sir Tim Berners-Lee and Professor Sir Nigel Shadbolt and offers training, \nmembership, research and strategic advice for organisations looking to explore the possibilities \nof open data.\nIn its first two years, the ODI has helped to unlock over US$55m in value through the application \nof open data. With 24 nodes around the world, the ODI has trained more than 500 people from \nover 25 countries. In 2014, the ODI trained officials from countries including Botswana, Burkina \nFaso, Chile, Malaysia, Mexico, Moldova, Kyrgyzstan and the UK on the publication and use of \nopen data. \nWhat is the Partnership for Open Data?\nThe Open Data Institute has joined Open Knowledge and the World Bank in the Partnership \nfor Open Data (POD), a programme designed to help policy-makers and citizens in developing \ncountries to understand and exploit the benefits of open data. The partnership aims to: support \ndeveloping countries to plan, execute and run open data initiatives; increase reuse of open \ndata in developing countries; and grow the base of evidence on the impact of open data for \ndevelopment. The initial funding comes from The World Bank’s Development Grant Facility (WB \nDGF). Under POD, the ODI has carried out open data readiness assessments, strategic advice, \ntraining and technical assistance for low- and middle-income countries across four continents. \nIn 2015, POD will merge with the Open Data for Development (OD4D) network. As part of this \nnew, larger network, the ODI will continue to take a lead in supporting the world’s government \nleaders in implementing open data, and in doing so will continue to publish practical guides \nand learning materials, such as this series of reports.\n\nBenchmarking open data automatically | Open Data Institute 20145\n1. Introduction to benchmarking open data6\n2. Adopting the Common Assessment Framework for open data7\n3. How feasible are automated metrics for the Common Assessment \nFramework?\n8\n4. The ideal approach for benchmarking open data\n4.1. Context/Environment: measuring the effect of context and \nenvironment on open data\n4.2 Data: measuring the nature and quality of open data\n4.3 Use: measuring how and why open data is being used\n4.4 Impact: measuring the benefits of open data\n12\n5. Towards a pragmatic, automated approach to benchmarking open \ndata\n5.1 Measuring context/environment: the scope for automation\n5.2 Measuring data quality: the scope for automation\n5.3 Measuring data use: the scope for automation\n5.4 Measuring data impact: the scope for automation\n22\n6. Recommendations for benchmarking organisations25\nGlossary28\nEndnotes29\nTable of contents\n\nBenchmarking open data automatically | Open Data Institute 20156\n1. Introduction to benchmarking open data\nThere is a global shift towards governments and organisations publishing more open data –  that \nis, data made available for anyone to access, use and share. For example, datacatalogs.org, \na meta-list of data portals, counts 390 catalogues across the world.\n2\n  The Open Government \nPartnership has grown from eight participating countries to 65.\n3\n In its latest iteration, the Open \nData Barometer, a regular survey of government open data readiness, implementation and \nimpact run by the Web Foundation and the Open Data Institute in 2013, targets more than 80 \ncountries.\n4\nPolicy-makers, civil society groups and businesses demand quantitative evidence for the \npromised benefits of open data. Many benchmarking efforts are trying to meet these demands. \nBenchmarking open data means evaluating and ranking countries, organisations and projects \nbased on how well they use open data. The process of benchmarking can improve accountability \nand emphasise best practices among existing open data projects. Table 1 lists several examples \nof leading open data benchmarking studies.\nTable 1. Examples of open data benchmarking studies\nStudyOrganisationDescription\nE-Gov Survey/\nIndex \n5\nUnited \nNations Public \nAdministration \nNetwork\nUN  E-Gov  Survey  analyses  e-government  and \ne-participation in member states including looking at \nthe publishing of open government data and open data \ninitiatives.\nGlobal Open \nData index \n6\nOpen Data \nCensus/Open \nKnowledge\nOpen Data Census explores the openness of a specific \nset of key government datasets for countries around \nthe world and its Global Open Data Index provides an \nannual global score comparison between them.\nOpen Data \nBarometer \n7\nWeb \nFoundation \n& Open Data \nInstitute\nOpen  Data  Barometer  measures  the  distribution  and  \nimpact of open government data policies and practices \naround  the  world,  using  multidimensional  analysis \nto  score  countries’  overall  progress  in  realising  the \npotential benefits of open data.\nOpen Data \nMonitor \n8\nEuropean Union \nConsortium \n(inc. Open Data \nInstitute\nOpen Data Monitor assesses trends in the data being \npublished openly by national and regional governments \nin Europe, through automated analysis of metadata in \ndata catalogues.\n\nBenchmarking open data automatically | Open Data Institute 20157\nIsolated research efforts, however, may lead to duplication, reduce comparability and stifle \ninnovative research. Even case studies that are, by design, unique, benefit from using an \noverarching framework that embeds their results into the wider context of open data research. \nThe growing importance of open data means that future research and benchmarking exercises \nwill need to happen on a larger scale, with higher frequency and less cost. Only a quantitative \nand scalable solution can meet these requirements while factoring in subjective indicators and \ncase study research. This study explores the feasibility of conducting automated assessment \nof open data, based on the Common Assessment Framework.\n2. Adopting the Common Assessment Framework for open data\nThe Common Assessment Framework (CAF) provides a standardised methodology for a \nrigorous analysis of the supply, use and impact of open data. The first draft of the framework \nwas developed by the World Wide Web Foundation, the Governance Lab at NYU, the ODI, and \nother organisations in a workshop held in June, 2014. It aims to loosely coordinate the efforts \nof researchers and organisations in designing comparable and complementary research.\n9\n The \nCAF builds on many of the existing open data benchmarking tools and processes. \nThe full framework, available in the appendix, consists of four conceptual dimensions:\n1. Context/Environment: the context within which open data is being provided. This might \nbe national, in the case of central government’s open data, or more specific, in a particular \nsector such as health, education or transport.  \n2. Data: the nature and quality of open datasets, i.e. their legal, technical and social openness, \nrelevance and quality. The framework also looks to identify core categories of data that might \nbe evaluated in assessments.\n3. Use: the types of users accessing data, the purposes for which the data is used and the \nactivities being undertaken to use it.\n4. Impact: the benefits gained from using specific open datasets, or from open data initiatives \nin general. Benefits can be studied according to social, environmental, political/governance, \nand economic/commercial dimensions.\nWithin each of these dimensions are a number of subcomponents. For example, ‘impact’ \n\nBenchmarking open data automatically | Open Data Institute 20158\nis split by social, environment, political/governance and economic/commercial categories. \nSubcomponents are, furthermore, expanded by core questions which aim to direct researchers \ntoward specific aspects to be addressed. For instance, within the ‘social’ subcomponent of \n‘impact’, comes the question “How can open data be used to increase equality, target resources \nto citizens, and improve public services?” The framework also lists both examples of potential \nindicators and existing benchmarking projects. \n3. How feasible are automated metrics for the Common Assessment \nFramework?\nThe four high level dimensions of the CAF vary widely in their potential for automation. Figure 3.1 \nprovides a conceptual overview of which dimensions are easy to quantify, given the availability \nof data. They are presented as a hierarchy, based on their potential for automation, but this \nmay simplify the implementation for some scenarios. \nFigure 3.1. Feasibility and comparability of the four dimensions, under ideal circumstances\nFeasibility refers to potential application of an automated assessment given ideal availability \nof data, metadata or corpus such as up-to-date available legislative records or high update \nfrequency of a dataset in a machine-readable format.\nComparability refers to the idiosyncratic nature of the dimension, namely how readily the \nautomated assessment may be generalised across other countries, times or domains. User \nstatistics for Transport for London’s open data, for example, may be applicable to other large \nurban agglomerations but limited in other respects. Licences for datasets, especially based \non Creative Commons, ought to be globally comparable.\n\nBenchmarking open data automatically | Open Data Institute 20159\nTable  3.1  provides  a  brief  introduction  to  each  of  the  dimensions,  an  overview  of  the  \ncurrent  approaches  in  each  and  their  potential  for  automation.  This  concise  analysis  \nallows us to moderate our expectations of the potential for automation in each of the \ndimensions.\nContext/Environment\nThe context within which \nopen data is being \nprovided. This might be \nthe national context in \nthe case of central Open \nGovernment Data, or \nmight be the context in \na particular sector such \nas health, education or \ntransport.\nWhen  publishers  release  digital  information  to  a  high  \nstandard, automated assessments of it are more likely to \nwork well. Careful consideration should go into validating \nhow   meaningful   the   chosen   metrics   and   applicable   \nmetrics are for open data.\nCurrent  approaches: Existing  benchmarking  organisations \nprovide  a  range  of  different  measures  around  the  context/\nenvironment of open data. The majority of these are qualitative \nstatements  collected  through  surveys  and  interviews. \nHowever, a few do draw upon quantitative metrics associated  \nwith some global performance indicators.\nPotential for automation: There is plenty of scope to develop \nsolid quantitative metrics, especially those based or derived \nfrom  global  performance  indices  and  national  government \nindicators. Automation is highly feasible for a given technical \nlevel,  for  example  legislation  published  on  the  web.  Some \nmethodological  questions  persist,  such  as  determining  the  \ncausal impact of open data beyond mere correlations.\nData\nThe nature and qualities of \nopen datasets. Including \nthe legal, technical \nand social openness of \ndata, and issues of data \nrelevance and quality. The \nframework also looks to \nidentify core categories \nof data that might be \nevaluated in assessments.\nAutomated  analyses  of  datasets  themselves  are  already  \na developed aspect of open data benchmarking, but they \ndepend on high-quality metadata. \nCurrent approaches: Quantitative metrics such as download \nstatistics are in theory available to open data benchmarking \norganisations,  but  are  not  necessarily  consistent  or \nimplemented.  Automated  assessment  implementations  are \nbeing researched through projects like OpenDataMonitor.\n10\n \nPotential  for  automation: Given  the  quantitative  nature \nof  data  portals  and  metadata,  data  benchmarking  is  the  \ndimension with the highest potential. It is, however, subject \nto  the  existence  of  high-quality  metadata  in  a  consistent, \nstandardised and complete format. An additional requirement \nmay be that datasets are organised in international, national \nor local data portals.\n\nBenchmarking open data automatically | Open Data Institute 201510\nUse\nHow is data being used \nand with what possible \noutcomes? The framework \nlooks at the category of \nusers accessing data, the \npurposes for which the \ndata will be used, and the \nactivities being undertaken. \nThis part of the framework \naddresses the ‘who, what \nand why’ of open data in \nuse. \nAssessing how open data is used is feasible for specific \ncases or applications, but assessing less straightforward \nuse, like secondary reuse of data, poses many challenges.\nCurrent  approaches: Use  of  open  data  is,  at  least  in  the  \nfirst  instance,  quantifiable  through  the  collection  of  access \nstatistics  of  applications,  portals  and  datasets.  Many \nbenchmarking organisations actively track the details of use \nthrough surveys, interviews and/or case studies.\nPotential  for  automation: Automation  is  highly  applicable \nto the primary use of data, subject again to metadata and \nimplementation.  The  scope  for  the  automated  assessment \nof  the  purpose  of  use  and  reuse  throughout  the  open  data  \necosystem, however, may be limited. Well-designed systems \nmay be able to quantify uptake (‘who’) and outcomes (‘what’) \nin certain domains. ‘Why’ people use open data is difficult \nto  observe  through  behaviours  and  therefore  may  not  be \nmeasurable through automated assessments.\nImpact\nThe benefits gained \nfrom using specific \nopen datasets, or from \nopen data initiatives in \ngeneral. Benefits can \nbe studied according \nto social, environmental, \npolitical/governance, and \neconomic/commercial \ndimensions.\nAssessing the impact of open data with automated metrics \nis  difficult,  both  conceptually  and  practically.  Justifying \nthe causal link between open data and its impact is, while \nnot impossible, a challenging methodological task.\nCurrent  approaches: To  the  best  of  our  knowledge,  there \nare  hardly  any  automated  metrics  that  measure  the  wider \neconomic, social or environmental impacts of open data. Some \nbenchmarking organisations, e.g. the Open Data Barometer, \nattempt to quantify impact through proxy measures, yet they \nare typically a comprehensive and costly study. If anywhere, \nthe most promising candidates for measuring impact through \nautomated metrics are found in highly specific use cases.\nPotential  for  automation: Economic,  and  to  a  lesser  extent \nsocial,  political/governance  and  environmental,  impact  are \nin principle quantifiable with an ideal provision of open data. \nIn practice, any automated metrics face the question of how \nmuch change can be attributed to open data initiatives. The \nkey here is to establish a credible link between metrics and \nputative impact.\n3.2 Barriers to introducing automated metrics\nBeyond the specific limitations set out above, there are universal barriers to introducing automated \nmetrics. These barriers, laid out in Table 3.2, apply to many scenarios because they represent \nmore general issues that people experience when working with data. They provide a conceptual \noverview that should inform the scope and potential of any specific open data project.\n\nBenchmarking open data automatically | Open Data Institute 201511\nTable 3.2. Barriers to automated metrics\nAvailability\nDoes the data exist?\nQuantitative methods rely on the existence of relevant and valid \ndata. The most basic barrier of automated assessment is the lack \nof data. For example, if no download statistics are available, use \nis hard to track retrospectively. \nRecommendation\nResearchers should consider automated methods during early \nstages of project design. For example, implement tools or site \nanalytics that capture usage data.\nData quality\nIs the data good enough?\nData quality spans a range of issues. It may refer to machine-\nreadable  properties,  completeness,  timeliness,  and  so  forth. \nFor example, assessing how up-to-date open data is requires \nthe  metadata  to  include  dates  and  update  frequencies  that  are  \nstandardised. \nRecommendation\nResearchers,  developers  and  policy-makers  should  adhere  to \ncommon data standards as much as possible. For example, data \npublishers may refer to the Open Data Certificate.\nValidity of quantitative \nmetrics\nIs the data meaningful?\nNumbers on a dashboard may not necessarily reflect its intended \npurpose. It is crucial to keep in mind that quantitative metrics are \nnever neutral and carry the implicit decisions by the researcher. \nFor  example,  tracking  the  number  of  datasets  in  a  national \ncatalogue may tell us about the maturity of the country, but often \nis not a useful proxy for the completeness of open data because \neven a large amount may miss strategic datasets.\nRecommendation\nChoosing meaningful metrics requires thinking of the context in \nwhich they appear. Researchers should be open to a pragmatic \napproach,  but  remain  critical  of  it  and  carry  out  revisions  if \nnecessary.\n\nBenchmarking open data automatically | Open Data Institute 201512\n4. The ideal approach for benchmarking open data\nThis section proposes a comprehensive, yet non-exhaustive, set of idealised constructs  and \nmetrics.\n11\n While some of them might not be practical, they are intended to help guide future \nbenchmarking efforts. They do not sketch out an ideal automated benchmarking or policy \ntool. This could be achieved by weighting and aggregating them into an index, but is beyond \nthe scope of this work. Each of the dimensions is represented by a section (Sections 4.1-4.4) \nwhere a table (Tables 4.1-4.4) lists the proposed constructs for each subcomponent with a \nnumber of illustrative metrics.\n4.1 Context/Environment: measuring the effect of context and environment on \nopen data\nMeasuring the effect of context and environment on open data requires a broad examination of \nthe legal, technical and organisational context and the environment in which open data is used. \nTable 4.1. List of proposed constructs to assess data context and environment\nCAF subcomponentConstructs and idealised metrics\nLegal and \nregulatory\nOpen data licensing provision\n• Existence of open licensing framework and policy\n• Textual analysis of licences\n• List of compatible licences\nFunctioning right-to-information (RTI) framework\n• Existence of RTI laws\n• Ratio of requests made to information granted \n• Mean time taken for request to be granted\nFunctioning public sector information (PSI) reuse policy\n• Existence of law and policy on PSI reuse\n• Statistics on the ease of reuse\n• Extent  of  adoption  of  open  data  legal  and  regulatory \nstandards\nInternet freedoms, privacy and restriction laws\n• Existence of internet privacy/restriction law and policy\n• Textual analysis of privacy/digital communication laws\n• Score of internet freedoms \n\nBenchmarking open data automatically | Open Data Institute 201513\nCAF subcomponentConstructs and idealised metrics\nOrganisational\nType/structure of organisations involved\n• Full  lists  of  businesses,  government  bodies  and  civil \nsociety actors using open data\n• Number of city/regional open data initiatives\n• Count of open data startup incubators\n• Existence/count/size of open data portals\n• Number/size of intermediary open data organisations\nRoles of organisations involved\n• Network analysis of open data actors\nMaturity of the existing open data ecosystem\n• Count of open data actors\n• Number of people or platforms reached by open data \n• Number  of  organisations  involved  by  date  they  started \nusing open data\nContinuity of open data usage\n• Rate of uptake in the use/publishing of open data per year\n• Measure of continual usage of open data \nPolitical will/ \nLeadership\nCommitment to transparency\n• Government transparency index\n• Measure of centrality of openness in policy\nGovernment data/technology context\n• Measure of the centrality of technology/data to government \npolicy\n• Level of government online service provision \n• Percentage of government documents that are digitised\n• Existence and strength of information management policy\n• Count of government data roles/positions (high level and \noverall) \nEngagement  of  government  with  other  actors  around  open  \ndata\n• Existence of information/data consultations \n• Measure  of  responsiveness  of  policy  to  consultation \nprocesses\n• Level of engagement between agencies and developers\nGovernment promotion of open data goals\n• Textual   analysis   of   government   communications \n(speeches/press releases/publications) for key words\n• Count/percentage of government departments/agencies \nreleasing open data\n• Extent/strength of promotion of PSI reuse \n\nBenchmarking open data automatically | Open Data Institute 201514\nCAF subcomponentConstructs and idealised metrics\nTechnicalSkills and resources\n• Number of data/computer science graduates\n• Level of data literacy in civil service/government \nTraining and education\n• Number of education courses around data/technical skills\n• Number  of  educational  modules  mentioning  data \nmanagement or computer science skills\n• Textual analysis of school curricula for data training\nTechnical infrastructure\n• Extent of technology uptake, for example:\n• access to internet\n• access to fibre optic\n• number of mobile phone users \n• number of smartphone users\n• Cost of technology relative to basic goods\n• Level  of  government  and  private  sector  investment  in \ndata/technology infrastructure\nSocialWider social context\n• Media freedom score\n• Media plurality/diversity\n• Analysis of social media surrounding open data\n• Civil liberties/political freedoms \nEngagement of civil society\n• Number/size of civil society/community/grassroots \norganisations using and/or promoting open data\n• Level of data literacy amongst population\nWill and leadership within civil society\n• Strength/size of academic output in the field of open data, \ne.g. number of papers/citations \n• Existence of infomediaries\n• Clout of civil society open data champions \n\nBenchmarking open data automatically | Open Data Institute 201515\nCAF subcomponentConstructs and idealised metrics\nEconomicWider economic context\n• Level  of  investment  in  technological  innovation  from \ngovernment and private sector\n• Percentage contribution of technology industry to GDP\n• Early stage funding for startups\nCapacity and support\n• Demand/supply for data science/technical positions \n• Level of funding for open data initiatives \n• Firm-level technology absorption\n• Count/size of hackathon/hackday events\n• Count/size of open data marketplaces\nWill and leadership within the private sector\n• Count of private sector open data champions \n• Count  of  private  sector  data  roles/positions  (high  level \nand overall)\n• Number of businesses using/seeking/demanding data\n4.2 Data: measuring the nature and quality of open data\nIn 2007, a group of open government advocates drafted a set of eight principles of open \ngovernment data (OGD).\n12\n For practical reasons, not all of these principles may be assessed \nin an automated fashion. The list in Table 4.2 goes into more detail. More information about \ntechnical aspects of the automated assessment of data catalogues can be found in the reports \nof the OpenDataMonitor project.\n13\nTable 4.2. List of proposed constructs for data\nCAF subcomponentConstructs and idealised metrics\nDefinitions and \ndimensions\nPrimary relates to the source of the data. What level of aggregation \nis appropriate, how to define the original source or how to assess the \nrawness of data are difficult questions beyond automatic metrics.\n• Total  number  of  data  catalogues  (more  is  not  necessarily \nbetter, depending on context)\n• Proportion of dataset distributions in each catalogue that are \nnot listed in any other catalogues\nAccessibility can be automated for many technical aspects. For \nexample, the distribution of data formats or the number of languages \nin a catalogue are usually easy to measure. Other, perhaps social \naspects, are more difficult to quantify. \n• Frequency of dataset distributions with previews\n• Frequency of different languages\n\nBenchmarking open data automatically | Open Data Institute 201516\nCAF subcomponentConstructs and idealised metrics\nNon-discriminatory: Measuring if data is available to anyone, with \nno requirement of registration may be trivial if each data catalogue \nfollows a standard policy. If not, it may still be possible to measure \nthe extent to which open data is available without discrimination via \nother metadata.\n• Proportion of datasets available only via an API\n• Proportion  of  datasets  available  in  a  human-readable  file \nformat \nMachine-readable: It is fairly straightforward to assess all individual \ndatasets on the extent to which they are machine-readable. However, \nmany  details  may  require  manual  input  and/or  only  emerge  as \nproblematic in an actual application. For example, metadata may \nbe machine-readable on a basic level but not include a meaningful \nschema. \n• Frequency of dataset distributions that are machine-readable\n• Frequency of error and warnings generated by, for example, \nCSVlint (http://csvlint.io, for CSV files)\nNon-proprietary: Measuring the range of data formats is usually \nfeasible in an automated fashion. The openness of different formats \nhas been measured, for example, with Tim Berners-Lee’s 5 stars of \nopen data. \n• Frequency of catalogues using specific software platforms\n• Frequency of dataset distributions by file format\nLicence-free: If  each  dataset  includes  an  appropriate  piece  of \ninformation  regarding  its  licence,  and  the  number  of  licences  is  \nlimited, it may be possible to measure the extent data is available \nwith an open licence. \n• Frequency of dataset distributions with an explicitly set license\n• Frequency of datasets distributions with an open license\nClassification / \nSectors of datasets\nSectors of datasets\n• Comparison of published datasets in a sector against list \nof key sector datasets, for example, based on the Global \nOpen Data Index\n14\n• Cluster analysis of datasets released by sector\nHigh value datasets\n15\nCompanies \n• Company/business register\nCrime and justice \n• Crime statistics/safety\n\nBenchmarking open data automatically | Open Data Institute 201517\nCAF subcomponentConstructs and idealised metrics\nEarth observation \n• Meteorology/weather, agriculture, forestry, fishing, and \nhunting\nEducation\n• List of schools, performance of schools, digital skills\nEnergy and environment \n• Pollution levels and energy consumption\nFinance and contracts \n• Transaction spend, contracts let, call for tender, future \ntenders,  local  budget,  national  budget  (planned  and \nspent)\nGeospatial \n• Topography, postcodes, national maps, local maps\nGlobal development\n• Aid, food security, extractives, land\nGovernment accountability and democracy\n• Government contact points, election results, legislation \nand statutes, salaries (pay scales), hospitality/gifts\nHealth\n• Prescription data, performance data\nScience and research\nGenome  data,  research  and  educational  activity, \nexperiment results\nStatistics\n• National Statistics, Census, infrastructure, wealth, skills\nSocial mobility and welfare\n• Housing, health insurance and unemployment benefits\nTransport and infrastructure\n• Public transport timetables, access points broadband \npenetration\n\nBenchmarking open data automatically | Open Data Institute 201518\nCAF subcomponentConstructs and idealised metrics\nQualityCompleteness may  be  measured  automatically,  however,  any \nmetric has to be reviewed over time. The set of open data evolves \nas more is understood about its impact and usefulness. It may be \npossible to compare completeness against a pre-defined universe \nof open data (see above). \n• Frequency of catalogued datasets\n• Size of datasets and catalogues\n• Frequency of catalogues by sector of publishing organisation\nTimeliness: Up-to-date  catalogues  and  timely  data  can  be \nmeasured automatically, provided the metadata is standardised.\n• Median days since latest dataset update \n• Proportion of datasets with stated update frequency\nMetadata: i.e. data completeness, standardisation and relevance.\n• Adherence to a standard such as the Dublin Core Metadata \nInitiative (DCMI)\n• Proportion of data file links that are broken\n• Number of fields in the metadata record that are populated\n• Open Data Certificate level of the dataset\n16\n4.3 Use: measuring how and why open data is being used\nMeasuring how open data is used requires an examination of: \n• who the users of open data are,\n•    what data they are using,\n•    why they are using it, and\n•    how they are using it to inform their projects.\nTable 4.3. List of proposed constructs for use\nCAF subcomponentConstructs and idealised metrics\nUsersCurrent users\n• Number of users/download statistics for each catalogue\n• Number of users/download statistics for each dataset \n• Analysis of user demographics/sectors\n\nBenchmarking open data automatically | Open Data Institute 201519\nCAF subcomponentConstructs and idealised metrics\nPotential users\n• Profile of existing users across demographics/sectors\n• Number of users using closed government data\n• Measure of size/scope of proprietary data usage\n• Value/size/scope of proprietary data market\nNon-users\n• Affordability of data services/infrastructure for various sized \nof businesses\n• Number of actors who have stopped releasing/using open \ndata\nPurposePerceived motives\n• Percentage using open data in current field versus percentage \ntrying to enter a new field\n• Observed behaviour: increased value, lowered cost, improved \nexperience, disrupted or enhanced existing activities\n• Type of project: business/social/environmental\nAmbition and goals\n• Scale of outputs: local, national, international\n• Percentage of those who publish/report results\n• Percentage of revenue types (premium, freemium etc)\nActivitiesUses/outputs\n• Count/size of secondary open data\n• Analysis of applications and related tools\n• Type of project outputs: report, data, software etc.\nSectors\n• Sector/type of datasets most published\n• Sector/type of datasets most used\n• Sector/type of actors most involved\n• Sector/type of outputs most produced (apps, reports, etc)\n4.4 Impact: measuring the benefits of open data \nMeasuring the impact of open data is perhaps the most important and most difficult task in \nbenchmarking open data. Demonstrating social, environmental, political and economic impact \nin specific settings is of most use if it is possible to show how the impact may be generalised. \nDemonstrating impact for a wider scope depends on establishing a credible causal link between \nthe open data initiative and its putative impact. The list of challenges that open data may \nsupport spans all areas, hence the list of proposed constructs below remains high-level.\n\nBenchmarking open data automatically | Open Data Institute 201520\nTable 4.4. List of proposed constructs for impact\nCAF subcomponentHigh-level constructs\nSocialEducation\n• Access to education\n• Quality of education\n• Lifelong learning and development opportunities\nHealth\n• Combating disease and increasing life expectancy\n• Promotion of healthy lives and well-being \n• Development of the healthcare system and healthcare delivery\nHuman settlements\n• Sustainable land use, building and infrastructure planning\n• Ability to house citizens\n• Ability to manage urbanisation\nTransportation \n• Access to transportation\n• Increased efficiency of transportation\n• Transport infrastructure\nSocial development\n• Gender equality and empowerment of women\n• Protection of vulnerable society members\n• Social inequality\n• Personal financial management\n• Social and economic security\nEnvironmentalEnvironment and natural resources management\n• Preservation of the environment and habitats\n• Resilience to natural disasters and climate change\n• Sustainability\n• Pollution\nFood and water\n• Access to affordable and healthy food\n• Access to clean water\n• Sustainable agriculture\nSanitation and waste management\n• Access to proper sanitation\n• Waste management capability\n• Recycling\nEnergy \n• Renewable energy\n• Efficiency in the delivery of energy\n• Reliability of energy in homes\n\nBenchmarking open data automatically | Open Data Institute 201521\nCAF subcomponentHigh-level constructs\nPolitical/\nGovernance\nGovernmental efficiency\n• Public services\n• Reduced crime and violence\nGovernmental accountability\n• Reduced government corruption\n• Attitudinal changes toward government agencies\nCivic engagement\n• Political freedom\n• Political participation\nEconomic/\nCommercial\nEconomic prosperity\n• Innovation and entrepreneurship\n• Wealth and inequality\n• Employment and unemployment statistics\n• Job creation\n• Trade and investment\nGrowth in the open data landscape\n• Total number of open data businesses\n• Size/profit of open data businesses\n• Number of new jobs created in the (open) data sector\n• Size of tax revenue generated from open data companies\n\nBenchmarking open data automatically | Open Data Institute 201522\n5.  Towards  a  pragmatic,  automated  approach  to  benchmarking  \nopen data\nThe automation of many metrics listed above is currently not feasible. This is in part due to \nthe barriers to automation discussed in this study. It is unlikely that in the foreseeable future \nthere will be reasonable proxy measures for some constructs. There are also many broader \npractical limitations, for example, incomplete or substandard metadata, that are common in \nmany datasets. It is therefore important to manage expectations surrounding what is possible \nwith regards to automation. \nIn order to operationalise these metrics, it is necessary to identify sources of data, and, so far, \nthey fall primarily into three categories:\n1. Global Performance Indices (GPI): GPI’s are useful sources of data for automated metrics, \ngiven that they are often comprehensive in country coverage, published online, reliable and \navailable on a wide range of topics (at least 150 indices exist).\n17\n Examples of other sources \ninclude the World Bank Data,\n18\n UNdata\n19\n and OECD data\n20\n platforms.\n2.Government data: In many cases metrics rely on (open) government data for a wide range \nof information regarding its own makeup, practices and legislation. UK examples of sources \nfor such data include legislation.gov,\n21\n government announcements\n22\n and data.gov.uk.\n23\n3. Portal metadata: Portal metadata is essential for analysis of the data dimension of the \nCAF. Portals might be local, regional, national or international in scale with appropriate \ngranularity or aggregation. Portals for France, for example, include the City of Paris open \ndata,\n24\n Région Île-de-France open data,\n25\n data.gouv.fr\n26\n and EU open data.\n27\nNote: For sources 2 and 3, government data and portal metadata, there are a number of caveats to automation:\na. Tools will need to be pointed toward the relevant sources by researchers requiring an initial investment in resources. \nb. In general, automation assumes a collaboration between the data providers and excludes other forms of collection \nsuch as scraping.\nc. To be useful, the data must be relevant and of sufficient quality.\nThe next section demonstrates how new or existing benchmarking organisations can create \nautomated assessment methods measuring metrics within CAF’s four dimensions. These \nmetrics should be able to supplement and streamline existing processes in a viable and \nuseful way. \n\nBenchmarking open data automatically | Open Data Institute 201523\n5.1 Measuring context/environment: the scope for automation\nTo measure the context and environment of open data, we can often rely on existing Global \nPerformance Indices. GPIs are in many cases available for all, or nearly all, countries and \nproduced yearly, which supports their automated integration. Table 5.1 lists a few examples \nused in the Open Data Barometer.\nTable 5.1. Examples of existing metrics using GPIs mapped to CAF constructs\nConstructExample metricSource\nGovernment data/\ntechnology context\nImportance of ICT to \ngovernment vision \n(Variable 8.01)\nWorld Economic Forum \nglobal information \ntechnology reports \n28\nTechnical infrastructure\nInternet users per 100 \npeople (IT.NET.USER.P2\n29\n)\nWorld Bank Data\n30\nWider social context\nCivil liberties ratingFreedom House Political\nFreedoms and Civil \nLiberties Index\n31\nCapacity and support\nFirm-level technology \nabsorption (Variable 9.02) \nWorld Economic Forum \nGlobal Competitiveness \nIndex\n32\nTable 5.2 shows examples of data sources that are based on government open data portals. \nTable 5.2. Examples of potential sources for CAF constructs for different countries\nConstruct(s)Metric(s)Example countriesSources\nLegal and \nregulatory \nconstructs\nTextual analysis \nof laws\nKenya\nSweden\nLaws of Kenya database\n33\nLaws and regulations of \nSweden\n34\n\nBenchmarking open data automatically | Open Data Institute 201524\nConstruct(s)Metric(s)Example countriesSources\nRTI laws\nMeasure of \neffectiveness\nBrazil \nUSA\nAccess to information \nstatistics\n35\nFreedom of information \nstatistics\n36\nGovernment \npromotion of \nopen data goals\nTextual analysis \nof government \ncommunications \nAustralia\nSouth Africa\nGovernment media \nreleases\n37\nDepartment of \nCommunications \nsubscriptions\n38\n5.2 Measuring data quality: the scope for automation\nPragmatic automated metrics exist for metadata that stems from data portals such as CKAN, \nSocrata, OpenDataSoft or DataPress. A detailed implementation is the monitoring platform \nbeing developed by the OpenDataMonitor project, using analysis and visualisation techniques \nto give insights into open data deployment across Europe.\n39\n The platform harvests metadata \nfrom local, regional and national open data hubs, and includes an extensive list of automated \nmetrics.\n40\n5.3 Measuring data use: the scope for automation\nPrimary use of open data is fairly easy to quantify if the data is linked to widespread digital \nanalytics tools, and example being the site analytics of the UK data portal data.gov.uk. Metadata \nfrom portals ought to provide a simple way to monitor download and user statistics with a \nhigh granularity, for example. However, it is much more difficult to automatically assess the \nuse of open data in secondary instances such as reuse of data. In some cases, the open data \nvalue chain can be extensive.\n5.4 Measuring data impact: the scope for automation\nAs has been discussed in detail, measuring impact with an automated approach is inherently \ndifficult. Most likely, researchers will have to rely on proxy indicators because high-level \nconstructs such as reduced corruption are hard to quantify. There will be several elements in \nan open data impact evaluation that require the analytical reasoning of a researcher. In fact, \n\nBenchmarking open data automatically | Open Data Institute 201525\nthe literature on impact evaluation is vast and open data initiatives may be able to adapt many \nof the leading practices.\nThis is not to say that in some cases an automated assessment is not attainable. However, \nit is the researcher’s or organisation’s responsibility to justify why such metrics are a valid \nrepresentation of the open data impact. \n6. Recommendations for benchmarking organisations\nGiven the varied scope and nature of benchmarking organisations, our recommendations \ncan only be generalised. The lists provided in the previous sections serve as guidelines, with \nsome more concrete suggestions. Based on this analysis and previous work, more automated \nassessments should be possible in the future. Moreover, automated metrics can offer an \nopportunity for larger scale, more frequent and less expensive assessments.\nThe following recommendations are for new and existing benchmarking organisations: \n1. Introduce automated assessments of open data quality, where data and metadata \nare available\nThe analysis of data’s nature and quality has the highest feasibility for automation. Data \nare typically quantitative, in some form, and are associated with metadata, i.e. data about \ndata. This means that if data is provided in, for example, a hosting solution such as CKAN, \nSocrata, DataPress or OpenDataSoft, researchers can build automated assessments on \ntop of these standardised platforms. The OpenDataMonitor project offers examples of \nhow this works. \n2. Integrate the automated use of Global Performance Indicators (GPIs)\nIn the last decade, the availability of GPIs has risen dramatically. While many may be \nunrelated to open data, there are several that may help to understand the context and \nenvironment of open data initiatives. The advantages are that these indicators are usually \navailable for free, with regular updates, for many or all countries and based on deliberate \nmethodologies. On its own, a GPI may not be sufficient for a benchmarking approach, \nbut, as part of a wider scope, there is potential for automation.\n3. Adopt an approach that considers the automated assessment of open data early \non in their planning \n\nBenchmarking open data automatically | Open Data Institute 201526\nIn many cases, automation fails for the most basic of requirements: the availability of \ndata. Without relevant and valid data sources, there will not be automated methods. It is \ntherefore crucial for researchers, developers and policy-makers to consider automation at \nthe design phase of their projects. Small changes such as the collection of key metadata \ncan make the difference whether an automated assessment is feasible later on. In general, \nthese considerations have wider benefits, for example, putting an emphasis on metadata \nmay ensure that data publishers spend enough time on preparing the data before its \nrelease.\nWe invite researchers to share their approaches to data analysis and automation.\n41\n As the open \ndata landscape evolves, established methods will improve, proposed methods will become \nmore feasible and new methods will emerge. Research in open data, similar to open data \nitself, should therefore lead by example and stimulate the network effect of sharing leading \npractices with the community.\n\nBenchmarking open data automatically | Open Data Institute 201527\nGlossary\nMethodology box 1. What is a construct?\nThe term ‘construct’ in social research is commonly used to denote an underlying \ntheme, concept or subject that cannot be measured directly. For example, a \nconstruct was identified for the legal context/environment open data licensing \nprovisions, which may be operationalised by a list of compatible licences or a \ntextual analysis of licences. Simple constructs may be measured with one or a few \nmetrics, while more complex ones such as internet freedoms may require a whole \nbattery of indicators.\nThe quality of a metric or indicator is reflected in its construct validity. For example, \nhow well does the number of data/computer science higher level graduates reflect \nthe availability of technical skills related to open data? Validity is typically represented \nas accuracy or the degree to which an indicator measures what it purports it does. It \nrefers to how far inferences can be justified from the chosen indicators. Sometimes \nit is called a ‘labelling issue’ or how well your operationalisation reflects what you \nare trying to measure.\nYou can find more information and background on constructs and validity in: \nAlasuutari, P., Bickman, L., & Brannen, J. (Eds.). (2008). The SAGE handbook of \nsocial research methods. Sage.\n\nBenchmarking open data automatically | Open Data Institute 201528\nEndnotes\n1.  The first draft of the framework was developed by the World Wide Web Foundation, the Governance Lab at NYU, \nthe ODI, and other organisations in a workshop held in June 2014. http://opendataresearch.org/sites/default/files/posts/\nCommon%20Assessment%20Workshop%20Report.pdf\n2.  Data Catalogs, http://datacatalogs.org, accessed on 2014-11-10.\n3.  Open Government Partnership, http://www.opengovpartnership.org, accessed on 2014-12-18. \n4.  Open Data Research Network, Research Project: Open Data Barometer, http://www.opendataresearch.org/\nproject/2013/odb, accessed on 2014-12-18.\n5.  United Nations Public Administration Network (2014). UN e-Government Surveys. Available at http://www.unpan.\norg/egovkb/global_reports/08report.htm, accessed on 2014-12-18.\n6.  Open Knowledge, Open Data Census, http://census.okfn.org, accessed on 2014-12-18.\n7.  Open Data Research Network (2013). Open Data Barometer. Available at http://www.opendataresearch.org/\nbarometer, accessed on 2014-12-18.\n8.  OpenDataMonitor, http://project.opendatamonitor.eu, accessed on 2014-12-18.   \n9.  World Wide Web Foundation & GovLab. (2014). Towards common methods for assessing open data: workshop \nreport & draft framework. Available at\nhttp://opendataresearch.org/sites/default/files/posts/Common%20Assessment%20Workshop%20Report.pdf, accessed \non 2014-12-18.\n10.  OpenDataMonitor, http://project.opendatamonitor.eu/, accessed on 2014-12-18. \n11.  ODI, Open Data Certificate, https://certificates.theodi.org/, accessed on 2014-12-18.  \n12.  See methodology box 1 in the glossary for more information.\n13.  Some general indicator examples can be found here: http://www.epsiplatform.eu/content/psi-scoreboard-\nindicator-list, , accessed on 2014-12-18.\n14.  E.g. Freedom House, 2013 Global Scores https://freedomhouse.org/report/freedom-net-2013-global-scores, \naccessed on 2014-12-18.\n15.  E.g. Transparency International, 2014 Corruptions Perception Index, http://www.transparency.org/cpi2014/\nresults, accessed on 2014-12-18.\n16. E.g. Reporters Without Borders, World Press Freedom Index 2014, http://rsf.org/index2014/en-index2014.php, \naccessed on 2014-12-18 \n17.  The Annotated 8 Principles of Open Government Data, http://opengovdata.org, accessed on 2014-12-18.\n18.  OpenDataMonitor, http://project.opendatamonitor.eu/, accessed on 2014-12-18. \n19. Open Knowledge, Open Data Census, http://census.okfn.org/, accessed on 2014-12-18.\n20. Taken from the G8 open data charter, available at https://www.gov.uk/government/publications/open-data-\ncharter/g8-open-data-charter-and-technical-annex, accessed on 2014-12-18. \n21. ODI, Open Data Certificate, https://certificates.theodi.org/, accessed on 2014-12-18. \n22. A full list of indices is awaiting publication: figure drawn from Kelley, J. G., & Simmons, B. A. (2014). The Power \nof Performance Indicators: Rankings, Ratings and Reactivity in International Relations (SSRN Scholarly Paper No. \nID 2451319). Rochester, NY: Social Science Research Network. Retrieved 2014-12-18 from http://papers.ssrn.com/\nabstract=2451319\n23. World Bank, Data, http://data.worldbank.org/, accessed on 2014-12-18.\n\nBenchmarking open data automatically | Open Data Institute 201529\n24. UNdata, http://data.un.org, accessed on 2014-12-18.\n25. OECD, Data, http://data.oecd.org, accessed on 2014-12-18.\n26. Legislation.gov.uk, http://www.legislation.gov.uk, accessed on 2014-12-18. \n27. Gov.uk, Announcements, https://www.gov.uk/government/announcements, accessed on 2014-12-18. \n28. Data.go.uk, http://data.gov.uk, accessed on 2014-12-18.\n29. ParisData, http://opendata.paris.fr/, accessed on 2014-12-18.\n30. Data.Iledefrance.fr, http://data.iledefrance.fr/explore/, accessed on 2014-12-18.\n31. Data.gouv.fr, https://www.data.gouv.fr/fr/, accessed on 2014-12-18. \n32. European Union Open Data Portal, https://open-data.europa.eu/en/data/, accessed on 2014-12-18. \n33. World Economic Forum (2012). The Global Information Technology Report 2013 Data Platform. Available at http://\nwww.weforum.org/global-information-technology-report-2013-data-platform, accessed on 2014-12-18.\n34. World Bank, Internet users (per 100 people), http://data.worldbank.org/indicator/IT.NET.USER.P2, accessed on \n2014-12-18. \n35. World Bank, Data, http://data.worldbank.org/, accessed on 2014-12-18.\n36. Freedom House, Freedom in the World, https://freedomhouse.org/report-types/freedom-world, accessed on \n2014-12-18. \n37. World Economic Forum (2014). The Global Competitiveness Report 2014-2015. Available at  http://reports.\nweforum.org/global-competitiveness-report-2014-2015, accessed on 2014-12-18. \n33. Kenya Law, The Laws of Kenya, http://www.kenyalaw.org:8181/exist/kenyalex/index.xql, accessed on 2014-12-\n18. \n39. Lagrummet.se, http://www.lagrummet.se, accessed on 2014-12-18. \n40. Acessoainformacao.gov.br, http://www.acessoainformacao.gov.br, accessed on 2014-12-18.\n41. FOIA.gov, http://www.foia.gov/developer.html, accessed on 2014-12-18.\n42. Australia.gov.au, Government Media Releases, http://www.australia.gov.au/news-and-media/government-media-\nreleases, accessed on 2014-12-18\n43. Department of Communications SA, Subscriptions, http://www.gcis.gov.za/content/newsroom/subscriptions, \naccessed on 2014-12-18. \n44. OpenDataMonitor, http://project.opendatamonitor.eu, accessed on 2014-12-18.\n45. Atz, U., Heath, T., Heil, M., Hardinges, J., & Fawcett, J. (2014) Best practice visualisation, dashboard and key \nfigures report. OpenDataMonitor. Open Data Institute, London, UK. Available at  http://project.opendatamonitor.eu/\nwp-content/uploads/deliverable/OpenDataMonitor_611988_D2.3-Best-practice-visualisation,-dashboard-and-key-figures-\nreport.pdf, accessed on 2014-12-18.\n46. Data.gov.uk, Site Usage, http://data.gov.uk/data/site-usage#totals, accessed on 2014-12-18. \n47.  Please contact us at research@theodi.org.\n\nBenchmarking open data automatically | Open Data Institute 201530","version":"1.10.100"}