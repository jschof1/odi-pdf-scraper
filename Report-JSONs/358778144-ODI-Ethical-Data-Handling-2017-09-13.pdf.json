{"numpages":35,"numrender":35,"info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"Adobe InDesign CC 2017 (Macintosh)","Producer":"Adobe PDF Library 15.0","CreationDate":"D:20170913083430+01'00'","ModDate":"D:20170913083435+01'00'","Trapped":{"name":"False"}},"metadata":{"_metadata":{"xmp:createdate":"2017-09-13T08:34:30+01:00","xmp:metadatadate":"2017-09-13T08:34:35+01:00","xmp:modifydate":"2017-09-13T08:34:35+01:00","xmp:creatortool":"Adobe InDesign CC 2017 (Macintosh)","xmpmm:instanceid":"uuid:a1f585a7-f4a0-f641-bb32-c3c11b073b85","xmpmm:originaldocumentid":"xmp.did:C25894B30B206811822A9858DD7B9C3F","xmpmm:documentid":"xmp.id:7e5ccb11-17ac-4a06-9e39-fa2ca1474120","xmpmm:renditionclass":"proof:pdf","xmpmm:derivedfrom":"xmp.iid:18d955ac-0cf3-46eb-b67a-c0301e87e6a3xmp.did:f328dbcb-4fbf-44e7-bd35-1495085e0e71xmp.did:C25894B30B206811822A9858DD7B9C3Fdefault","xmpmm:history":"convertedfrom application/x-indesign to application/pdfAdobe InDesign CC 2017 (Macintosh)/2017-09-13T08:34:30+01:00","dc:format":"application/pdf","pdf:producer":"Adobe PDF Library 15.0","pdf:trapped":"False"}},"text":"\n\nHelping\norganisations\nnavigate\nethical\nconcerns in\ntheir data\npractices\nSeptember 2017\n\n2 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nAuthors: Ellen Broad, Amanda Smith and Peter Wells\nEditors: Anna Scott and Charlotte Fleming\nDesign: Adrian Philpott and Christie Brewster \nTable of contents \nExecutive summary  3\nIntroduction              5\nThe relationship between data ethics and legal compliance  5\nThe recent history of privacy laws  5\nData protection laws in Europe today  8\nIntellectual property and database laws  9\nLaws relating to confidential information     10\nLaws relating to anti-discrimination     10\nExisting data ethics frameworks  12\nA data project can involve only non-personal data and still be unethical      14\nData collection     15\nGaps in data     15\nBias in data     17\nUnlawful data collection     20\nData sharing  20\nSharing data knowing it may cause harm to individuals or society  21\nCreating unfair monopolies     22\nData use  23\nThe purpose of a data service or use  23\nManipulating people’s participation in democratic processes     25\nBias in model design     26\nResponding to errors and enabling feedback     26\nConclusion              28\nAppendix              29\nData Ethics Canvas  29\nData Ethics Canvas user guide  30\n\n3 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nExecutive summary\nData ethics is a rapidly emerging area. Increasingly, those collecting, sharing and working with \ndata are exploring the ethics of their practices and, in some cases, being forced to confront \nthose ethics in the face of public criticism.\nCodes of data ethics are being developed across sectors, ethics training is becoming more \ncommon and debates are accelerating on issues like the monetisation of personal data, bias \nin data sources and algorithms, and the consequences of under-representation in data.\nBuilding trust\nFor some time, the Open Data Institute has been working on measures to help organisations \nbuild trust in how they collect, use and share data, and to foster better use of data overall.\n1\n \nTrust is an essential component of society. When trust breaks down, the public lose faith in \nthe institutions that provide them with services, and organisations lose the ability to share data \nand collaborate in ways that could improve all our lives. \nThis  paper  starts  by  exploring  the  relationship  between  data  ethics  and  legal  compliance,  \nsome existing data ethics frameworks and ethical considerations in data collection, sharing \nand use.\nAfter  this  exploration  –  and  considering  the  challenge  of  agreeing  practical  data  ethics  \nframeworks for sectors, societies or globally that it highlights – we offer an approach, the Data \nEthics Canvas, for organisations to identify and manage data ethics considerations. \nChallenging common assumptions\nBy exploring data ethics in depth, we uncovered common assumptions about data ethics that \nwe wanted to challenge.\n1     See:     https://theodi.org/open-data-in-government-how-to-bring-about-change; \nhttps://theodi.org/blog/policy-design-patterns-that-help-you-use-data-to-create-impact;  \nhttps://theodi.org/guides/openness-principles-for-organisations-handling-personal-data\n\n4 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\n1. ‘Data  ethics  is  only  an  issue  where  activities  involve  personal  data.’  Ethics  \nissues must also be considered in the collection and use of non-personal data. For \nexample, not publishing the location of bus stops in poorer neighbourhoods can \nmean that the advantages of smartphone-mapping and route-finding services are \nnot available to people who live in those areas, increasing existing inequalities.\n2. ‘Data  ethics  is  concerned  with  data  protection  compliance.’  Complying  with  \nlegal obligations – under the EU General Data Protection Regulation, for example \n– is just one part of treating data ethically. Data-related activity can be unethical \nbut still lawful.\n3. ‘Data ethics is (only) about how organisations use data.’ Data ethics is about \nthe  impact  that  all  data  activities  have  on  people  and  society.  Collecting  and  \nsharing data only about certain groups of people may disadvantage them relative \nto others. All activities should be subject to ethical examination.   \nWe intend to raise awareness of the kinds of ethics issues – involving both personal and non-\npersonal data – that can arise from how data is collected, who it is shared with and what it \nis  used  for.  The  paper  uses  case  studies  to  explore  the  ethical  issues  that  may  arise  either  \nthrough  malicious  or  non-malicious  intent  but  due  to  a  lack  of  proper  consideration  before  \ndata is collected, shared and used. \nTo help increase the level of consideration we developed the Data Ethics Canvas, a prototype \n– for identifying potential data ethics issues associated with a data project or activity. It is based \non  the  Ethics  Canvas,\n2\n  a  tool  for  assessing  the  ethical  implications  of  any  project  designed  \nby the ADAPT Centre for Digital Content Technology (CC-BY-SA), which is itself based on the \noriginal Business Model Canvas by Alex Osterwalder.\n3\n \nWe  hope  to  see  the  Data  Ethics  Canvas  used  by  teams  starting  new  data  activities.  It  is  \ndesigned to be collaborative, involving people from across teams with different perspectives. \nWe  will  continue  to  test  and  improve  the  Data  Ethics  Canvas  in  the  months  to  come,  and  \nwould welcome feedback and ideas. The Data Ethics Canvas and a user guide are included in \nthe appendix of this report. You can write to us at policy@theodi.org.\nThis report has been delivered through a collaborative partnership between ODI and Arup, the \nglobal consultancy firm. Arup has worked with ODI since 2014 to showcase and develop the \nuse of open data.  \n2 https://www.ethicscanvas.org.\n3 https://strategyzer.com/canvas/business-model-canvas.\n\n5 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nIntroduction\nEthics help us to navigate what is right and wrong in the work we do, the decisions we make \nand the expectations we have of the institutions that impact on our lives. Across sectors – from \nmedicine, bioscience and journalism to government, research and statistics – codes of ethics \nhave been created and updated as people confront the moral problems of their time.\nThe  last  decade  has  seen  the  rise  of  an  age  of  data  abundance.  New  technologies  and  \nmechanisms for harnessing data increasingly affect our day-to-day lives – from machine-\nlearning to robotics, automated services to smart devices – making it possible for people with \ndisabilities to live more fulfilling lives or people with smartphones to more easily navigate their \ncities.\nThe  increased  amount  of  and  use  of  data  calls  into  questions  pressing  issues  of  fairness,  \nresponsibility and accountability, and whether existing legislation is fit to safeguard against \nharm to an individual or group’s privacy, welfare or physical safety.\nData ethics is a branch of ethics that is addressing these questions and shaping the context \nin  which  organisations  collecting,  using  and  sharing  data  operate.  Increasingly,  sectors  and  \norganisations are being called upon to develop their own data ethics principles, policies and \nprocesses. A number of existing data ethics frameworks and principles are explored in this \npaper. This exploration should help people to understand the current landscape and why we \nchose to develop the Data Ethics Canvas.\n\n6 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nWhat is data ethics?\nIn a paper for the Royal Society in late 2016, researchers Luciano Floridi and Mariarosaria \nToddeo define data ethics as:\n“The  branch  of  ethics  that  studies  and  evaluates  moral  problems  related  to  data \n(including generation, recording, curation, processing, dissemination, sharing and use), \nalgorithms (including artificial intelligence, artificial agents, machine learning and robots) \nand corresponding practices (including responsible innovation, programming, hacking \nand professional codes), in order to formulate and support morally good solutions (e.g. \nright conducts or right values).”\n4\nAt the ODI, we believe short and accessible definitions are necessary to include more \npeople  in  debates  that  impact  them.  We  suggest  and  use  a  different  data  ethics \ndefinition of: a  branch  of  ethics  that  evaluates  data  practices  with  the  potential  \nto adversely impact on people and society – in data collection, sharing and use. \n4 http://rsta.royalsocietypublishing.org/content/374/2083/20160360#sec-1.\n\n7 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nThe relationship between data ethics and legal compliance\nCompliance with relevant legislation and regulation is part of handling data ethically and data \nethics templates must take compliance into account. It is possible that breaking the law can be \ndone in the interests of ethics. However, in cases where ethics and societal norms contradict \nlegal  compliance  it  is  likely  that  the  law  needs  updating.  Existing  legislation  has  often  been  \ndeveloped due to prior ethical debates.\nThis section provides a high-level overview of the current legal context shaping access, use \nand sharing of data that forms part of the backdrop for any data ethics framework. It is focused \non  UK  and  European  Union  legal  frameworks.  It  includes  both  legislation  that  is  explicitly  \nrelated to data, such as data protection, and legislation that has a wider scope, such as anti-\ndiscrimination and consumer protection.\nUnderstanding  the  breadth  and  history  of  such  legislation  helps  people  working  with  data  \nethics to understand the environment and prior work that they are building on.\nThe recent history of privacy laws\nWhile  data  ethics  has  only  been  established  as  a  discrete  branch  of  ethics  in  the  last  half-\ndecade, changing attitudes to data – often in response to new technologies and innovations \n– have been influencing legal reform for nearly half a century. The evolution of data protection \nlaws reflects changing expectations of privacy and consumer control. Data security and storage \nlaws  impose  responsibilities  on  organisations  storing  sensitive  data.  Intellectual  property  \n(IP) and database laws reflect expectations of being able to extract value from investing in \ncollecting and organising data.\nIn 1980, the Office for Economic Cooperation & Development (OECD) published its Guidelines \non the Protection of Privacy and Transborder Flows of Personal Data.\n5\n The opening sentence \nof the preface remains relevant today:\n“The development of automatic data processing, which enables vast quantities of data \nto be transmitted within seconds across national frontiers, and indeed across continents, \nhas made it necessary to consider privacy protection in relation to personal data.” \n5 http://www.oecd.org/sti/ieconomy/oecdguidelinesontheprotectionofprivacyandtransborderflowsofpersonaldata.htm.\n\n8 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nThe OECD guidelines attempted to combine the existing privacy laws and build consensus \naround  basic  principles  that  would  underpin  future  legislative  intervention.  The  guidelines  \ninformed the Convention for the Protection of Individuals with regards to Automatic Processing \nof Personal Data, a 1981 Council of Europe Treaty which recognised a right to privacy within \nthe context of increased personal data flows across national borders.\n6\n The OECD principles \nwere reflected in the UK’s first Data Protection Act in 1984, and some of the OECD principles \nare still reflected in the UK and European Union’s data protection laws today.\n7\nA right to privacy also exists under human rights law. It is recognised in Article 8 of the European \nConvention on Human Rights (and reflected in the UK Human Rights Act 1998),\n8\n  entitled  a  \n‘right to respect for family and private life’. ‘Private life’ refers to things like a person’s sexuality, \nbody, personal identity, family and relationships, and their personal information. It is an open-\nended provision that also allows a number of exceptions for national security, public safety, the \neconomic wellbeing of a country, protection of health and protection of the freedoms of others. \nThe EU Charter of Fundamental Rights also recognises a right to the protection of personal \ndata (Article 8).\n9\nData protection laws in Europe today\nIn  1995,  the  European  Union  adopted  the  Data  Protection  Directive,  intended  to  unify  data  \nprotection  legislation  across  EU  member  states.\n10\n  The  Data  Protection  Directive  will  be  \nsuperseded by the General Data Protection Regulation (GDPR) when it comes into force on 25 \nMay 2018. The GDPR is designed to give European citizens greater control over personal data \nagainst a backdrop of increasing automation, expanding mobile connectivity, government and \ncorporate surveillance, and large-scale commercial hacks of sensitive personal data.\n11\n \nThe GDPR updates the rights of people (or ‘data subjects’) in how data about them is collected, \nstored, managed and used, and expands existing ones. It updates the right of data erasure \n(a  ‘right  to  be  forgotten’)  and  rights  to  data  amendment.  It  introduces  a  new  right  of  data  \nportability which allows individuals to obtain and reuse personal data about themselves and \n6 https://www.coe.int/en/web/conventions/full-list/-/conventions/treaty/108.\n7 Schedule 1, Data Protection Act 1998 (UK). Interestingly, while openness about development of data processes and \npolicies is a principle of the OECD guidelines, it is not reflected in the UK’s Data Protection Act.\n8 http://www.legislation.gov.uk/ukpga/1998/42/schedule/1.\n9 http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:12012P/TXT.\n10 http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=CELEX:31995L0046:en:html.\n11  See for example the Sony Pictures hack (2014) of employee emails, salaries, unreleased films and other employee \npersonal data; Home Depot (2014) and Target (2013) breaches of customer credit card data; Ashley Madison leak of \ncustomer records (2015).\n\n9 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nshare it for their own purposes across different services (ICO, 2017).\n12\n  The  GDPR  expands  \nthe  responsibilities  (and  liabilities)  for  organisations  managing  or  processing  personal  data.  \nUnder Article 35 of the GDPR, mandatory Data Protection Impact Assessments (DPIAs)\n13\n are \nintroduced where the processing is likely to result in a high risk to the rights and freedoms of \nindividuals (in particular when using new technologies).\n14\n Several aspects of the GDPR inform \nthis paper.\nWhile the UK has commenced negotiations to leave the European Union, the UK Government \nhas confirmed its intention to implement the GDPR and bring in a new Data Protection Bill that \nis understood to include the provisions of the GDPR.\n15\n The ODI has developed both principles \nand  guidance  for  organisations  collecting  and  processing  personal  data  that  incorporates  \nobligations under the GDPR.\n16\nIntellectual property and database laws\nIntellectual property laws can influence how organisations and individuals collect, process and \nshare data. In the EU, as well as potential copyright rights in data, the database right awards a \nproperty right for substantial investment by people and organisations in obtaining, verifying or \npresenting the contents of a database.\n17\n This restricts how organisations might access, copy, \nmonetise, amend or share data belonging to someone else. \nIn  circumstances  where  a  database  right  exists,  for  example,  an  organisation  collecting,  \nscraping  (extracting  data  from  websites)  or  otherwise  accessing  data  from  a  third  party,  \nthe  organisation  needs  to  be  sure  that  what  they  are  doing  is  lawful:  that  they  have  sought  \npermission from the data owner, or accessed the data under an open licence or other licence \nthat permits what they are trying to do. \n12 https://ico.org.uk/for-organisations/data-protection-reform/overview-of-the-gdpr/individuals-rights/the-right-to-data-\nportability.\n13  Privacy Impact Assessments (PIAs), which were already promoted by the UK Information Commissioner’s Office as best \npractice.\n14   Note: not all new technologies involve a likely high risk to individuals.\n15 https://www.gov.uk/government/news/government-to-strengthen-uk-data-protection-law.\n16 https://theodi.org/guides/openness-principles-for-organisations-handling-personal-data.\n17   Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, \nArticle 7.\n\n10 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nLaws relating to confidential information \nOn 8 June 2016 the European Parliament and Council adopted the Trade Secrets Directive, \nstandardising  corresponding  national  laws  in  member  states.\n18\n  It  covers  both  long-held,  \nstrategic information held by a company (e.g. a secret recipe or a chemical compound) and \nshort-lived information, such as the results of a marketing study or the launch date of a new \nproduct.\n19\n It is strictly concerned with circumstances in which confidential information has \nbeen misappropriated. \nLaws relating to the treatment of trade secrets and confidential information of this nature exist \nin  many  national  jurisdictions.  While  it  is  typically  rare  that  organisations  handling  data  will  \nfind themselves in breach of laws relating to trade secrets – unless they unlawfully acquired \nthat information – they are included here as part of the regulatory context shaping data ethics \ndebates. \nLaws relating to anti-discrimination\nSocietal  attitudes  towards  discrimination  and  expectations  of  accountability,  fairness  and  \nequal treatment have also evolved over time, and are reflected in varying legislation across \nsectors.\nEmployers are expected to provide a basic duty of care to employees and safe workplaces \nunder occupational health and safety laws. Consumers are protected from unfair treatment by \ngoods and service providers under consumer protection laws. Organisations and individuals \nmust take care to avoid causing harm to people or property or risk being accused of negligence, \nwhile fairness and equal treatment are reflected in anti-discrimination laws pertaining to age, \nrace, sexuality, gender, religion and disabilities. \nThe  Universal  Declaration  of  Human  Rights  (UDHR)  –  proclaimed  by  the  UN  General  \nAssembly  in  1948  –  recognises  that  every  person  is  entitled  to  economic  rights,  social  \nrights   including   education,   and   rights   to   cultural   and   political   participation   and   civil   \nliberty.  Article  2  of  the  UDHR  asserts  that  these  rights  apply  equally  to  every  person: \n“[...] without distinction of any kind, such as race, colour, sex, language, religion, political \nor other opinion, national or social origin, property, birth or other status.”\n20\n18 http://ec.europa.eu/growth/industry/intellectual-property/trade-secrets_en.\n19   Ibid.\n20   Article 2, Universal Declaration of Human Rights (1948): http://www.un.org/en/universal-declaration-human-rights/index.\n\n11 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nThe International Covenants on Economic, Social and Cultural Rights and Civil and Political \nRights (1966) affirm and expand on these rights. These covenants and other international human \nrights  instruments  have  shaped  regional  and  domestic  laws  regarding  anti-discrimination,\n21\n \nincluding the UK’s Equality Act 2010.\n22\nLaws relating to anti-discrimination reflect societal expectations of fair and equal treatment. \nThey  inform  ethical  considerations  regarding  data  collection,  sharing  and  use  –  that  data  \npractices be free of bias and inaccuracies that may lead to discrimination against people or \ngroups of people, and that in practice people are not discriminated against. While they are not \nspecific to data, they should not be forgotten by data ethics.\n21   See for example the Convention on the Rights of the Child (1989) and the Convention on the Elimination of all forms of \nRacial Discrimination (1965).\n22 http://www.legislation.gov.uk/ukpga/2010/15/contents.\n\n12 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nExisting data ethics frameworks\nTo inform the Data Ethics Canvas we considered existing data ethics frameworks and principles \nfrom  government,  research  and  industry  sectors.  There  is  much  work  being  undertaken  in  \nthis space and it will continue to evolve. This paper is not intended to look at the landscape \ncomprehensively, and we are capturing data ethics frameworks as we discover them.\nExisting data ethics frameworks explored included:\n The UK Government Data Science Ethics Framework (launched May 2016)\n23\n Ethical  guidelines  for  data  protection  and  privacy,  drafted  by  the  European  \nCommission’s FP7 working group (2009)\n24\n \n UK National Statistician’s Data Ethics Advisory Committee principles\n25\n The Jisc code of practice for educational institutions undertaking learning analytics \n(2015)\n26\n The US Farm Bureau’s privacy and security guidelines for farmer data (2016)\n27\nAcross those explored, common elements included:\n that any collection and use of data is fair and lawful \n that any personal data collected from people is collected for a specific purpose, \nand only used for that purpose (or for future uses that are not ‘incompatible’ with \nthat original purpose)\n that any collection of personal data not be excessive; simply what is required for \nmeeting that original purpose\n that  organisations  managing  personal  data  be  open  about  their  processes,  \npolicies, uses and collection of personal data \nTo a lesser extent (depending on context), some of the data ethics frameworks included:\n a  requirement  that  any  collection  and  use  of  data  be  based  on  a  clear  user  \nneed, and/or with public benefit (this is part of the UK Government Data Science \nEthics framework)\n formal  recognition  that  people  supplying  personal  data  own  the  data  that  is  \nabout  them  and  their  livelihood,  families  and  property.\n28\n  This  is  becoming  an  \n23 https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/524298/Data_science_ethics_\nframework_v1.0_for_publication__1_.pdf.\n24 http://ec.europa.eu/research/participants/data/ref/fp7/89827/privacy_en.pdf.\n25 https://www.statisticsauthority.gov.uk/national-statistician/national-statisticians-data-ethics-advisory-committee.\n26 https://www.jisc.ac.uk/guides/code-of-practice-for-learning-analytics.\n27 http://www.fb.org/tmp/uploads/PrivacyAndSecurityPrinciplesForFarmData.pdf.\n28   Complexities surrounding ‘ownership’ language with respect to personal data have been documented by the ODI in ‘How \ndo we own data?’: https://theodi.org/blog/how-do-we-own-data. ‘Ownership’ is the language used by the US Farm \nBureau in their principles. \n\n13 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nincreasing  concern  in  agriculture,  where  data  about  weather,  soil  conditions,  \npesticides used, crop yield and growth are collected from farmers, without clear \nownership rights in place.\n Recognition  that  a  person’s  choice  to  provide  personal  data  may  not  be  a  \nwholly free choice. The Farm Bureau guidelines require organisations collecting \npersonal data to explain the effects and abilities of a farmer’s decision to opt in, \nopt out or disable the availability of services and features offered by the ATP. If \nmultiple options are offered, farmers should be able to choose some, all, or none \nof the options offered. \nMany of these elements place conditions around how data is stored, processed and shared \n(and who has access), as a way to guard against unethical use. However, they do not enable \ncritical thinking about the ways in which people might be affected by a project, which validates \nwhether the conditions placed on data use are proportionate. \nThe frameworks explored are, on the whole, concerned with the ethical treatment of personal \ndata.  The  ethics  framework  that  most  applies  to  broader  data  use  is  the  UK  National  \nStatistician’s Data Ethics Advisory Committee principles.\n29\n Across most frameworks studied, \nsafeguarding privacy, empowering data subjects with control over how personal data is used, \nand protecting against data breaches were key themes. These are essential considerations for \nany project involving use of personal data.\nHowever, in our research we saw gaps in existing frameworks. We see the need to expand \nconceptions of data ethics in future ethics codes and frameworks to include:\n circumstances not involving personal data\n at any stage of a data handling activity:\n data collection\n data sharing\n data use (including the design of models and algorithms)\n issues outside of privacy and user control, such as:\n bias in data and data model design\n anti-competitive practices\n practices that reinforce inequalities or stereotypes\n practices that propagate falsehoods\n inaccuracies in data\n margin for error\n29 https://www.statisticsauthority.gov.uk/national-statistician/national-statisticians-data-ethics-advisory-committee.\n\n14 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nOrganisations navigating data ethics require tools that help them survey the impact of their \ndata  use  or  project,  to  help  understand  how  choices  made  might  be  considered  ‘right’  or  \n‘wrong’.  The  Data  Ethics  Canvas  proposes  a  new  approach,  with  impact  on  people  and  \nsociety at its core.\nA data project can involve only non-personal data and still be \nunethical\nIssues of bias, inaccuracies and inconsistencies in data can arise regardless of the nature of \nits  source,  whether  personal  or  non-personal.  Researchers  analysing  trends  in  aggregated  \nsearch-engine data, for example, must account for gaps in who has contributed to that data \n(a  predominantly  digitally  literate  population).  Identifying  and  mitigating  issues  within  data  \nsources that could negatively affect certain population demographics if left untreated is part \nof treating data ethically. \nCase study: It is more expensive to play Pokemon Go in minority neighbourhoods\n \nPokemon  (short  for  the  original  Japanese  title  of  ‘Pocket  Monsters’)  is  a  popular    \nfranchise owned by Nintendo with two other companies (Game Freak and Creatures). \nPokemon  are  imaginary  creatures  and  players  (known  as  trainers)  can  catch  these  \nanimals and use them to battle other trainers. \n \nPokemon Go is a free-to-play, location-based augmented reality game developed by \nNiantic  in  collaboration  with  Nintendo.  In  the  game,  players  use  their  mobile  device  \nGPS to locate, capture, battle and train virtual Pokemon. Players collect ‘pokeballs’ – \nneeded to catch pokemon, and therefore to play the game – from pokestops (check \npoints) and battle other pokemon at ‘gyms’, which are based on their GPS location. \nWithout access to pokestops, players must pay for pokeballs and other items. Players \nwith easy access to pokestops and gyms, on the other hand, can earn more experience \npoints and collect more items for free.\nThe locations of pokestops and gyms in Pokemon Go were derived by Niantic from \nthe locations of ‘portals’ in a previous augmented-reality GPS-based game, Ingress. \n\n15 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nLocations  for  Ingress  were  predominantly  crowd-sourced,  both  via  an  existing  \ndatabase – Historical Marker Database – and by Ingress players. The Historical Marker \nDatabase reflected contributions from approximately 3,000 volunteers across the US \nwho were mostly male. Ingress players also appeared to be mainly male, young and \nEnglish-speaking. \n30\nWhen  Pokemon  Go  was  released,  users  noted  that  pokestops  and  gyms  occurred  \nmuch   less   frequently   in   poor   neighbourhoods,   and   black   and   other   minority   \nneighbourhoods.\n31\n In New York City, for example, pokestops were densely populated \nin Manhattan but sparse in outer boroughs, particularly Brooklyn and Queens, where \nhigher  percentages  of  the  population  are  from  minorities.  Portals  were  densest  in  \nmajority white and asian neighbourhoods.\n32\nStudying the locations of Pokestops, derived from Ingress data, exposes the limitations \nof crowd-sourced data. In Pokemon Go – which has had significantly broader reach \nthan Ingress – gaps in the underlying pokestop location data stand to potentially drive \nup the cost of playing Pokemon Go for users in predominantly black, hispanic or low \nsocio-economic neighbourhoods.\nThe underlying data sources for the locations of pokestops in Pokemon Go are non-\npersonal,  however  these  geolocation  gaps  stand  to  negatively  impact  on  certain  \nPokemon  Go  users  (by  increasing  the  overall  cost  of  participation).  A  data  ethics  \nframework that helps people to identify how data use affects different demographics \ncould have helped identify this issue before the launch of the game and allowed the \ndeveloper to take steps to mitigate the issue and broaden the pool of players.\n30 http://www.miamiherald.com/news/nation-world/national/article89562297.html.\n31   Ibid.\n32   Ibid.\n\n16 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nData collection\nEthical issues that arise from data collection if not identified and mitigated may result in harm \nto individuals or a community. If you are collecting or creating a dataset for the first time – or \nusing existing data sources – consider whether any gaps, inaccuracies or bias might exist in \nyour information sources. \nGaps in data \nData omissions do not always cause direct harm, but where a limited data source is used to \ngenerate insights about a larger population or provide a service at scale, omissions can have \nnegative impacts. The Pokemon Go case study offers an example of how gaps in data sources \n– despite having limited impact on their original users – can negatively impact on members of \na community when used for a wider service. \nThe absence of certain fields or categories of information in a data source can reflect a historic \nbias.  In  the  UK,  for  example,  the  ‘prescribed  particulars’  of  a  marriage  still  only  require  the  \nfather’s name and occupation, not the mother’s.\n33\n The omission of information about a mother’s \nname and occupation has follow-on effects in fields like genealogy, sociology and history. It \nis easier for people to analyse how male occupations evolved over the centuries than it is for \nthose of females. \nWhere  data  is  crowdsourced,  gaps  in  the  representation  of  certain  population  demographics  \nneed to be taken into account. Social media feeds like Twitter and Facebook are being mined and \nanalysed for everything from forecasting influenza outbreaks\n34\n to responding to natural disasters.\n35\n \nTheir limitations as data sources from which to gain insights about a wide population are well \nknown. For example, in the US, a 2016 survey from the Pew Research Centre of adults using \nsocial media indicated that approximately 79% of US-based adults were on Facebook; 24% \non Twitter; 32% on Instagram; and 29% on LinkedIn.\n36\n Also, user demographics of each of \nthese services varied: women were more likely to use Facebook than men; Instagram users \ntended to be younger, with more women online likely to use Instagram than men; Twitter was \nmore popular among highly educated users.\n37\n33 http://theodi.org/blog/pressure-still-needed-to-put-mothers-names-on-marriage-certificates.\n34 http://currents.plos.org/outbreaks/article/twitter-improves-influenza-forecasting.\n35 http://www.iflscience.com/technology/twitter-tracks-intensity-natural-disasters.\n36 http://www.pewinternet.org/2016/11/11/social-media-update-2016.\n37   Ibid.\n\n17 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nFinally, not all adults use social media or go online. In the UK, the Office for National Statistics \nfound in 2016 that 10.2% of the population had never used the Internet.\n38\nTypically, data scientists and social researchers take the limitations of social media datasets \ninto account in their research design and the insights they present. But relying simply on social \nmedia  data  –  without  accounting  for  gaps  in  the  data  source  –  would  skew  an  emergency  \nresponse towards digitally literate, mobile, active internet users and risk negatively affecting \nvulnerable members of a population – the elderly, poor or recent migrants, for example.  \nBias in data \nThere are several ways in which bias in data can arise. Bias can result from:\n survey questions being constructed by people with a particular intent or framing\n selectively collecting data from groups with particular backgrounds\n underlying bias in people from whom data is sourced\nThe following two case studies show some of the ways in which biases can occur.\n \nCase study: Trialling facial recognition software to identify potential offenders\nA  2016  machine-learning  paper  from  Shanghai  Jiao  Tong  University  investigated  \nwhether criminality in humans could be detected based on analysis of facial features.\n39\n \nThey collected 1856 ID photos of Chinese males aged between 18 and 55, controlling \nfor certain features (no facial hair, scars etc), which was divided into subsets of ‘non-\ncriminals’  and  ‘criminals’,  respectively.  The  images  of  non-criminals  were  gleaned  \nfrom  the  internet.  ID  photos  –  not  mugshots  –  of  criminals  were  provided  by  police  \ndepartments and the ministry of public security. \nUsing machine-learning techniques, the researchers identified certain discriminating \nstructural  features  for  predicting  criminality,  such  as  lip  curvature,  eye  inner  corner  \ndistance and a ‘nose-mouth angle’. They concluded that the faces of general law-abiding \n38 https://www.ons.gov.uk/businessindustryandtrade/itandinternetindustry/bulletins/internetusers/2016.\n39 https://arxiv.org/pdf/1611.04135v1.pdf.\n\n18 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\ncitizens had a greater degree of resemblance compared with the faces of criminals and \nthat criminals’ facial features were more varied.\nSince its release, the paper has been criticised for failing to account for underlying bias \nin how the photos of offenders had been collected, and for an overreliance on machine-\nlearning techniques to infer criminality, at the expense of insights from criminology and \nphysiology.\n40\n \nThe paper does not reflect on whether the ID photos of criminals reflect wider bias in \nterms of who in Chinese society is more likely to be arrested (increasing the chances, \namong this portion of the population, that someone will be found to have committed \na crime). If a member of the population ‘looks different’, they are more likely to be \nsuspected  of  committing  a  crime.  With  those  accused  of  committing  crimes  more  \nlikely to be members of a population who ‘look different’ from the general population, \nconviction is going to reflect this skewed focus on that segment of the population.\n40 https://motherboard.vice.com/en_us/article/new-program-decides-criminality-from-facial-features.\nCase study: Developing sentencing algorithms to predict the likelihood of reoffending\nIn  the  US,  sentencing  algorithms,  called  ‘risk  assessments’,  have  been  developed  \nto predict the likelihood of further criminal behaviour and adjust offender supervision \nappropriately.\n41\n Propublica – a US-based non-profit media organisation – obtained the \nrisk scores assigned to more than 7,000 people arrested in Broward County, Florida in \n2013–2014, based on one such algorithm, and measured them against actual rates of \nreoffending. Propublica observed that of those deemed likely to reoffend, 61% were \narrested for subsequent crimes within two years.\n42\n They found that the formula was \nparticularly  likely  to  falsely  predict  black  defendants  as  future  criminals  at  twice  the  \nrate of white defendants. White defendants were more likely to be mislabelled as low-\nrisk but go on to reoffend than black defendants. \n41 https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.\n42   Ibid.  \n\n19 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nIt can be difficult for a single organisation or researcher to spot and eliminate bias. Some \nstartups  and  organisations  are  beginning  to  open  up  their  data  sources  and  the  algorithms  \nthey have created to enable feedback on whether any bias or other gaps or inaccuracies exist.  \nA  predictive  policing  startup  in  the  US,  CivicScape,  released  its  algorithms  and  data,  along  \nwith documentation detailing how its algorithm worked via Github to enable greater scrutiny. \n“By making our code and data open-source, we are inviting feedback and conversation \nabout CivicScape in the belief that many eyes make our tools better for all [...] We must \nunderstand and measure bias in crime data that can result in disparate public safety \noutcomes within a community.”\n45\nData ethics frameworks should encourage openness about how data is proposed to be used \nand include a section on ‘how can people engage with you?’ to help organisations to consider \nand describe choices made in the design of their data models and systems, what measures \nare in place to address errors and enable feedback, and whether the appeal mechanisms put \nin place are reasonable. As a result of considering these decisions, organisations might decide \nto change them.\n45 https://github.com/CivicScape/CivicScape/blob/master/evaluation_notebooks/notebooks/PreventingBias.ipynb.\nThe sentencing algorithm analysed by Propublica was designed by a for-profit company, \nNorthpointe. Their core product is a set of risk scores derived from 137 questions in \na survey, answered by defendants or extracted from criminal records. While ethnicity \nis not an explicit question in the survey, several questions posed are likely to increase \nthe risk scores among black defendants. Questions asked include: ‘Has one of your \nparents ever been sent to jail or prison?’ and ‘How many of your friends/acquaintances \nare taking drugs illegally?’ In the US, black children are more than seven times more \nlikely than white children to have a parent in prison.\n43\n The 2014 US National Survey \non Drug Use and Health indicated that as of 2014, the rate of illegal drug use among \nAfrican Americans (ages 12 and over) per month was 12.4%, above a national average \nof 10.2%.\n44\n The higher likelihood that a black defendant had a family member who was \nincarcerated and/or an acquaintance using drugs illegally influenced their risk scores \nadversely. \n43 https://www.prisonfellowship.org/resources/training-resources/family/ministry-basics/faqs-about-children-of-\nprisoners.\n44 https://www.samhsa.gov/specific-populations/racial-ethnic-minority.\n\n20 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nUnlawful data collection\nThe circumstances in which data has been collected can give rise to questions of ethics and \nquestions of legality. Data might be acquired unlawfully – in breach of intellectual property laws, \nlaws relating to confidential information, or data protection laws. Where data projects involve \npre-existing data sources, it can be difficult, if not impossible, to determine whether these \nsources were collected from lawfully – because there might be no documentation regarding \nthe origin of data sources, and because laws surrounding data collection can be complex. \nHaving access to an organisation’s commercially sensitive product data, for example, without \nany  evidence  of  permission  or  involvement  from  that  organisation,  may  indicate  that  it  has  \nbeen acquired unlawfully by someone, and influence decisions made about how that data \nshould be used.\nSome  data  collection  might  not  be  unlawful  but  can  still  be  unethical.  Instances  of  data  \ncollection relating to indigenous populations offer an example of this. In South Africa, the San \npeople have issued a code of ethics for researchers which requires (among other things) that \nconsent be granted before photos are taken or published of San individuals, and that approval \nfor research proposals be granted by the San councils before research is undertaken.\n46\n \nData sharing\nData exists on a spectrum, ranging from closed to shared to open.\nShared data is data provided to restricted organisations or individuals. Sharing data is essential \nto maximising its benefits, and can be done in a variety of ways. \n Data can be shared with a specific individual or group of individuals (for example, \na doctor might share a patient’s x-rays with a surgeon and hospital team). \n Data might be made available to certain accredited or authorised entities via a \nsecure data-sharing platform. \n Social media data might be purchased by researchers and companies hoping to \nunderstand consumer trends. \nAn  organisation  can  also  make  their  data  available  as  open  data:  data  that  anyone  can  \naccess, use and share. \n46 http://www.smithsonianmag.com/smart-news/san-people-south-africa-issue-code-ethics-researchers-180962615.\n\n21 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nFigure 1: The Data Spectrum \nHow we share data is shaped by the nature of the data in question, the circumstances in which \nit has been collected and laws that restrict its distribution. Sometimes terms and conditions \nattached  to  data  collection  –  surveys,  questionnaires,  signups  for  services  –  guarantee  that  \nindividual survey responses will not be shared. When accessing medical services, we often \nagree to our health data being shared in specific ways. Limits to how data is shared and with \nwhom  can  be  explicit  or  implicit.  Data  protection  laws,  intellectual  property  laws  and  laws  \nrelating to confidential information also shape who will have access to data. \nSharing data knowing that it may cause harm to individuals or \nsociety\nAgreeing to provide data to organisations or individuals where you are aware it could reasonably \nresult in harm creates ethics concerns. ‘Harm’ can be direct or indirect. Harm could result from \ndisadvantage or prejudice arising from how the data is used by the organisation with whom it \nis shared; it could be physical harm, if exposure of certain kinds of information poses threats to \npeople’s safety. This can occur even where the distribution of that data is lawful. Publishing the \ndetails of abortion providers – names, clinic locations and hours – in a country or region where \n\n22 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nyou know they are likely to be the target of violence may be unethical, even if the publication \nof that information is lawful. How and where the information is published is important, people \nwho need an abortion need to be able to find a provider but abortion providers need to be \nprotected from malicious actions.  \nThe  US  government  recently  passed  a  law  repealing  changes  to  online  privacy  rules  for  \nUS  citizens  introduced  by  the  previous  administration.  This  included  requirements  that  \nInternet Service Providers (ISPs) – organisations that provide access to the internet – obtain \nconsumers’  consent  before  sharing  or  selling  their  browsing  information  and  other  data.\n47\n  \nWhile ISPs in the US can now lawfully record and sell a citizen’s browsing history, to whom \nit is sold can pose ethical issues – for example, to predatory loan companies, known scam \noperations or organisations that may use a citizen’s browsing history to target, blackmail or \nunfairly affect them. Being mindful of who has access to data you have collected, what risk \nof harm or prejudice might arise from that access, and taking steps to mitigate that, is part of \ntaking responsibility for the impact that data you share may have on people and society.  In \nsome circumstances, organisations may make a decision not to collect certain kinds of data, \nso as to avoid potential misuse by others. \nCreating unfair monopolies\nUnfair  monopolies  can  arise  where  data  access  is  restricted  to  one  organisation  or  a  small  \nnumber  of  organisations,  where  it  might  otherwise  reasonably  be  shared.  Again,  whether  \nan exclusive arrangement can be said to be unethical will depend on circumstances. In the \nEuropean Union, exclusive arrangements regarding access to public sector data are prohibited \n– with some limited exceptions – by the Directive on the Re-use of Public Sector Information \n(PSI).\n48\nExclusive arrangements are prohibited under the PSI regulations because they prevent others \nfrom benefiting from public sector data and can restrict competition. The potential uses of a \ndata asset – medical research, disease prevention or natural disaster planning, for example – \nmay require it to be accessible to more than one organisation.  \n47   SJ Res 34: “A joint resolution providing for congressional disapproval under chapter 8 of title 5, United States Code, \nof the rule submitted by the Federal Communications Commission relating to “Protecting the Privacy of Customers of \nBroadband and Other Telecommunications Services”. March 28 2017: https://www.congress.gov/bill/115th-congress/\nsenate-joint-resolution/34 . Note: the original law was due to come into effect in 2017, and now no changes will take \nplace.\n48   Article 11, Directive 2003/98/EC Directive on the Reuse of Public Sector Information https://ec.europa.eu/digital-single-\nmarket/en/european-legislation-reuse-public-sector-information.\n\n23 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nIncreasingly,  private  sector  data  providers  also  provide  infrastructure  for  organisations  and  \nindividuals analysing data, with lock-in effects. In agriculture, organisations like Climate Corp \n(headquartered in San Francisco) are transitioning from providing farmers with insights based \non existing open and proprietary data sources, to providing physical sensor infrastructure in \nfields monitoring things like soil quality, moisture and temperature.\n49\n \nEthical issues may arise where the terms of service associated with physical data infrastructure \nand the data they collect – via smart meters, field sensors and smart cars – prevent people \nin  practice  from  switching  providers  and  accessing  historical  data  about  their  behaviour,  \ncollected on equipment that they own, outside of that infrastructure.\nIn some instances, the nature of a data asset and its potential uses might oblige organisations \nto make it more accessible. Organisations managing data assets that stand to significantly \nimprove lives have a duty of care to ensure it is available through public interest research or \nessential service provision. If the data concerns core public infrastructure – such as weather, \nmapping, energy, transport and health – or stands to enable powerful new discoveries, then \nthere  is  a  responsibility  to  examine  any  and  all  ethical  implications.  This  idea  underpins  the  \n1996 Bermuda Principles for the Human Genome Project, which argued genomic data ‘should \nbe freely available and in the public domain, in order to encourage research and development \nand to maximise its benefit to society’.\n50\nData ethics frameworks should prompt debate about the potential creation of data monopolies \nand encourage open debate as to whether they are necessary or appropriate for the particular \ncontext. \nData use\nThe purpose of a data service or use\nSome data analytics services can have negative impacts on certain demographics. Data about \na person’s online browsing habits, their geographical location and social media interactions \nare frequently used for targeted advertising, including to the most vulnerable. Advertisements \ntargeting people with poor credit offering payday loans, and people with limited job opportunities \nbeing offered expensive courses at for-profit colleges have been well documented.\n51\n \n49 http://www.climateinsights.com.\n50  Kaye, J., Keeney, C. et al. (2009). Data Sharing in Genomics: Re-shaping Scientific Practice. In Nat Rev Genet. 2009 May; \n10(5): 331–335: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2672783.\n51   See for example Cathy O’Neil’s ‘Weapons of Math Destruction’ (2016).\n\n24 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nHow people – even those not directly impacted – might perceive the purpose of a data use, \neven where the use is not purposefully exploitative, should be taken into account by teams \nnavigating  the  ethical  considerations  of  a  project.  Public  opposition  to  reported  stories  of  \ndata use can drive expectations of ‘ethical’ organisations. Even where organisations believe \nthat what they are undertaking is lawful and ethical – perhaps even with permission from the \npeople their data model or service affects – others might disagree. Reflecting on how that \npurpose might be perceived by others helps organisations to avoid accusations of unethical \nbehaviour will help them achieve their purpose. \nCase study: Unroll.me faces backlash for its data commercialisation model\nUnroll.me is a US-based startup offering a free tool for managing subscription emails. \nIn  April  2017,  details  included  in  a  New  York  Times  story  about  the  ways  in  which  \nUber used data collected by Unroll.me for commercial intelligence caused a backlash \namong their users.\n52\n Users allow Unroll.me to access their emails and email histories \nas part of Unroll.me’s service. Retail data analytics company and owner of Unroll.me, \nSlice Intelligence, sells anonymised email data from Unroll.me to businesses looking \nfor insight into the services people access and their purchasing habits.\nDespite  agreeing  to  Unroll.me’s  terms  and  conditions  on  accessing  their  inbox  tool,  \nUnroll.me users and potential users were seemingly unaware that their email histories \nwere being monetised by Slice Intelligence and sold. People announced boycotts of \nUnroll.me on social media, sparking an apology from Unroll.me’s CEO, Jojo Hedaya. \n53\n Hedaya noted that while Unroll.me’s Terms of Service Agreement and Privacy Policy \nreferenced these practices, they may not have been properly reviewed by customers. \nHedaya committed to clearer messaging about data use in their application, on their \nwebsite and in FAQs. \nWhile the purposes for which Unroll.me used their customer data were outlined in their \nterms of service agreement, this did not alter the perception that their behaviour was \nunethical, when news of how Unroll.me data was used by Uber came to light. The root \nof  this  perception  was  a  sense  that  Unroll.me  did  not  clearly  disclose  to  their  users  \nhow email data would be used and who would have access to it. Unroll.me may have \nassumed that these uses were acceptable to users based on the terms of service. \n52 https://mobile.nytimes.com/2017/04/23/technology/travis-kalanick-pushes-uber-and-himself-to-the-precipice.html.\n53 http://blog.unroll.me/we-can-do-better.\n\n25 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nTaking into account how a use of data may be perceived, even where permission for \nthat use had been sought, can help organisations navigating the boundaries between \nethical and unethical use.\nIn many instances, it will be hard to determine with certainty whether the purpose for a data \nservice, model or analysis is ethical. Taking into account how people might perceive it, and \nwhether people may be harmed by it, can help organisations to identify potential areas of risk.\nManipulating people’s participation in democratic processes\nHow  information  is  tailored  to  individuals  and  groups  online  (to  distort  their  perception  and  \nreinforce biases) is coming under increasing scrutiny. Using personal data about an individual’s \nsocial  network,  their  interests  and  online  browsing  habits  to  selectively  present  information  \ncan have damaging consequences, particularly where individuals are not aware that it is their \npersonal data that is shaping the information they see. \nIt  has  been  reported  that  Cambridge  Analytica  used  data  modelling  and  psychographic  \nprofiling during the 2016 US election and the UK Brexit referendum to segment populations \nand selectively provide political information. This has led the UK Office of the Information \nCommissioner to initiate an inquiry into data misuse in politics.\n54\n \nSegmenting  potential  voters  based  on  their  interests,  their  geographic  location  and  other  \ndemographic  information  has  long  been  part  of  political  practices  like  door  knocking,  mail  \ndrops and issue campaigning. It has driven the success of platforms like Nation Builder, which \ncreates action-focused websites with causes that people can donate to\n55\n and organisations \nlike Blue State Digital, who grow communities, build platforms and transform organisations for \nthe digital age.\n56\n To some extent, law already shapes how politicians and political parties can \nengage with people in the course of elections including how they declare gifts and donations, \nthe content and authorisation of advertising, and rules around information provision on election \nday.  When  does  political  advertising  and  audience  segmentation  cease  being  acceptable  \nand become unreasonably intrusive? This is being actively debated, and the extent to which \nan  organisation’s  use  of  data  manipulates  people’s  participation  in  essential  processes  like  \nelections should be carefully considered in ethical frameworks and debates. \n54 https://www.theguardian.com/technology/2017/mar/04/cambridge-analytics-data-brexit-trump.\n55 http://nationbuilder.com.\n56 https://www.bluestatedigital.com/eu.\n\n26 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nBiases in model design\nBiases in model design – in analytics, machine learning and artificial intelligence – are becoming \nmore apparent. The issues do not simply arise from biases in underlying data sources but also \ndesign decisions that are made while algorithms are being developed.\nIn 2016, the first international beauty contest judged by AI – billed as analysing ‘objective’ \nfeatures, such as facial symmetry and wrinkles – identified nearly all white winners.\n57\n Updates \nto Google Photos in 2015 accidentally saw black people identified as ‘gorillas’.\n58\n Researchers \nfrom Carnegie Mellon University in 2015 discovered that significantly fewer women than men \nwere shown advertisements online for jobs of over $200,000.\n59\nAlong  with  taking  into  account  potential  biases  in  data  sources,  teams  undertaking  data  \nprojects need to be aware of biases in how a model or AI is designed. Assumptions and biases \nin  model  design  –  such  as  that  women  would  not  be  interested  in  high-paying  jobs\n60\n  –  can  \nshape society and lead to data sources that reinforce biases. \nResponding to errors and enabling feedback\nAn organisations can influence whether a project is considered ethical by planning for potential \nerrors  in  a  data  service  or  model,  and  minimising  the  impact  of  these  errors.  Similarly,  the  \nextent to which people affected by a model can meaningfully request feedback and appeal \ndecisions can influence perceptions of the ethics of a service. Assessing the mechanisms an \norganisation has in place to detect and mitigate potential errors (and limitations such as bias), \nand empower people affected by their service or use, is part of the Data Ethics Canvas.\n57 http://beauty.ai.\n58 http://mashable.com/2015/07/01/google-photos-black-people-gorillas/#UwcwL02Leuqn.\n59 https://www.degruyter.com/view/j/popets.2015.1.issue-1/popets-2015-0007/popets-2015-0007.xml.\n60 https://www.theguardian.com/technology/2015/jul/08/women-less-likely-ads-high-paid-jobs-google-study.\n\n27 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nCase study: Australian welfare payments provider criticised for 20% error rate in \ndebt model\nOver Christmas in December 2016, news stories emerged of the Australian Department \nof Human Services’s automated debt recovery notices to recipients of welfare payments \nfrom its subsidiary, Centrelink. Their system cross-referenced Centrelink payment data \nwith Australian Taxation Office records to identify discrepancies in reported income, \nand issued debt notices accordingly. Prior to the fully automated service in July 2016, \nidentified discrepancies were reviewed and verified by Department of Human Services \nstaff  before  debt  notices  were  issued.  Since  its  introduction,  more  than  200,000 \nAustralians have been issued with debt recovery notices.\n61\nIn response to an inquiry set up by the Australian Parliament in first quarter of 2017, \nthe Department confirmed that in 20% of cases, people appealing their debt notices \nwere found not to owe money.\n62\n Under the original model, recipients of debt notices \nhad 21 days to appeal the stated debt. The burden of proving that a debt notice was \ninaccurate fell onto notice recipients.\nThe volume of notices issued with a relatively high error rate put pressure on Centrelink \nstaff responding to customers appealing debt notices. Debt notice recipients reported \ndifficulties contacting Centrelink by phone and attending offices in person.\n63\n In response \nto criticisms, the Department for Human Services extended the period for appeal from \n21 days to 28 days. \nIn this instance, people affected by the automated system tended to be vulnerable \nmembers of the Australian population – people with disabilities, financial difficulties, \nand  people  confronting  periods  of  hardship.  Whether  the  Department  of  Human  \nServices  had  adequate  mechanisms  in  place  for  these  people  to  appeal  automated  \ndebt notices, and steps taken to minimise error, may be factors in determining whether \nthe overall operation of the programme was ethical.\n61 http://www.sbs.com.au/news/article/2017/03/08/department-tells-senate-inquiry-centrelinks-robo-debt-\nscheme-should-stay.\n62 http://mediahub.humanservices.gov.au/media/lets-talk-about-facts.\n63 http://www.abc.net.au/news/2017-03-03/centrelink-debt-controversy-what-is-robodebt/8317764.\n\n28 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nRarely will datasets and models be completely free of error, bias and other limitations. How \norganisations respond to error and inconsistency, and the mechanisms they put in place to \nmitigate potential harm to people affected by their data service or use, are central to taking \nethics considerations seriously. \nConclusion\nDiscussions of ‘ethical’ and ‘unethical’ data practice are increasing in frequency and intensity. \nSocieties  are  facing  various  data  challenges  arising  from  issues  such  as  bias  in  data  and  \nalgorithm  design,  a  lack  of  transparency  around  how  personal  data  is  monetised,  concern  \naround how personal data informs the information we see and our participation in society, and \nhow and in what circumstances people and organisations can access data at all. \nWe are at the beginning of an age of services, decisions and technologies that rely on data. \nDebates  surrounding  ethical  data  collection,  sharing  and  use  are  only  a  few  years  old,  but  \nbuild  on  a  rich  tradition  of  ethical  practice  in  sectors  like  medicine,  journalism,  computing,  \nenvironmental  management,  and  in  the  laws  that  shape  our  expectations  of  property  and  \ninformation (such as data protection and IP) and of each other (such as in anti-discrimination \nlaws).\nRather than come up with an explicit ethics code or set of ethical principles for data management, \nwe have proposed a tool – the Data Ethics Canvas – to help organisations to identify and work \nthrough issues in their data practice that may negatively impact on individuals or society. What \npeople perceive to be ethical and unethical use of data is rapidly evolving, and so codes of \npractice are likely to quickly become out of date. The Data Ethics Canvas is designed to spark \ndebate, challenge assumptions and help organisations address potential ethics issues before \nthey happen.\nOrganisations  need  to  take  responsibility  for  aspects  of  their  data  practice  that  stand  to  \nnegatively affect people and society. As ethics codes continue to develop within sectors, \norganisations and governments, we hope that the kinds of issues covered in this paper shape \nthe content of those codes.\nMost  importantly,  we  hope  that  organisations  developing  their  own  ethics  codes  and  \nframeworks, and applying the Data Ethics Canvas, share their learnings and insights openly. \nInviting discussion demonstrates a willingness to explore ethical obligations and engage with \npeople on data management. Building trust and demonstrating leadership in data ethics will \nbe crucial to successful business, governance and research in the 21st century.\n\n29 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nData Ethics Canvas\nWhat are your data sources?Name and describe key data sources used in your project, whether you’re collecting them yourself or getting access from third parties.\nWho has rights over your data sources?Where did you get the data from? e.g. is it data produced by an organisation or data collected directly from individuals?Do you have permission or another basis on which you’re allowed to use this data? What ongoing rights will the data source have?\nAre there any limitations \n \nin your data sources?Which might influence the outcomes of your project, like:\n \n»  bias in data collection, inclusion, \nalgorithm\n \n»  gaps, omissions\n \n»  other sensitivities\nWhat policies/laws shape your use of this data?Data protection legislation, IP and database rights legislation, sector specific data sharing policies/regulation (e.g. health, employment, taxation)Sector specific ethics legislation?\nAre you going to be sharing this data with other organisations?If so, who? \nWhat’s your core purpose for using this data?What is your primary use case, your business model? Are you collecting more data than is needed for your purpose?\nWho could be negatively affected?Could the manner in which this data is collected, shared, used cause harm?\n \n»\nbe used to target, profile, prejudice people\n \n»  unfairly restrict access (eg exclusive \narrangements)\nCould people *perceive* it to be harmful? \nDo people understand your purpose?If this is a project/use that could impact on people or more broadly shape/impact society, do people understand your purpose? Has this been clearly communicated to them?\nHow are you minimising negative impact?What steps can you take to minimise harm?  Are there measures you could take to reduce limitations in your data sources? Could you monitor potential negative impact to support mitigating activities? What benefits will these actions add to your project?\nWho will be positively affected by this project?What individuals, demographics, organisations? How will they be positively affected? Do they know and understand how they are positively affected?\nHow can people engage with you?Can people affected appeal or request changes to the service? To what extent?Are the appeal mechanisms reasonable?\nAre you communicating potential risks/issues, if any?How are limitations and risks being communicated to people affected by your project,  and organisations using data?What channels are you using?When is your next review?When will this Data Ethics Canvas be reviewed?How will ongoing issues be monitored?What are your actions?What steps are you going to take prior to moving forward with this project?\nAUGUST 2017\ntheodi.org\n\n30 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nAppendix\nData Ethics Canvas user guide\nThe Data Ethics Canvas is based on the Ethics Canvas,\n64\n a higher-level framework for assessing \nthe  ethical  implications  of  any  project  developed  by  the  ADAPT  Centre  for  Digital  Content  \nTechnology. The ADAPT Centre’s ethics canvas is itself based on the original Business Model \nCanvas by Alex Osterwalder.\n65\n The Data Ethics Canvas will continue to evolve with user testing \nand feedback. For example, its format may change: it may become an interactive online tool, \na print-out or series of cards. \nThe Ethics Canvas:\n focuses on the people and communities affected, ways in which they might be \naffected and steps to mitigate impact\n is designed to prompt discussion and critical thinking, enabling identification of \npotential areas of risk and evaluates impact in context\n considers that one type of data activity can have lots of outcomes (and potential \nconsequences), depending on the context within which the activity takes place, \nits  purpose  and  the  organisation  involved.  It  goes  beyond  being  a  checklist  for  \ncompliance\nWhy use the Data Ethics Canvas\n It helps identify and discuss potential ethics issues arising from an organisation \ncollecting, using and sharing data as a team\n It is a flexible tool, designed to tease out potential risks without predisposing an \noutcome and can sit alongside more formal ethics guidelines\n It  will  increase  the  positive  impact  of  your  work  by  raising  issues  and  \nconsiderations that help to design better products and services, and reduce bias\n64 https://www.ethicscanvas.org.\n65 https://strategyzer.com/canvas/business-model-canvas.\n\n31 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nWhen should organisations consult the Data Ethics Canvas?\n At  the  start  of  a  project  and  throughout  the  project  lifecycle  to  help  identify  \npotential ethical concerns, and as a tool to aid decision-making when changes \nare made \n Projects where data collection, use or distribution are likely to impact individuals \nor wider society\n Projects involving any type of data, regardless of whether it is open, shared or \nclosed data, personal or non-personal\nWhen using the canvas organisations should take into account various aspects of data practice:\n How data is collected\n Information that is included and excluded\n Design of methods of data collection\n Accuracy and trustworthiness of data sources\n Circumstances in which data was collected \n How data is shared\n Individuals and organisations with access to data\n Individuals  and  organisations  excluded  from  accessing  data,  or  restricted  in  \nhow they may use data\n How data is used\n Design of models and algorithms\n Manipulative data presentation\n Purpose of use\n Margin for error\nThe  case  studies  researched  in  this  paper  help  to  navigate  these  aspects  of  data  practice,  \nidentify potential ethical concerns and decide on an appropriate course of action.\nSince the Data Ethics Canvas is not a checklist, it does not offer many clear-cut answers for \nteams about whether their particular project or use might be unethical. Often, it is the context \nwithin which a data project takes place – the people or communities affected, how they are \naffected and the steps the organisation takes to minimise harm – that determine whether \nthat  organisation  has  behaved  ethically.  The  Data  Ethics  Canvas  helps  organisations  take  \nresponsibility for the potential impacts of their data practices on people and society. \n\n32 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nImplementing a Data Ethics Canvas within an \norganisation or sector\nWhere a data practice has the potential to adversely impact on individuals or society, or could \nbe  perceived  by  the  public  to  have  an  adverse  impact,  completing  the  Data  Ethics  Canvas  \nshould be encouraged. We have placed emphasis on ‘impact’ to distinguish data models and \nservices that directly and indirectly shape our lives from internal or nonconsequential activities. \nMaking completion of a Data Ethics Canvas mandatory within an organisation for these kinds \nof projects helps to establish good practice, prior to any formal regulation. It demonstrates an \norganisation’s commitment to treating data ethically. \nThere  is  precedent  for  a  decision-making  tool  like  the  Data  Ethics  Canvas  in  other  data  \nregulatory areas. Under the EU General Data Protection Regulation (GDPR), the completion \nof  a  Data  Protection  Impact  Assessment  (DPIA)  is  mandatory  for  organisations  where  data  \nprocessing is likely to result in a high risk to the rights and freedoms of natural persons (Article \n35). Article 35(3) goes on to highlight specific circumstances in which a DPIA will be required, \nincluding where extensive personal information about a person is analysed and used as the \nbasis of decisions which have legal effects or otherwise significantly affect that person.\nThe  substance  of  a  DPIA  resembles  the  substance  of  Privacy  Impact  Assessments  (PIA),  \nwhich the UK Office of the Information Commissioner recommends organisations processing \npersonal  data  undertake.\n66\n  While  PIAs  in  their  present  form  in  the  UK  are  not  mandatory,  \nthe Office of the Information Commissioner can direct organisations to conduct PIAs.\n67\n At a \nminimum, DPIAs under the GDPR must include:\n a systematic description of processing operations and purpose of processing\n an assessment of the necessity and proportionality of processing operations in \nrelation to the described purpose\n an assessment of risks to the rights and freedoms of data subjects\n measures taken to mitigate risks and ensure compliance with the GDPR\n66 https://www.oaic.gov.au/agencies-and-organisations/guides/guide-to-undertaking-privacy-impact-assessments.\n67   Ibid.\n\n33 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\nDPIAs act as a decision-making tool for data processors and controllers under data protection \nlaws  in  the  same  way  the  Data  Ethics  Canvas  is  designed  to  function  for  broader  ethical  \nconsiderations. The canvas requires:\n a description of the data project, model or practice and its purpose\n identification of people likely to be affected by the activities\n an  assessment  of  potential  risks  and  limitations  associated  with  the  activities,  \nwhich could negatively affect people and society\n steps taken to mitigate those risks and limitations\nImpact assessments which consider the purpose of a project, its risks and potential adverse \nimpacts  and  ways  of  mitigating  those  impacts  exist  in  multiple  countries  –  the  Data  Ethics  \nCanvas could be made mandatory and subject to review.\nWe  recommend  that  when  the  Data  Ethics  Canvas  is  completed,  it  is  made  shareable  with  \nindividuals  or  groups  affected  by  the  particular  data  practice  at  their  request  and,  where \npossible, more openly, including on public-facing websites.\n\n34 Helping organisations navigate ethical concerns in their data practices | Open Data Institute 2017\n\n","version":"1.10.100"}