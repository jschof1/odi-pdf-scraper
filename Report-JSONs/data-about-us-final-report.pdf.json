{"numpages":42,"numrender":42,"info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"Adobe InDesign 14.0 (Windows)","Producer":"Adobe PDF Library 15.0","CreationDate":"D:20191007140055+01'00'","ModDate":"D:20191031093530Z","Trapped":{"name":"False"}},"metadata":{"_metadata":{"xmp:createdate":"2019-10-07T14:00:55+01:00","xmp:metadatadate":"2019-10-31T09:35:30Z","xmp:modifydate":"2019-10-31T09:35:30Z","xmp:creatortool":"Adobe InDesign 14.0 (Windows)","xmpmm:instanceid":"uuid:8a9ace3a-a47d-4003-b0ca-78bb6b740b80","xmpmm:originaldocumentid":"xmp.did:001B78E7082168118A6DD26267566E72","xmpmm:documentid":"xmp.id:d1802cb0-ec42-ae41-ace0-3ea3311714a7","xmpmm:renditionclass":"proof:pdf","xmpmm:derivedfrom":"xmp.iid:b0635b1e-4ef6-fe47-bafc-c993a79bfbfcxmp.did:683329c3-9560-0044-b831-d748b2754f2axmp.did:001B78E7082168118A6DD26267566E72default","xmpmm:history":"convertedfrom application/x-indesign to application/pdfAdobe InDesign CC 14.0 (Windows)/2019-10-07T14:00:55+01:00","dc:format":"application/pdf","pdf:producer":"Adobe PDF Library 15.0","pdf:trapped":"False","fa1:postscriptname":"AkzidenzGroteskStd-MdAkzidenzGroteskStd-RegularSabonMTStd-RegularSabonMTStd-SemiboldAkzidenzGroteskStd-LightSabonMTStd-ItalicAkzidenzGroteskStd-BoldSabonMTStd-SemiboldItAkzidenzGroteskStd-SuperAkzidenzGroteskStd-It","fa1:profileblob":"AAAEWAAAAALNw/RNAAAAAHJzbmYAAARIHarJ5wAAAA1lbWFuwAAA07gcJLUAAAAAAwAYAAAAABQDAAAUAQEAAAAAAzsABgIBAAAAAAJGADADAQAAAAADAAAYBAEAAAAAAkYAMAUBAAAAAAMYABUGAQAAAAACFAAICAEAAAAAAgwAOgkBAAAAAAJ2ABwLAQAAAAACdgAwDAEAAAAAAqYAMAEDAAEECQMtAA4CAwABBAkBTABgAwMAAQQJAtYAKgQDAAEECQFMAGAFAwABBAkC1gAqBgMAAQQJAOgAEAgDAAEECQDYAHQJAwABBAkBrAA4CwMAAQQJAawAYAwDAAEECQBWAGEAcgBpAG8AdQBzACAAQgBlAHIAdABoAG8AbABkACAAZABlAHMAaQBnAG4AZQByAHMAIABpAG4AYwBsAHUAZABpAG4AZwAgAEcAdQBlAG4AdABlAHIAIABHAGUAcgBoAGEAcgBkACAATABhAG4AZwBlAFYAZQByAHMAaQBvAG4AIAAwADAAMQAuADAAMAAwADsAQwBvAHIAZQAgADEALgAwAC4AMAAxADsAbwB0AGYALgA1AC4AMAAyAC4AMgAyADkAMQA7ADEANQAuADAANgBXAGgAdAB0AHAAOgAvAC8AdwB3AHcALgBiAGUAcgB0AGgAbwBsAGQAdAB5AHAAZQBzAC4AYwBvAG0ALwBpAG4AZgBvAC8AZABlAHMAaQBnAG4AZQByAHMALgBoAHQAbQBsVmFyaW91cyBCZXJ0aG9sZCBkZXNpZ25lcnMgaW5jbHVkaW5nIEd1ZW50ZXIgR2VyaGFyZCBMYW5nZVZlcnNpb24gMDAxLjAwMDtDb3JlIDEuMC4wMTtvdGYuNS4wMi4yMjkxOzE1LjA2V2h0dHA6Ly93d3cuYmVydGhvbGR0eXBlcy5jb20vaW5mby9kZXNpZ25lcnMuaHRtbABBAGsAegBpAGQAZQBuAHoALQBHAHIAbwB0AGUAcwBrACAAUwB0AGQAIABNAGUAZABBAGsAegBpAGQAZQBuAHoARwByAG8AdABlAHMAawBTAHQAZAAtAE0AZEFremlkZW56LUdyb3Rlc2sgU3RkIE1lZEFremlkZW56R3JvdGVza1N0ZC1NZABSAGUAZwB1AGwAYQByTWVkaXVtAAAAZnlsZwAAAAUAAEOx0J930QAAAAAgeHRtAAAABQAABIRsN6n0AAAAAHhtZmYAAAAFAAAAAAAAAAAAAAAAICBtYgAAAAUAAAAA/////wAAAAB0bmZuAAAABQAAAAD/////AAAAACBjbmUAAAAGAAAHfvENtaIAAAAAAAD//25lcnAAAAAFAAAANSBXhwkAAAAAeWFsYQAAAAUAACO87gynxgAAAABucmVrAAAABQAAEaD3jy/IAAAAAG5ya2YAAAAFAAAeSAKhllIAAAAAeWFsdwAAAAUAAAAAAAAAAAAAAAAgdG1mAAAAA09UVE8=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AAAEDAAAAALNw/ROAAAAAHJzbmYAAAP8OwSZNwAAAA1lbWFuAAAAwLv5LV8AAAAAAtsVAAAAABAC2wAMAQEAAAAAArsACAIBAAAAAAKlAB4DAQAAAAAC2wAVBAEAAAAAAk4AOAUBAAAAAAKwABMGAQAAAAAChgAfCwEAAAAAAdYAPAwBAAAAAALDABgBAwABBAkC8AAIAgMAAQQJAhIAPAMDAAEECQIoACYEAwABBAkBKABwBQMAAQQJAigAJgYDAAEECQGYAD4LAwABBAkAsAB4DAMAAQQJAGgAdAB0AHAAOgAvAC8AdwB3AHcALgBtAG8AbgBvAHQAeQBwAGUAaQBtAGEAZwBpAG4AZwAuAGMAbwBtAC8AaAB0AG0AbAAvAGQAZQBzAGkAZwBuAGUAcgAvAGQAZQBzAF8AaQBuAGQAZQB4AC4AaAB0AG0AbAAuAFYAZQByAHMAaQBvAG4AIAAxAC4AMAAwADQAOwBQAFMAIAAwADAAMQAuADAAMAAzADsAQwBvAHIAZQAgADEALgAwAC4AMwA4ADsAbQBhAGsAZQBvAHQAZgAuAGwAaQBiADEALgA2AC4ANQA5ADYAMABoAHQAdABwADoALwAvAHcAdwB3AC4AbQBvAG4AbwB0AHkAcABlAGkAbQBhAGcAaQBuAGcALgBjAG8AbQAuaHR0cDovL3d3dy5tb25vdHlwZWltYWdpbmcuY29tL2h0bWwvZGVzaWduZXIvZGVzX2luZGV4Lmh0bWwuADEALgAwADAANAA7AEEARABCAEUAOwBTAGEAYgBvAG4ATQBUAFMAdABkAC0AUwBlAG0AaQBiAG8AbABkVmVyc2lvbiAxLjAwNDtQUyAwMDEuMDAzO0NvcmUgMS4wLjM4O21ha2VvdGYubGliMS42LjU5NjBodHRwOi8vd3d3Lm1vbm90eXBlaW1hZ2luZy5jb20uMS4wMDQ7QURCRTtTYWJvbk1UU3RkLVNlbWlib2xkAFMAYQBiAG8AbgAgAE0AVAAgAFMAdABkU2Fib24gTVQgU3RkIFNlbWlib2xkAEIAbwBsAGRmeWxnAAAABQAAjh0n5lzoAAAAACB4dG0AAAAFAAAE4J/kvO4AAAAAeG1mZgAAAAUAAAIW+rXKkAAAAAAgIG1iAAAABQAAAAD/////AAAAAHRuZm4AAAAFAAAAAP////8AAAAAIGNuZQAAAAYAAAkekqmNnAAAAAAAAP//bmVycAAAAAUAAAhba0VOwgAAAAB5YWxhAAAABQAAG/R2XrpOAAAAAG5yZWsAAAAFAAAAAP////8AAAAAbnJrZgAAAAUAABXwXVpH2AAAAAB5YWx3AAAABQAAAAgLU8jaAAAAACB0bWYAAAADT1RUTw==AAAEYAAAAALNw/RNAAAAAHJzbmYAAARQEP5QZQAAAA1lbWFugAAA1aOs42kAAAAAAwoaAAAAABQDCgAUAQEAAAAAAx8ABQIBAAAAAAJ6ADADAQAAAAADCgAaBAEAAAAAAnoAMAUBAAAAAAMkABgGAQAAAAACFAAICAEAAAAAAgwAOgkBAAAAAAKqABwLAQAAAAACqgAwDAEAAAAAAkYANAEDAAEECQM8AA4CAwABBAkBTABgAwMAAQQJAtoAMAQDAAEECQFMAGAFAwABBAkC2gAwBgMAAQQJAOgAEAgDAAEECQDYAHQJAwABBAkBrAA4CwMAAQQJAawAYAwDAAEECQBWAGEAcgBpAG8AdQBzACAAQgBlAHIAdABoAG8AbABkACAAZABlAHMAaQBnAG4AZQByAHMAIABpAG4AYwBsAHUAZABpAG4AZwAgAEcAdQBlAG4AdABlAHIAIABHAGUAcgBoAGEAcgBkACAATABhAG4AZwBlAFYAZQByAHMAaQBvAG4AIAAwADAAMQAuADAAMAAwADsAQwBvAHIAZQAgADEALgAwAC4AMAAxADsAbwB0AGYALgA1AC4AMAAyAC4AMgAyADkAMQA7ADEANQAuADAANgBXAGgAdAB0AHAAOgAvAC8AdwB3AHcALgBiAGUAcgB0AGgAbwBsAGQAdAB5AHAAZQBzAC4AYwBvAG0ALwBpAG4AZgBvAC8AZABlAHMAaQBnAG4AZQByAHMALgBoAHQAbQBsVmFyaW91cyBCZXJ0aG9sZCBkZXNpZ25lcnMgaW5jbHVkaW5nIEd1ZW50ZXIgR2VyaGFyZCBMYW5nZQBBAGsAegBpAGQAZQBuAHoALQBHAHIAbwB0AGUAcwBrACAAUwB0AGQAIABMAGkAZwBoAHRWZXJzaW9uIDAwMS4wMDA7Q29yZSAxLjAuMDE7b3RmLjUuMDIuMjI5MTsxNS4wNldodHRwOi8vd3d3LmJlcnRob2xkdHlwZXMuY29tL2luZm8vZGVzaWduZXJzLmh0bWwAQQBrAHoAaQBkAGUAbgB6AEcAcgBvAHQAZQBzAGsAUwB0AGQALQBMAGkAZwBoAHRBa3ppZGVuei1Hcm90ZXNrIFN0ZCBMaWdodEFremlkZW56R3JvdGVza1N0ZC1MaWdodABSAGUAZwB1AGwAYQByAABmeWxnAAAABQAAYAujjaJEAAAAACB4dG0AAAAFAAAHCB+GnOYAAAAAeG1mZgAAAAUAAAIW6aH9NAAAAAAgIG1iAAAABQAAAAD/////AAAAAHRuZm4AAAAFAAAAAP////8AAAAAIGNuZQAAAAYAAAd+pZGO2gAAAAAAAP//bmVycAAAAAUAAAhoseRtRQAAAAB5YWxhAAAABQAAQMhGiQ1NAAAAAG5yZWsAAAAFAAAU4pCVkcIAAAAAbnJrZgAAAAUAACO8vx25oAAAAAB5YWx3AAAABQAAAAgLU8jaAAAAACB0bWYAAAADT1RUTw==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AAAEKAAAAALNw/ROAAAAAHJzbmYAAAQYC7+FewAAAA1lbWFuQAAAx+TOckAAAAAAAskcAAAAABACyQAMAQEAAAAAAtYADwIBAAAAAAKKACADAQAAAAACyQAcBAEAAAAAAlIAOAUBAAAAAAKVABUGAQAAAAACqgAfCwEAAAAAAhYAPAwBAAAAAALlABgBAwABBAkC/QAWAgMAAQQJAZgAQAMDAAEECQGuACoEAwABBAkBKABwBQMAAQQJAa4AKgYDAAEECQHYAD4LAwABBAkAsAB4DAMAAQQJAGgAdAB0AHAAOgAvAC8AdwB3AHcALgBtAG8AbgBvAHQAeQBwAGUAaQBtAGEAZwBpAG4AZwAuAGMAbwBtAC8AaAB0AG0AbAAvAGQAZQBzAGkAZwBuAGUAcgAvAGQAZQBzAF8AaQBuAGQAZQB4AC4AaAB0AG0AbAAuAFYAZQByAHMAaQBvAG4AIAAxAC4AMAAwADMAOwBQAFMAIAAwADAAMQAuADAAMAAxADsAQwBvAHIAZQAgADEALgAwAC4AMwA4ADsAbQBhAGsAZQBvAHQAZgAuAGwAaQBiADEALgA2AC4ANQA5ADYAMAAxAC4AMAAwADMAOwBBAEQAQgBFADsAUwBhAGIAbwBuAE0AVABTAHQAZAAtAFMAZQBtAGkAYgBvAGwAZABJAHQAaAB0AHQAcAA6AC8ALwB3AHcAdwAuAG0AbwBuAG8AdAB5AHAAZQBpAG0AYQBnAGkAbgBnAC4AYwBvAG0ALmh0dHA6Ly93d3cubW9ub3R5cGVpbWFnaW5nLmNvbS9odG1sL2Rlc2lnbmVyL2Rlc19pbmRleC5odG1sLlZlcnNpb24gMS4wMDM7UFMgMDAxLjAwMTtDb3JlIDEuMC4zODttYWtlb3RmLmxpYjEuNi41OTYwMS4wMDM7QURCRTtTYWJvbk1UU3RkLVNlbWlib2xkSXRodHRwOi8vd3d3Lm1vbm90eXBlaW1hZ2luZy5jb20uU2Fib24gTVQgU3RkIFNlbWlib2xkIEl0YWxpYwBTAGEAYgBvAG4AIABNAFQAIABTAHQAZABCAG8AbABkACAASQB0AGEAbABpAGMAZnlsZwAAAAUAAI/YwKMN4AAAAAAgeHRtAAAABQAABOAXoWSFAAAAAHhtZmYAAAAFAAACFl6gnXcAAAAAICBtYgAAAAUAAAAA/////wAAAAB0bmZuAAAABQAAAAD/////AAAAACBjbmUAAAAGAAAJHpKpjZwAAAAAAAD//25lcnAAAAAFAAAIXTS0ruUAAAAAeWFsYQAAAAUAAB1OVsjRYwAAAABucmVrAAAABQAAAAD/////AAAAAG5ya2YAAAAFAAASUB4ysjgAAAAAeWFsdwAAAAUAAAAIC1PI2gAAAAAgdG1mAAAAA09UVE8=AAAEYAAAAALNw/ROAAAAAHJzbmYAAARQGqCd8wAAAA1lbWFugAAA1atKTqMAAAAAAwoaAAAAABQDCgAUAQEAAAAAAx8ABQIBAAAAAAJ6ADADAQAAAAADCgAaBAEAAAAAAnoAMAUBAAAAAAMkABgGAQAAAAACFAAICAEAAAAAAgwAOgkBAAAAAAKqABwLAQAAAAACqgAwDAEAAAAAAkYANAEDAAEECQM8AA4CAwABBAkBTABgAwMAAQQJAtoAMAQDAAEECQFMAGAFAwABBAkC2gAwBgMAAQQJAOgAEAgDAAEECQDYAHQJAwABBAkBrAA4CwMAAQQJAawAYAwDAAEECQBWAGEAcgBpAG8AdQBzACAAQgBlAHIAdABoAG8AbABkACAAZABlAHMAaQBnAG4AZQByAHMAIABpAG4AYwBsAHUAZABpAG4AZwAgAEcAdQBlAG4AdABlAHIAIABHAGUAcgBoAGEAcgBkACAATABhAG4AZwBlAFYAZQByAHMAaQBvAG4AIAAwADAAMQAuADAAMAAwADsAQwBvAHIAZQAgADEALgAwAC4AMAAxADsAbwB0AGYALgA1AC4AMAAyAC4AMgAyADkAMQA7ADEANQAuADAANgBXAGgAdAB0AHAAOgAvAC8AdwB3AHcALgBiAGUAcgB0AGgAbwBsAGQAdAB5AHAAZQBzAC4AYwBvAG0ALwBpAG4AZgBvAC8AZABlAHMAaQBnAG4AZQByAHMALgBoAHQAbQBsVmFyaW91cyBCZXJ0aG9sZCBkZXNpZ25lcnMgaW5jbHVkaW5nIEd1ZW50ZXIgR2VyaGFyZCBMYW5nZQBBAGsAegBpAGQAZQBuAHoALQBHAHIAbwB0AGUAcwBrACAAUwB0AGQAIABTAHUAcABlAHJWZXJzaW9uIDAwMS4wMDA7Q29yZSAxLjAuMDE7b3RmLjUuMDIuMjI5MTsxNS4wNldodHRwOi8vd3d3LmJlcnRob2xkdHlwZXMuY29tL2luZm8vZGVzaWduZXJzLmh0bWwAQQBrAHoAaQBkAGUAbgB6AEcAcgBvAHQAZQBzAGsAUwB0AGQALQBTAHUAcABlAHJBa3ppZGVuei1Hcm90ZXNrIFN0ZCBTdXBlckFremlkZW56R3JvdGVza1N0ZC1TdXBlcgBSAGUAZwB1AGwAYQByAABmeWxnAAAABQAAQNc0OyatAAAAACB4dG0AAAAFAAAEhCW3ql4AAAAAeG1mZgAAAAUAAAIWOhcJfgAAAAAgIG1iAAAABQAAAAD/////AAAAAHRuZm4AAAAFAAAAAP////8AAAAAIGNuZQAAAAYAAAd+8Q21ogAAAAAAAP//bmVycAAAAAUAAAhoz7Nn/wAAAAB5YWxhAAAABQAALZxLVO/SAAAAAG5yZWsAAAAFAAAWMoXBZo8AAAAAbnJrZgAAAAUAAC6U4QirEgAAAAB5YWx3AAAABQAAAAgLU8jaAAAAACB0bWYAAAADT1RUTw==AAAEWAAAAALNw/RNAAAAAHJzbmYAAARIHFn3MwAAAA1lbWFuAAAA02Vd8rAAAAAAAwgbAAAAABQDCAAUAQEAAAAAAx0ABgIBAAAAAAJ+ADADAQAAAAADCAAbBAEAAAAAAn4AMAUBAAAAAAMjABUGAQAAAAACFAAICAEAAAAAAgwAOgkBAAAAAAKuABwLAQAAAAACrgAwDAEAAAAAAkYAOAEDAAEECQM4AAwCAwABBAkBTABgAwMAAQQJAt4AKgQDAAEECQFMAGAFAwABBAkC3gAqBgMAAQQJAOgAEAgDAAEECQDYAHQJAwABBAkBrAA4CwMAAQQJAawAYAwDAAEECQBWAGEAcgBpAG8AdQBzACAAQgBlAHIAdABoAG8AbABkACAAZABlAHMAaQBnAG4AZQByAHMAIABpAG4AYwBsAHUAZABpAG4AZwAgAEcAdQBlAG4AdABlAHIAIABHAGUAcgBoAGEAcgBkACAATABhAG4AZwBlAFYAZQByAHMAaQBvAG4AIAAwADAAMQAuADAAMAAwADsAQwBvAHIAZQAgADEALgAwAC4AMAAxADsAbwB0AGYALgA1AC4AMAAyAC4AMgAyADkAMQA7ADEANQAuADAANgBXAGgAdAB0AHAAOgAvAC8AdwB3AHcALgBiAGUAcgB0AGgAbwBsAGQAdAB5AHAAZQBzAC4AYwBvAG0ALwBpAG4AZgBvAC8AZABlAHMAaQBnAG4AZQByAHMALgBoAHQAbQBsVmFyaW91cyBCZXJ0aG9sZCBkZXNpZ25lcnMgaW5jbHVkaW5nIEd1ZW50ZXIgR2VyaGFyZCBMYW5nZQBBAGsAegBpAGQAZQBuAHoALQBHAHIAbwB0AGUAcwBrACAAUwB0AGQAIABSAGUAZwB1AGwAYQByVmVyc2lvbiAwMDEuMDAwO0NvcmUgMS4wLjAxO290Zi41LjAyLjIyOTE7MTUuMDZXaHR0cDovL3d3dy5iZXJ0aG9sZHR5cGVzLmNvbS9pbmZvL2Rlc2lnbmVycy5odG1sAEEAawB6AGkAZABlAG4AegBHAHIAbwB0AGUAcwBrAFMAdABkAC0ASQB0QWt6aWRlbnotR3JvdGVzayBTdGQgSXRhbGljQWt6aWRlbnpHcm90ZXNrU3RkLUl0AEkAdABhAGwAaQBjZnlsZwAAAAUAAEYRvVz8WAAAAAAgeHRtAAAABQAABITR5cSHAAAAAHhtZmYAAAAFAAAEHIEI7tUAAAAAICBtYgAAAAUAAAAA/////wAAAAB0bmZuAAAABQAAAAD/////AAAAACBjbmUAAAAGAAAHfvENtaIAAAAAAAD//25lcnAAAAAFAAAIYIhYIHwAAAAAeWFsYQAAAAUAACYWzqwicgAAAABucmVrAAAABQAAEnKP99fAAAAAAG5ya2YAAAAFAAAcuJSa1/MAAAAAeWFsdwAAAAUAAAAIC1PI2gAAAAAgdG1mAAAAA09UVE8="}},"text":"\n\nAbout Data About Us1 \nAbout Data \nAbout Us\nBy Renate Samson, \nKayshani Gibbon and \nAnna Scott\nSeptember  2019\n\nAbout Data About Us1 \nContents\nExecutive           Summary           3\nIntroduction           4\nBackground           5\nWhat           we           did           12\nThe           focus           group           13\nData about us: the four categories 21\nThe           workshop           25\nConclusion           35\n\nAbout Data About Us2 \nAbout this report\nThe About Data About Us report was produced by the Open Data \nInstitute (ODI) and the RSA (Royal Society for the encouragement of  \nArts, Manufactures and Commerce) and was commissioned by Luminate. \nResearch was carried out by the ODI, the RSA and Luminate. The report \nwas written by: Renate Samson (ODI), Anna Scott (ODI) and Kayshani \nGibbon (RSA), in collaboration with Jeni Tennison (ODI), Peter Wells \n(ODI), Kitty von Bertele (Luminate), Toby Murray (RSA) and Charlotte \nHolloway (RSA)\nAbout the ODI\nThe ODI works to build a strong, fair and sustainable data economy by \nhelping governments and businesses around the world get data to people \nwho need it. It is independent, nonprofit and nonpartisan, founded in \n2012 by Sir Tim Berners-Lee and Sir Nigel Shadbolt. From its headquar-\nters in London and via its global network of  startups, members and \nnodes, the ODI offers training, research and strategic advice for organisa-\ntions looking to explore the possibilities of  data.\nAbout the RSA\nThe RSA believes in a world where everyone is able to participate in \ncreating a better future. Through our ideas, research and a 30,000 strong \nFellowship we are a global community of  proactive problem solvers, \nsharing powerful ideas, carrying out cutting-edge research and building \nnetworks and opportunities for people to collaborate, influence and \ndemonstrate practical solutions to realise change.\nAbout Luminate \nLuminate is a global philanthropic organisation focused on empowering \npeople and institutions to work together to build just and fair societies. \nWe support innovative and courageous organisations and entrepreneurs \naround the world, and we advocate for the policies and actions that will \ndrive change across four impact areas: Civic Empowerment, Data & \nDigital Rights, Financial Transparency, and Independent Media. We work \nwith our investees and partners to ensure that everyone has the opportu-\nnity to participate in and to shape the issues affecting their societies, and \nto make those in positions of  power more responsive and accountable. \nLuminate was established in 2018 by Pierre Omidyar, the founder of  eBay, \nand his wife Pam. The organisation is part of  The Omidyar Group. www.\nluminategroup.com  \n\nAbout Data About Us3 \nExecutive Summary\nOur data lives are complicated. \nNever before has data played such an integral and granular role in how \nwe live. On a daily basis, we are asked to make decisions about personal data \nabout us – consenting to it being gathered and used for many purposes. \nWe are only just starting to grasp the impact that these decisions have \non us, and others. We must think differently about data, and the rights and \nresponsibilities around it. \nWe must engage with and listen to people about how they feel, and stop \nwriting off ‘the public’ as being complacent or ignorant about data protection \nissues, as they often are by people in power and in the media.\nRecently, ‘data ownership’ has been raised by some as a possible way to \ngive us control over the data about us. But, given that data about us is rarely \njust about us as individuals, but usually about other people too – many have \ncriticised ownership as an overly simplistic solution. Critics have said that \ninstead we need to strengthen our ‘data rights’ and the responsibilities to \nmaintain them, with a more systemic approach including legislation, regula-\ntion, policymaking, education, and advocacy.\nWe explored how members of  the UK public feel about data about them, \nabout having ownership or rights around it, and what kind of  control or \nprotection they feel is missing or needs strengthening. We did this over the \ncourse of  two focus groups and a workshop in London.\nTo help, we developed a graphic that explains the different types of  data \nabout us: ‘personal data’, ‘sensitive data’, ‘behavioural data’, and ‘societal \ndata’. We also tested and developed compelling narratives to help people \nunderstand and explore these different types of  data in context.\nWe did this because we saw it can be hard for people to decide how they \nfeel about sharing data about them, without being able to consider the \ndifferent elements or ways it is used. Sharing sensitive data about us so a \ncompany can target us with adverts is different from sharing societal data \nabout us (data which should be aggregated and anonymised) so it can be used \nto improve public services for everyone.\nPeople told us that they generally feel positive about the benefits brought \nby the internet and being more connected, but want greater honesty and \ntransparency, agency and control, rights and responsibility, context and fair-\nness, and compliance and enforceability over how data about them is used. \nUltimately, they want to know that where data is concerned they will be \ntreated as people, not as robots.  \nThis report is part of  a range of  outputs, including a video, a summary \nreport and a graphic explaining the types of  data about us. \nWe hope this work will help to start a wider conversation between people, \ngovernments, businesses, NGOs, interest groups, and think-tanks. Tweet your \nviews using #WeAreNotRobots.\n\nAbout Data About Us4 \nIntroduction \nData is everywhere. It is part of  the infrastructure of  all of  our lives, institu-\ntions, public organisations and private businesses. We are all involved – often \nunknowingly – in its creation, management, and use. Just as roads connect \nus, data connects us. Just as roads generate noise and air pollution, data can \ncreate equivalent harms or risks to individuals and communities alike. \nWhat is data? We use the term to cover a range of  things. Some data is \nabout us as individuals, some data had at one time been about us but we \nare no longer recognisable in it, and some data has nothing to do with us. \nData can be personal, sensitive, behavioural or societal. It can come in many \nshapes and sizes, and be accessible to many or only a few. The ODI’s Data \nSpectrum,\n1\n shows the different levels of  data access, from closed, to shared, \nto open. Closed data is restricted to an organisation; shared data is accessible \nto specific people or groups; and open data is data anyone can access, use and \nshare. \nThis project, undertaken by the RSA, the ODI, and Luminate has sought \nto learn from members of  the UK public about their relationship, thoughts, \nand feelings about data.\nWe have explored how members of  the UK public feel about the idea of  \nhaving rights over data about them, or ownership over it. We tested people’s \nunderstanding, reactions and emotions, and sense of  responsibility over data \nabout them, and how they felt about it being used for different purposes in \nsociety – from private sector companies, to employers, to public authorities.\nWe also wanted to give people space to explore what kind of  control, \ninsight, protection, or security they felt was missing or needed strengthening. \nWe focused on speaking with people in the UK for this initial tranche of  \nresearch, but the data rights/ownership debate is not confined to the UK – it is \nan international conversation and we recognise that the views of  people will \nvary from country to country. \nThis report explains our research and how we developed different ways of  \ndescribing the different types of  data about us – from personal to sensitive, \nbehavioural to societal – to help people differentiate and explore what types \nof  data about them they felt comfortable or uncomfortable being shared or \naccessed. We had found that the lack of  these clear definitions made it hard \nfor people to decide how they felt.\nWe have relayed what the public told us, in their words, about what they \nwanted to see happen next in terms of  greater rights, clearer responsibilities, \nand enhanced protections when it comes to data about us. This report is part \nof  a range of  outputs, including a video, a summary report, and a graphic \nexplaining the types of  data about us. \n1. Open Data Institute (2019) The Data Spectrum. [online] Available at: theodi.org/about-\nthe-odi/the-data-spectrum\n\nAbout Data About Us5 \nBackground\nWhat is data about us?\nWe might think of  data – particularly personal data – as being solely \nabout us as individuals. The fact that it is referred to as ‘personal’ means \nwe often refer to it as ‘my data’ or ‘data about me’, both of  which imply a \nsense of  ownership, individual agency, and control. This language indi-\ncates that we have an emotional relationship with the data, and consider \nit to be something that belongs to us individually and only contains \ninformation relevant to us. \nIn reality, data about us is rarely just about us as individuals. It is \nalmost always about ourselves and others; be it family, friends, colleagues, \nor people we happen to be in the same place as at any one time. \nThe data that we hold on our mobile phones, or that we share with any \ninternet connected device – such as a computer, a voice-activated assistant \nsuch as an Amazon Echo or Google Home device, a smart thermostat, \na connected car, a home surveillance system, even a smart TV – is likely \nto gather data about others too, from telephone numbers to emails, text \nmessages to photos, documents, voices, images, behaviours and so on. \nAny device designed to learn from us also learns about the people we \nknow, live with or interact with. It may then combine that data with data \nfrom people considered or defined to be ‘like us’ because they have the \ninterests and behaviours that we are deemed to demonstrate. \nData about us also goes beyond what is collected and shared through \nconnected technologies. DNA and genetic data is often seen as personal to \nus, but our genetics also contain elements of  our family, including people \nwe are distantly related to but may never have met.\n2\n This can be brilliant \nfor helping to determine familial genetic diseases and for taking action to \nprotect each other. Sharing it can help find medicines and cures for others \nwith similar diseases, and sharing our DNA in relation to diseases can \nhelp to identify other people who may be prone to them but completely \nunsuspecting.\n3\n \nWhen we are asked whether we would like to share data about us, we \nare often being asked to make a decision about data about others too. \nHow data about us can be used\nData alone can tell one story, but data combined can create deeper \ninsights.\nData about us is therefore often collected and combined (by public \n2. Erlich, Y., Shor, T., Pe’er, I., and Carmi, S. (2018) Identity inference of  genomic data using \nlong-range familial searches. Science, Vol. 362, Issue 6415, pp. 690-694. Available at: science.\nsciencemag.org/content/362/6415/690\n3. Hunt, E. (2018) Your father’s not your father’: when DNA tests reveal more than you \nbargained for. The Guardian. [online]. Available at: www.theguardian.com/lifeandstyle/2018/\nsep/18/your-fathers-not-your-father-when-dna-tests-reveal-more-than-you-bargained-for\n\nAbout Data About Us6 \nsector organisations and private companies) with data about others, along \nwith non-personal data, to improve services. Sometimes we are aware that \ndata about us is being collected, but sometimes it is far from clear. \nCombining data can be done to make decisions that create positive and \nmeaningful outcomes for communities, society, and individuals. But it can \nalso lead to the creation of  an intrusive network of  unseen organisations \nusing data to make inferences about us, which lead to assumptions that \nsteer us into algorithmic filter or preference bubbles, price discrimination \nor even denial of  services or products. \nThese inferences are now commonly made by data analysis through \n‘machine learning’. Machine learning is a set of  algorithms that can be \nused to gather insights and make predictions about data. \nA machine learning algorithm uses ‘training data’ (assumed to be \nrepresentative of  something, such as a history of  what we have watched on \nNetflix or listened to on Spotify) to create a statistical model.\n4\n This model \nis used to make predictions about things, based on the training data. \nThese models (and the predictions made by them) can vary greatly in how \nrepresentative or accurate they are. \nData about us is often used to create insights about, or predict, our \nbehaviour. \nFor example, when we go online, data may be collected about our \nbehaviour: what websites we look at, how long we spend on them, what \ndevice we are using, what we browse, our purchase history and so on. This \ndata is often used to train machines to predict which adverts we are most \nlikely to respond to. The idea is that the more data about us is gathered, \nthe easier it will be to build more accurate machine learning models of  \nour behaviour, and create systems to predict our actions. \nThis process of  things being inferred about us can have a range of  \neffects on our personal agency and more broadly on society as a whole. \nInferences based on our behaviour can help algorithms to determine \noutcomes that may benefit us individually, for example by steering us \ntowards a relevant piece of  information, a news story that we may find \ninteresting, or an advert for a product or service that we may value or \nmay improve our lives. Similarly, the data gathered about the behaviour \nof  groups of  people can, when aggregated and stripped of  personal \nidentifiers, be used to help with planning of  services, provide insights into \nmedical opportunities, establish ways of  challenging pollution, depriva-\ntion, and creating a better world. \nHowever, just as inferences can create positive outcomes, they can also \nrestrict, block, or prohibit access to alternative products, viewpoints, \nentertainment, or news stories.\nInferences or assumptions about who we are, based on general analysis \nof  behaviour, habits and browsing history can impact us. For example, \nit can affect how search outcomes are shown to us, how products are \nadvertised to us, how content such as videos are recommended, and even \nthe news and political campaigns we are shown. \nIn 2016, the Guardian newspaper revealed how filter bubbles impacted \n4. Yu, A. (2019) How Netflix Uses AI, Data Science, and Machine Learning — From A \nProduct Perspective. Medium. [online]. Available at: becominghuman.ai/how-netflix-uses-ai-\nand-machine-learning-a087614630fe \n\nAbout Data About Us7 \nthe political content that US voters saw and shared.\n5\n The power of  \nknowing what people are interested in, and only showing them content \nbased on that interest, is one of  the key elements of  how online political \nmessaging has become increasingly targeted to individuals, as opposed to \ngeographic areas.\n6\nBeing targeted as individuals in this way is a process that we have not \nexperienced before. Previously the concept of  encouraging people to \nengage with an idea, campaign, or product was general – all audiences \nor consumers were shown the same content. The ability to now show \nus nuanced content – aligned with interests or views that we have been \npresumed to have – is new to most of  us and one that is causing concern \nacross society. \nIn Eli Pariser’s 2011 book, ‘The Filter Bubble’, he explains that “you \nmay think you are captain of  your own destiny, but personalisation can \nlead you down a road to a kind of  informational determinism in which \nwhat you’ve clicked on in the past determines what you see next”. \nSince this book was published, arguably filtering and preference \nbubbles have become a standard practice online. As stated in an academic \npaper from July 2019 for the American Marketing Association:\n7\n “auto-\nmated recommendations are now ubiquitous in consumer domains”, and \nthat there is “a dangerous risk” that “consumers display overdependence \non algorithmic recommendations in a manner that may both reduce their \nown welfare and propagate biases system-wide”.\nSo how do we, as consumers and service users, feel about these sorts \nof  processes happening? In order to find meaningful ways to consider and \nanswer this, we need to understand what data about us is, how it is used, \nwhat it feeds into, how it can be shared, and how it can be protected. \nIt is for this reason we wanted to test people’s awareness of  data about \nthem, and develop narratives that could engage people, and help them to \ndecide and express how they felt about data, whether data about them \nshould be something they could own, or what sorts of  rights and respon-\nsibilities should exist around data. \nData ownership\nThere has recently been an increase in popular and political commentary \naround the notion of  ‘data ownership’, in both the UK and international \npress. \nDebates around whether we can have property rights over data have \nrumbled along for a few years, particularly in relation to the level of  \ncontrol that we can or should have over data about us, and how it can or \ncan’t be used. \n5. Carrie Wong, J. , Levin, S., Solon, O. (2016) Bursting the Facebook bubble: we asked voters \non the left and right to swap feeds. The Guardian. [online]. Available at: www.theguardian.\ncom/us-news/2016/nov/16/facebook-bias-bubble-us-election-conservative-liberal-news-feed\n6. Smith, A. (2018) Public Attitudes Toward Computer Algorithms. [online] Washington \nD.C. Pew Research Center. Available at: www.pewinternet.org/2018/11/16/public-attitudes-\ntoward-computer-algorithms/\n7. Banker, S. and Khetani, S. (2019) Algorithm Overdependence: How the Use of  \nAlgorithmic Recommendation Systems Can Increase Risks to Consumer Well-Being, Journal of  \nPublic Policy & Marketing. doi: 10.1177/0743915619858057.\n\nAbout Data About Us8 \nMore recently, the concept of  ownership as a determiner of  control has \nbeen mooted, in reaction to the Facebook/Cambridge Analytica scandal \nand the widespread misuse of  data about us.\n8\nWithin the corridors of  power in the UK the concept of  data ownership \nwas raised in relation to rights regarding data back in 2015 by the Liberal \nDemocrats when they proposed a Digital Bill of  Rights.\n9\n The Bill outlined \nthe principle “that personal data belongs by default to the individual to \nwhom it refers; that the individual citizen has a right to access all their \nown data, in an open digital format; and, where reasonable, individual \ncitizens can decide who else has access to their data”. \nThen in 2018, prior to the passing of  the Data Protection Act 2018 \nand the implementation of  the General Data Protection Regulation in \nMay 2018, the Labour Party briefly proposed an idea for a Digital Bill of  \nRights.\nOne of  the rights proposed was of  ownership. It stated as Article 6 of  \nthe Bill the idea that “every data subject has the right to own and control \nhis or her personal data. Every data subject is entitled to proportionate \nshare of  income or other benefit from his or her personal data as part of  \nthe right to own”. \nDuring a Bill debate in the House of  Commons, the then Shadow \nMinister Liam Byrne described the debate about who owns the copyright \nto data, or how new data could be created by joining data with someone \nelse’s data, as “vexed”.\n10\n He also said that “the question of  who owns \nthe copyright, and therefore who owns the value of  data that is personal \nin origin, is only going to grow”. Very little meat was put on the bones \nof  what was meant by ownership of  data. With the Bill failing to go any \nfurther, the concept of  ownership has taken somewhat of  a back seat. \nThe notion of  having property rights over data was also aired by the \nformer Conservative Chancellor of  the Exchequer, now editor of  the \nLondon Evening Standard, George Osborne. In a speech he made in \nMarch 2019, Mr Osborne outlined clearly and concisely the way that data \nabout us is used to bring profit to advertising companies, rather than to \nus as the data providers.\n11\n His solution was ownership: “say you had the \nright to take your accumulated data from one producer and share [it] with \nanother that offers you something better in return [...], say social media \ncompanies had to pay you for using your data. Say it became an asset, or \nperhaps even a reward for your labour”.\nThe idea of  determining a value for personal data, or developing an \napproach based on the idea of  personal copyright law, was raised by musi-\ncian and entrepreneur will.i.am in an article in The Economist in 2019.\n12\n \n8. Wikipedia. Facebook–Cambridge Analytica data scandal [online]. Available at: \nen.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal \n9. Archer, L. (2014) ‘Liberal Democrats plan new Digital Bill of  Rights’ [online]. Available at: \nwww.libdems.org.uk/new_digital_bill_of_rights\n10. Byrne, L. (2018) Data Protection Bill [Lords] Deb, 22 March 2018, c301) London: \nHansard. Available at: www.theyworkforyou.com/pbc/2017-19/Data_Protection_Bill/07-\n0_2018-03-22b.297.1?s=%22bill+of+data+rights%22+section%3Auk#g307.2\n11. Osborne, G. (2019) The Politics of  Newspapers www.standard.co.uk/comment/\ncomment/george-osbornes-hugh-cudlipp-lecture-the-politics-of-newspapers-a4085671.html\n12. will.i.am. (2019) We need to own our data as a human right—and be compensated for it \nThe Economist. [online]. Available at: www.economist.com/open-future/2019/01/21/we-need-\nto-own-our-data-as-a-human-right-and-be-compensated-for-it \n\nAbout Data About Us9 \nIn it, will.i.am suggests that “the ability for people to own and control \ntheir data should be considered a central human value. The data should \nbe treated like property and people should be fairly compensated for \nit”. He goes on to say how, as a musician, he benefits from the copyright \nsystem and that the same rules should apply to personal data. \nWhile we are mainly interested in the UK and European approaches to \ndata protection and rights, since the majority of  big tech companies that \nwe share data with are American, it is worth checking what is being said \nin the US, in relation to controls over personal data. will.i.am’s views, or \nthose that are similar, are also felt by others.\nIn the wake of  Facebook/Cambridge Analytica, recent commentary \nhas focused on the idea of  ownership being a way of  enhancing data \nrights. This has come from a range of  people, such as Senator John \nKennedy, Silicon Valley, and former Facebook investor and advisor Roger \nMcNamee. \nSenator Kennedy is one of  a number of  representatives in the US trying \nto address data protection concerns. In March 2019 he proposed a three-\npage Bill: the ‘Own Your Own Data Act’.\n13\n Meanwhile, Senators Mark \nWarner and Josh Hawley are seeking to make social media companies \ninform people of  the value of  personal data collected about them.\n14\n \nIn his 2019 book, ‘Zucked’, Roger McNamee writes about attempts \nby Senate representatives to get a data privacy bill of  rights through \nCongress that would “actually restore ownership and control to users”. \nHe admitted this would be hard but felt that the concept of  “owning your \ndata” was a declaration that would promote privacy and freedom. Further \nproposals in the bill covered the right to “opt in” to data being shared as \nopposed to being required to opt out, and a range of  other rights similar \nto the EU’s General Data Protection Regulation (GDPR) such as the right \nto obtain, correct or delete personal data; the right to be notified about \nbreaches; and the right to data portability.\nWhile these are just a flavour of  the issues people are grappling with, \nthey all acknowledge that the data-exchange we experience is unbalanced. \nThe process of  giving up data for free services has, in reality, become an \nunfair and inequitable value-exchange. But is data ownership desirable, or \neven possible? \nData rights and responsibilities\nThe meaning of  the term ‘rights’ depends on its use and context. Broadly \nspeaking, rights can be social, legal or ethical principles. To have a right \nto something means that we have permission to do it or are entitled to it, \nand that other people are responsible for enabling us to have it. This is \ncalled a ‘positive right’ because it requires action. We can also have ‘nega-\ntive rights’, which require inaction. We might have a positive right to vote, \nbut a negative right not to vote.\n15\nHaving corresponding responsibilities is something that all of  the \n13. Kennedy, J. (2019) Own Your Data Act. Washington D.C. Available at: www.govtrack.us/\ncongress/bills/116/s806/text \n14. Hart, K. (2019) Scoop: Bipartisan senators want Big Tech to put a price on your data \nAxios [online]. Available at: www.axios.com/mark-warner-josh-hawley-dashboard-tech-data-\n4ee575b4-1706-4d05-83ce-d62621e28ee1.html\n15. Wikipedia. Rights [online]. Available at: en.wikipedia.org/wiki/Rights\n\nAbout Data About Us10 \ndifferent types of  rights have in common. Human rights are specific, apply \nto everyone, and are defined and protected by law. It is the responsibility \nof  governments to act in certain ways – or to refrain from certain acts – to \npromote and protect human rights and fundamental freedoms of  their \ncitizens.\n16\n For example, if  we have the right to education, as we do under \nArticle 2 of  the Human Rights Act, then the State is responsible for \nproviding us with access to education.\n17\nOther sorts of  rights are more like normative, social rules established \nby a group, community or society. These are not necessarily protected \nby law, but they reflect standards that have been agreed on by a group or \nsociety, and, in order to make sense as rights, they must have people with \nthe responsibility to protect them.\n18\n \nLike ‘rights’ in general, the terms ‘digital rights’ and ‘data rights’ have \nbeen used in many different ways, with many different and overlapping \nmeanings. Many people talk of  ‘digital rights’ or ‘data rights’ in relation \nto intellectual property, while others use them in reference to relevant \nhuman rights and legal rights that exist to protect people’s freedoms to \naccess and use digital media or data.\nIn this report, we refer to ‘data rights’ as the rights that we (should) have as \nindividuals or groups around data. They might be rights to access data, with-\ndraw data, or even benefit from, or not be harmed by, data’s use or impacts. \nThe Open Data Institute’s theory of  change promotes ethical con-\nsiderations to data collection, management and use, and equity around \nwho accesses, uses and benefits from data. This relies on governments, \nbusinesses, civil society, and individuals themselves being responsible \nfor ensuring ethics and equity, through their actions or inactions.\n19\n This \nrelates to ‘data rights’ as normative, ethical and legal constructs. \nIn the UK and Europe, the use of  personal data is controlled by data \nprotection law. Since May 2018, the GDPR has been enacted by all EU \nmember states, including the UK, which has enshrined GDPR into law in \nthe Data Protection Act 2018.\n20\n \nThe regulation is based on data rights and responsibilities. People have \neight data protection rights under the GDPR, they are: \n1. The right to be informed – ie be told what is happening with \ndata about you\n2. The right to rectification – ie if  the data is inaccurate, have the \nright to ensure it is amended, corrected and made accurate\n3. The right to access – ie any organisation must be able to provide \nyou with access to data about you, if  you request it\n4. The right to restrict processing – ie you have the right to ensure that \n16. Un.org. (2019). Human Rights. [online] Available at: www.un.org/en/sections/issues-\ndepth/human-rights \n17. Equalityhumanrights.com. (2019). Article 2 of  the First Protocol: Right to education | \nEquality and Human Rights Commission. [online] Available at: www.equalityhumanrights.\ncom/en/human-rights-act/article-2-first-protocol-right-education\n18. Wikipedia. Rights [online]. Available at: en.wikipedia.org/wiki/Rights\n19. Open Data Institute (2019) Our Theory of  Change [online] Available at: theodi.org/\nabout-the-odi/our-vision-and-manifesto/our-theory-of-change/#1531394343108-b226e61c-833d\n20. UK Parliament (2018) Data Protection Act 2018 [online] Available at: www.legislation.\ngov.uk/ukpga/2018/12/contents/enacted\n\nAbout Data About Us11 \ndata can be held but not processed any further \n5. The right to erasure (also known as the right to be forgotten) – ie \nthe right to request that a company deletes data about you if  you \nask them to\n6. The right to data portability – ie data about you must be provided \nto you securely, and in a readable and portable format, so you can \nshare it with other organisations if  you wish\n7. The right to object – any organisation must respond to you if  you \nraise a concern with them about how data about you is being used \n8. Rights relating to automated decision making and profiling – you \nhave the right to ask an organisation to restrict the use of  data \nabout you for profiling or automated decision making, and to have \na human point of  view and decision made. It is not always clear \nhowever when such activity is taking place. \nOrganisations using data have the responsibility of  determining the lawful \nbasis for processing data. This includes ensuring that they only hold the data they \nneed, that it must be accurate and up to date, that it shouldn’t be held for longer \nthan is necessary – particularly if  it can identify us – unless it is for scientific, \npublic interest or historical research purposes. It must be secure, and anyone hold-\ning the data must be accountable and responsible for demonstrating compliance \nwith the principles. In certain cases, organisations have rights to access personal \ndata if  they can demonstrate a legitimate, legal or public need for it. \nSince its launch, the GDPR has been seen as a welcome and clearly outlined \nframework for data protection law. It is not perfect but as the next step of  data \nprotection in a data-driven world, it is seen to be a pretty solid foundation. \nThere are widespread calls for GDPR-type protections to be adopted inter-\nnationally.\n21\n Already, the GDPR has been used as the framework by countries \noutside of  Europe to build their own data protection laws around. Brazil, for \nexample, signed off on a General Data Protection Law at the end of  2018,\n22\n which \nadopts the GDPR concepts of  data subjects, data controllers, data processors, \nand it also develops standards for consent.\n23\n California has recently passed the \nConsumer Privacy Act 2018, which will become law in 2020.\n24\n The Act will offer \na range of  GDPR principles, such as the right to data deletion, transparency of  \nhow data is used, and the right to tell a business not to sell personal data to a third \nparty including as an opt out. In India, the Personal Data Protection Bill has been \nproposed as a legal framework to establish limits on how data can be collected \nand processed with regards to necessity, proportionality, and fairness.\n25\n21. Cook, T. (2019) You Deserve Privacy Online. Here’s How You Could Actually Get It \nTime [online]. Available at: time.com/collection/davos-2019/5502591/tim-cook-data-privacy/ \n22. Brazil National Congress (2018) the “Brazilian Internet Law”. [online] Available at: \nwww.pnm.adv.br/wp-content/uploads/2018/08/Brazilian-General-Data-Protection-Law.pdf\n23. DLA Piper. Data Protection Laws of  the World. [online]. Available at: www.\ndlapiperdataprotection.com/index.html?t=law&c=BR&c2=\n24.  SB 1121, Dodd. California Consumer Privacy Act of  2018. [online].leginfo.legislature.\nca.gov/faces/billTextClient.xhtml?bill_id=201720180SB1121\n25. Srikrishna Committee (2018), The Personal Data Protection Bill, 2018. [online]. Available \nat: dataprotectionactindia.com/wp-content/uploads/2019/01/Personal-Data-Protection-Bill-\nIndia.pdf\n\nAbout Data About Us12 \nWhat we did\nOver the spring and early summer of  2019 we convened two focus groups \nand one participatory workshop in London. Our aim was to understand \npublic feelings towards data and their reasoning behind it. \nWe wanted to understand how people responded to and engaged with \nthe concepts of  ‘data ownership’ and ‘data rights’, and find narratives that \nwould help people to understand, decide, and express the relationship \nthey have with data about them. We chose to use research methods that \nwould allow us to capture qualitative data, holding conversations with \nsmall groups of  people, listening to and learning about people’s personal \nexperiences and insights. \nWe worked with an independent research company to recruit people \nto take part in these sessions. The sessions were held at the weekend and \nparticipants were paid for their time in order to minimise barriers to \nparticipation. \nTo ensure a diversity of  views, the participants were selected to \nrepresent a range of  ages, ethnicities, abilities, and socioeconomic back-\ngrounds. Participants were selected to ensure that the group had varied \nattitudes, based on their answers to the questions: ‘Do you think the \ninternet is a positive thing or not?’ ‘Where do you place yourself  on the \npolitical spectrum left to right?’ \nThe focus groups took place over a day in April 2019. There were two \ngroups, each attended by 15 participants. In these, we tested people’s \nunderstanding of  data, of  ownership and of  rights, and whether data \nownership or data rights appealed to them as concepts.\nThe workshop took place over a day in June 2019. It was attended by \n13 participants. In the morning, we tested three stories about data about \nus being used in different ways – in particular, how engaging the stories \nwere, whether they were informative or made the participants feel differ-\nently about it. In the afternoon they were asked to develop the stories in \ngroups. They were then joined by four more participants who were tasked \nto be non-biased ‘judges’, and provide feedback on their stories – how \nclear, engaging, and informative they were.\nOne limitation of  our research was its geographic focus on Greater \nLondon. It would be good to develop the work to explore what attitudes \nare felt about data about us in other regions of  the UK and abroad.\n\nAbout Data About Us13 \nThe focus group \nWhat we tested and learned\nOwnership and control \nWe split the two focus groups: one to focus on exploring the concept of  \ndata ownership, and one to explore the concept of  data rights. \nDuring the data ownership focus group we asked participants what \nownership meant to them as a concept, and then asked them to describe \ndifferent types of  data. We then paired ‘ownership’ and ‘data’ together \nto understand whether the idea of  ‘data ownership’ resonated with our \nparticipants. \nWe conducted the same process with the second focus group, replacing \nthe concept of  ‘ownership’ with ‘rights’. \nThe participants had broad takes on what data is. One person ex-\nplained to us that: \n“Data is now literally anything you can put into a computer. So it can be it \ncan be your words, it can be your image, it can be your opinion, numbers. \nYeah, I think that’s maybe what some people have forgotten that as soon \nas your image becomes data it can be transferable manipulated, whatever. \nYeah, it can be stored”.\nParticipants told us that ownership meant control, “freedom to \nchoose” and “decision making rights”. They suggested we can own not \nonly physical goods, but also intangible things like opinions: \n“Yes. I own my vote, no one can take it away from me”.\nIn some cases people said that ownership gives us the ability to exclude \nothers, or to collect revenue from people if  they use things that we own. \nOverall, we learned that ownership provided people with a sense of  \nsecurity and freedom of  choice. Some people highlighted that ownership \nis a privilege that we don’t all get to benefit from. \nWhen it came to the concept of  data ownership, one person expressed \nthat data ownership seemed like a “myth”. They said: \n“I don’t think we’ll be able to own our own data, we’re way too far gone. \nIt’s a myth. They’ve got all of  our data, and maybe only if  you haven’t \nbeen born yet. I think it depends how much data they have on you, I think \nit’s very intrusive. And if  I had my own way, I’d have all my data back”. \nThey suggested that we could only own the things we have control \nover, and that once we share data we lose ownership over it: \n\nAbout Data About Us14 \n“You have a choice of  how many physical opinions you share, your life is \nbroken down into core data I guess, like date of  birth, name, etc. Now I \nimagine most people only want to share the bare minimum, and I guess \nthat way, the more you share, the more you lose ownership”. “I think \nownership is a very strong word to use in a very wooly conversation. I \nthink we have all agreed we don’t own our data. Do we even want to own \nour data?” \nOne person said that that they didn’t believe that we even own virtual \nitems that we buy online: \n“I think ownership is becoming a blurred term now. People have physical \nobjects that they physically own, but my son buys purely virtual items, ie \nguns on his video game. But he has all these things that he thinks he owns, \na catalogue of  things he owns, but it’s virtual, intangible object and he’s \nprobably just bought a licence”.\nThis point was reiterated by another person, who said: \n“I think everyone else defines the terms on which you can own something \n[...] so I had a film with a streaming service, but they shut down the \nservice, I no longer have access to that film. So I’m told I own my data, but \nrealistically, I don’t own it on my terms, I own it on someone else’s terms”. \nSome people said they thought that, because of  the way terms and \nconditions are written, ownership now belongs with companies not with \nindividuals. For example, we were told: \n“I now know that whatever I post on Facebook or Instagram belongs \nto them and that includes my face [...] I think there is a clause written \nsomewhere deep down where I’ve signed up to that”. \nOther people said they thought the words ‘possess’ or ‘access’ were \nbetter for describing the relationship that these commercial entities had \nwith data about us. \nThere was an assumption held by some that the companies who hold \ndata about us have a responsibility to ensure it is used correctly and held \nsafely and securely. \nJust as will.i.am raised copyright as a way of  having control over how \ndata about us is used, people in the focus groups explored the same idea. \nThere was agreement that intellectual property and copyright was an \ninteresting idea. But in the online world the issue of  copyright is contro-\nversial.\n26\n The creative freedom for people to make new user generated \nonline content based on content created by others (which may have their \ncopyright attached to it) is a sensitive issue and gets to the heart of  owner-\nship and permission to use or even to sell data. \nUltimately, people felt strongly about wanting to control data about \n26. Reynolds, M. (2019) What is Article 13? The EU’s divisive new copyright plan explained. \nWired [online]. Available at: www.wired.co.uk/article/what-is-article-13-article-11-european-\ndirective-on-copyright-explained-meme-ban\n\nAbout Data About Us15 \nthem, but they did not consider ‘data ownership’ to be a realistic concept \nor one that inspired feelings of  confidence or safety:\nOne person said: \n“We don’t have the control we think we’ve got, because people are able to \nuse it even if  we don’t give them permission”.\nAnother said: \n“Once it’s online you don’t have control. You can strip all the data, but \nonce you share something it can be screenshotted and passed around. \nOnce you put something out there, you have no control what happens to \nthat photo anymore”.\nAnother said: \n“The only point we own data is when its within ourselves, and as soon as \nyour reveal it, you don’t own it”.\nPart of  why people at the focus groups were sceptical of  data owner-\nship was the issue of  how to value data about us. The value that data has, \nafter all, depends on many things and is an ever-changing concept. What \nhas certain value to one person or entity has a different or no value to \nanother. \nOne person said that: \n“Every bit of  data has a value, it just depends on how you segment it, \nwho’s looking at it, what people want to achieve what that data, whether \nit’s to give a more personalised service, understanding your political or \nfinancial services. To a financial service my data is valuable, but the same \ndata is not valuable to my friends”. \nIt is hard to disagree with this statement. Who, what and how we could \ndefine the value of  data are the initial questions to be asked. They were \nnot ones we answered in this project but are being asked more broadly by \neconomists, policymakers and businesses.\nIt was this level of  complexity that led the members of  the public we \nspoke with to decide that ownership over data about us is not something \nthey found to be logical or desirable. However, they emphasised that they \nwanted data about them to be used responsibly, and some suggested they \nwanted to have more choice, control and agency in how they share data \nabout themselves.\nRights and responsibilities\nWhen we discussed the overall concept of  rights (rather than specific \nrights and responsibilities) with the second focus group, people said \nthey felt that having rights meant being safe, having freedom within \nreason, having control, and that rights were an ownership of  some kind. \nParticipants agreed that there were different kinds of  rights – some legally \nprotected and others just perceived, like the “right to be offended”. \nThey raised many similar themes as the first focus group did around \n\nAbout Data About Us16 \nownership, but focusing on rights more quickly inspired the group to start \ntalking about frameworks and responsibility. \nMany people said they saw rights as generally positive things, and only \nnegative if  they impinge on other people’s rights. They said that the rights \nof  different people could come into conflict due to changing norms, and \nthat new legislation is needed to protect those rights. \nEveryone we spoke to in these focus groups, and in the later workshops \n(set out below) had heard of  the GDPR and could outline the regulation’s \nbasic premise. For example, we were told: \n“Under data protection legislation it’s all written down isn’t it? What the \nresponsibilities are, and who has it. Whether anybody actually complies \nwith them is another matter, isn’t it?” \nHow the GDPR was implemented was an issue that a number of  \npeople raised. One person summed it up by saying: \n“Some companies did a much better job than others, some said: ‘look \nwe’ve got that information and we’re going to hang onto it unless you \ntell us otherwise’, some people went on and on and on [...] eventually \nyou could just be bamboozled with information so that you can’t make a \ndecision”. \nSome could tell us one or more of  the rights that the GDPR has given \nus. For example, one person said they had undertaken a subject-access \nrequest:\n“I did a data-access request for what a company had on me, and I was \nreally shocked at what they had on me. It was about me personally [...] \n99% of  it was banal, functional information, but there’s 1% of  it that was \noutrageous”. \nAnother person told us that:\n“I’m a fan of  GDPR. I’ve used it and had stuff moved”.\nSome people we spoke with said they had engaged with the GDPR as \npart of  their work or business practice. Their views very much depended \non the jobs they had. For example, we heard from one person that: \n“GDPR was a pain in the arse, but it did some good”. \nAnother person said: \n“No one understood it for ages, but I think it can be good and it can be \nbad”. Someone else said: “It negatively affects business because [business] \nis all about seamless service, where they know everything about you. \nWhereas now they have got to ask”.\n\nAbout Data About Us17 \nA conversation took place around how rights are drafted, legislated \nand enforced in order to ensure fairness. Some participants said that a \nbroad spectrum of  people needed to be involved in crafting them, and that \nit should be a democratic process. Others were keen to ensure that people \nwho understand data are involved. One person said that: \n“With legislation going forward, there’s going to be a lot of  tightening \nup around data, and I hope that is done by people who understand data, \nrather than politicians who have been pushed into a certain office and have \nno understanding of  it, genuinely, or the implications of  it. I think irrel-\nevant of  political parties, laws are passed by people who don’t understand \nthe details in the first place”. \nNew rights come with new responsibilities. The group felt that legisla-\ntion needed to be enforceable, with one participant summing up that: \n“Rights have got to be enforceable. Otherwise they’re not rights, they’re \njust wishing something”.\nThe group discussed the difficulties associated with trying to enforce \nrights on a global level. This is an interesting point in relation to whether \nthe internet has borders, and whether an international approach to rights \nfor data can be put in place. Determining an international treaty of  data \nrights was considered by people in the focus groups to be ideal but too \ncomplex. One participant said: \n“data doesn’t respect boundaries”.\nWhen asked what they thought of  the term ‘data rights’, some people \nfirst said that it implied protection: \n“Part of  data rights is to have your data protected. Like your bank \ninformation, your medical information, that kind of  thing”.\nThere was then a long discussion about protection, with many sharing \na view that organisations asking for data about us had a responsibility to \nprotect it, but that it was often unclear whether it happened in practice. \nOne person said:\n“So, I don’t know how it works. And they know I don’t know how it \nworks. We’re trusting them. And when I say I agree at the bottom of  their \nthings I don’t read it”.\nOn terms and conditions, another person said that: \n“Nobody reads it. Apparently you gave Facebook permission to have your \nface, which I never knew”.\nThe concept of  protection led the group to discuss the concept of  \ncontrol – in particular how out of  control they felt when it came to know-\ning or choosing how data about them was to be used. Some people felt \n\nAbout Data About Us18 \nthat the very notion of  being asked to share data about them with services \nonline meant losing control: \n“I think once it’s out there the notion of  having control at all. I think it’s \ngone completely, really. And truly. I think that’s the bottom line. I hate to \nsound pessimistic”. \nAnother person described a sense of  resignation about the control they \nhave over data about them they have shared online. They said they felt \nthat \n“trying to get something back off the internet is like trying to take the piss \nback out of  the swimming pool”. \nAnother person said: \n“I feel like we need to be educated. Like, when I sign up to a website, when \nI do my banking, when I do anything on the internet, and then they ask me \n‘Do you accept these terms and conditions? Give me your name’, I have no \nidea where all that’s going. I just feel like I can’t do anything unless I give \nmy data away”.\nThe group generally agreed that they felt a loss of  control and that \nthey were not happy with the current situation. \nOne person said they would like to have a ‘dial’ that we could turn \nin order to choose how much data about us is shared with a device or a \nwebsite: \n“I think we need a dial. One hundred percent let it all go you can know \neverything about me, or zero percent, you can know nothing about me. \nYou don’t even have to type it in, just give it literally a control”. \nThis idea was leapt on by others in the group who saw it as an oppor-\ntunity, with one person summing up the view of  the table as being: \n“We decide what we want to share”. \nWe also learned that people thought it would be useful to be given a \nmoment to decide whether they really wanted to proceed. One person said \nthat they liked being asked, ‘Do you want to delete this?’ but they would \nalso like to be asked, ‘Do you really want to post this?’ before pressing \nsend: \n“I do think we can make it very clear that we have to be careful and \ncautious. This ‘are you sure’ question that comes up when you’re deleting \nsomething? It should come up when you’re putting something in as well, \nshouldn’t it?”\nOne person said that overall: \n“We’ve got to learn to be responsible online and not be so impulsive. \n\nAbout Data About Us19 \nThere’s a lot of  impulsiveness. And also corporations have got to learn to \nbe responsible and open about what they’re doing about it”.\nThere were some strong opinions about companies collecting vast \nquantities of  data about us, and how they were using it. There was \ndiscussion about the positives that came with more personalised services, \nbut more of  the discussion seemed to focus on people’s concerns about \npersonalised services with a distinction being made between having the \nchoice to sign up for a service, and being shown unsolicited targeted \nadvertising. As three people said, respectively:\n“Profit. I mean, in the end we’re being targeted”. \n“They say that if  something’s free to you, then you’re the product”.  \n \n“They’re constantly coming to you, you’re not going to them [...]Yes my \nprivacy has been infringed. It’s in my home, I’m on my phone or whatever. \nI haven’t chosen to have these people talk to me”.\nConcerns about how responsible companies are with the data they \nhold were also raised, with one person saying: \n“We’re relying on the companies that we use to stay credible and use it \nwith integrity. If  that’s the case, that’s why we sign up, cause we’re assum-\ning they’re going to be responsible. If  we knew they wouldn’t be, I bet \nnone of  us would say yeah”.\nAs with the first focus group, participants cared about how data about \nthem was being used. Overall they were more open to sharing data about \nthemselves if  they could do so anonymously and it was being used for \npublic benefit. Some people suggested they would be happier to share \ndata about them if  it could be guaranteed that it wouldn’t be shared with \na third party. We were told that, most important of  all, people wanted to \nhave a choice in how data about them was shared and used.\nOne person said they didn’t want to own data about them necessarily, \n“but I do want someone to ensure that it’s not misused, or only used in a \npositive way. I want it regulated”. \nWhat we developed after the focus groups\nWe are often led to believe, by the press, parliament and in published \nsurveys, that the UK public’s knowledge and understanding of  data \nprotection is low. However, the discussions had between people in our \nfocus groups suggested otherwise.\nWhile people showed a broad understanding of  personal data, we \nfound that they wanted to be able to better describe when and why they \nwere comfortable, or uncomfortable, sharing data about them online.\nOne person said: \n“We have to differentiate between what’s personal, what’s sensitive, and \n\nAbout Data About Us20 \nwhat’s open. Because open it seems like none of  us care [about] [...] but \nwhat’s personal, and sensitive, that’s the question mark, and how can we \ncontrol it?”\nSome said they felt greater education was needed. While the GDPR is \nseen to have been helpful in outlining personal and sensitive data, it has \nbeen, for some, less good at helping people to understand that data about \nthem can have other uses, and can provide deeper nuances or insights into \nthem as individuals, groups or communities, and society as a whole. \nFor this reason, we sought to find a way to clearly differentiate between \nthe different types of  data about us. \nWe developed a graphic setting out these different types of  data, with \nexamples, in order to help people see how it can have different elements. \nSome data about us is deeply personal, and can be used to create insights \nand inferences. Some data about us is personal but, if  stripped back and \naggregated with data about other people, less individual in nature, and \nhelpful for wider decision making. \nThis tool was designed to help make people aware of  what kind of  \ndata about themselves they are being asked to share, what data they have \nto share and why, and what data they would prefer not to share. We felt \nthis would help people better understand what they are being asked to \nconsent to and assist with improving education. \nWe tested this tool in the workshop, and we have iterated on it since for \nmore clarity around societal data. Both these graphics are featured below.\n\nAbout Data About Us21 \nData about us: the four \ncategories\nAs we’ve explained, in this graphic tool we have sought to differentiate \nbetween and define the different types of  personal data about us. \nThe first two boxes from the left, ‘personal data’ and ‘sensitive data’, \nare clearly defined in the GDPR – the second two to the right, ‘behav-\nioural data’ and ‘societal data’, less so. \nWe presented our initial version (featured below) during a short \npresentation to the workshop groups. \n We have subsequently iterated in this graphic to provide more clarity \naround societal data.\nThis graphic is licensed under a Creative Commons Attribution-ShareAlike 4.0 International \nLicense \n\nAbout Data About Us22 \nWhat are the different types of data about us, and how are \nthey used? \nPersonal data as defined by the GDPR is any information relating to an \nidentified or identifiable natural person. Some of  it is data that we are \nfamiliar with sharing in order to be identified: our name, address, date of  \nbirth, telephone number, email address. \nIn a connected world there are increasing technical identifiers linked \nto our personal data: these are MAC addresses and IP (internet protocol) \naddresses. These can be used to identify the devices that individuals use, \nand hence the individuals themselves. \nAll of  these identifiers link directly to individuals but may also link to \nothers. For example, we may tell other people our name, we may share \nour birthday with people we don’t know, our devices are shared amongst \nfamily members and our location is, depending on where we are, an \nidentifier we share with everyone else at that location at that time.\nThe GDPR lists ‘cookies’ as an online identifier. Cookies are complex \nthings. They come in a range of  different flavours. Functional cookies, \nauthentication cookies and unique identification cookies are cookies \nthat act like a memory recalling where we have been online and what \ninformation we have given to a website. These make pages on the internet \nwork, they leave a crumb enabling the website to remember that we have \npreviously visited it. These cookies tend to be persistent – they embed \nthemselves in our browsers. Sometimes they are used to mirror us between \ndevices so that browsers remember us across a phone, laptop, or desktop. \nThese cookies are focused on recognising it’s us and remembering the \ninformation that we have given to the website before, such as a password, \nour bank details, etc.\nSensitive personal data is data that describes integral features of  who \nwe are: our ethnicity, gender, genome, biometrics (such as our fingerprint, \nDNA, facial biometric, voice and gait – all of  which are completely \nunique to us as individuals), sexual orientation, and sex life. It also \nincludes health data, educational data, employment history, criminal \nconvictions, political party membership, amongst others. \nDepending on the circumstances, making someone’s sensitive personal \ndata public, or misusing it in some way, could cause them serious harm. \nThis data explicitly provides more detail about us, and deeper insights \ninto us. It’s data that, if  misused, could lead to bias, discrimination or \nother harmful situations. \nThere are a range of  laws and rights to protect us from such harms, \nsuch as the Human Rights Act and the Equality Act, which protect a range \nof  human characteristics.\n27\n The GDPR similarly makes clear that, because \nthis type of  data is more focused on specific aspects of  a person, special \nprotection is needed to ensure that organisations wanting to ask for this \ndata, use it, share it, store it etc have a specific need and clear reason as to \nw h y.   \nIt is worth noting again that while this data is specific to individuals, \nit also describes people around them. For example, we share our genome \nwith members of  our family (even distant members or people we have \n27. Equality and Human Rights Commission (2019). Protected characteristics [online]. \nAvailable at: www.equalityhumanrights.com/en/equality-act/protected-characteristics\n\nAbout Data About Us23 \nnever met). If  we are asked to make a decision about how this data about \nus is used, we may find that we have to make a conscious decision about \nhow its use might impact others. \nBehavioural data is not specifically defined by the GDPR in the same \nway as personal or sensitive personal data. It includes data about an \nindividual’s behaviour and data that organisations use to infer, or guess, \nhow we will behave. \nBehavioural data tends to be collected via lots of  different types of  \ncookies, namely: performance, tracking, third-party, targeting, advertis-\ning, and social media cookies. The focus of  these cookies is to track, \nmonitor and analyse the behaviours we demonstrate when we are online: \nthe what, when, why, and how of  our online activity. \nBehavioural data is also collected when we buy things with credit \ncards, store cards or, in certain cities, travelcards. \nThe adverts and content that we see online are often sold by AdTech \nand social media companies using cookies. These cookies are able to \nidentify us, so the GDPR offers some protection to us in terms of  personal \ndata, but currently the companies that use these types of  cookies are \npaying no attention to the law, a problem highlighted in a recent report \nfrom the Information Commissioner’s Office (ICO) detailing the AdTech \nindustry’s use of  cookies.\n28 \nBehavioural data is a deeper, more complex type of  data about us. It is \nnot just used to identify us, but also to understand us from watching and \nanalysing our behaviours. This can be done both on- and offline. \nTake location data. Our specific location when we stand in the street \nis an ‘identifier’ pinpointing us to a certain location, but if  that identifier \nstays on and moves with us it creates a constant real-time stream of  loca-\ntion data about us, giving an insight into our behaviour, not just a way \nof  identifying where we are. Combine real-time location data with data \nabout purchases we’ve made on our contactless bank card, while on that \njourney, and a more nuanced and detailed picture is created about us. \nThe same applies to a journey that we may take online. The website we \nvisit is the location, but what we browse gives an insight into our person-\nality – taste, mood, preferences etc. The insight of  one website by itself  is \nfar from nuanced or accurate, which is why these cookies stick with us as \nwe go from one website to another.\nThis ‘behavioural data’, as we’ve defined it, is data that historically \nhas not been captured before in such a constant way. It is this data about \nus that seems to cause the most concern and anxiety for people. This is \nthe data that people want to have more control over, which is why we have \ntried to define it more clearly, and what it can be made up of, so people \ncan understand what is being gathered when they are presented with a \nchoice of  cookies, to accept or deny. \nWhile cookies are covered in the GDPR, overall regulation for cook-\nies comes from the Privacy and Electronic Communications Regulation \n(PECR).\n29\n28. Information Commissioner’s Office (2019). Update report into adtech and real time \nbidding. London: Information Commissioner’s Office. Available at: ico.org.uk/media/about-the-\nico/documents/2615156/adtech-real-time-bidding-report-201906.pdf\n29. Information Commissioner’s Office (2019). What are the Privacy and Electronic \nCommunications Regulations? [online]. Available at: ico.org.uk/for-organisations/guide-to-\npecr/what-are-pecr \n\nAbout Data About Us24 \nSocietal data is data about us that incorporates elements of  personal data \n(for example our location, our health, our energy use data) but does not \nneed to include personal identifiers, that link back to us as individuals, in \norder to be useful. \nSocietal data forms part of  important data infrastructure, which is \ncrucial to understanding and meeting our society’s needs. Examples of  \nsocietal data are road and rail traffic, footfall in cities, disease statistics, \nand school attendance. \nIf  handled correctly, societal data should be made to be as open as pos-\nsible, with the data being open for anyone to access, use or share.\n30\n Open \ndata can help governments, businesses and communities make decisions \nthat benefit society as a whole, create products or tools such as maps or \ntransport alerts, spot patterns in healthcare, or determine where local \nservices are needed most. For these uses, societal data about us is often \ncombined with non-personal data, like the names of  towns, the location \nof  bus stops, or the temperature at a particular time of  day.\nEven though societal data is often about us, in that it might include \ndata about our commutes to work, in order to bring value it does not \nrequire anyone using it to know precisely who we are. Often this data is \naggregated, focusing on total numbers of  people rather than individuals. \nThe potential for us to be identified, or someone to learn more about \nus as individuals from aggregated data, will depend on how many ques-\ntions are asked of  the data, and how much the person looking already \nknows or can find out about us from other sources. There is with any \ndataset the risk that a person can be identified or re-identified, even if  we \nare told data about us is anonymised. \nOf  course some societal data will include, or generate, behavioural \ndata. Knowing how many people are getting the 5:15pm train out of  \nManchester Piccadilly station, for example, gives an insight into people’s \ntravel behaviour. Understanding how many people have contracted the flu \nin Plymouth in the month of  March gives insights into people’s health. An \norganisation might use this aggregated data to infer something about the \nbehaviour of  either specific individuals or groups of  people. \nThe GDPR requires that societal data, as we have defined it, to be \nhandled in clearly defined, specific ways. Just as with all data it must be \nhandled lawfully, fairly and transparently. Anyone intending to process \nsocietal data should ensure they determine the appropriate lawful basis, \nbe it consent, contract, legal obligation, vital interest, public task, or \nlegitimate interest. \nWhile societal data can bring opportunities, we must not be com-\nplacent and assume that all opportunities will benefit all of  us fairly. A \ndemocratic debate about the use of  data about us for societal decision \nmaking, and how the benefits are shared, is necessary. Just as with all data \nuse, ensuring that the purposes of  the use are legitimate, necessary, and \nproportionate is critical. \n30. The Open Data Institute (2019). What is open data and why should we care? [online]. \nAvailable at: theodi.org/article/what-is-open-data-and-why-should-we-care \n\nAbout Data About Us25 \nThe workshop\nWhat we tested and what we learned\nBased on the conversations and findings from the two focus groups in \nApril, we felt the next step should be to test narratives about data. We \ndecided to do this within a workshop environment, so people could work \ntogether in groups to construct stories and test their knowledge. \nBefore the workshop, we write three stories about data to test. Each \nstory was presented as a scripted conversation between two people. We \nincluded a small context-setting section before introducing a dialogue \nbetween two people talking about a real-life situation in which data about \nthem was being used. \nWe asked participants to capture their initial reactions to each story in \na feedback table before opening up to table conversations facilitated by a \nmoderator and a note-taker. \nWe wanted to learn if  talking about different uses of  data make people: \n •\nMore aware of  the various types of  data about them which are\nused within society .\n •\nIncrease their interest in and demand for rights and responsibili-\nties around data.\nWe wanted to learn what feelings different situations and examples \nof  data use generated amongst the participants. Did they find the story \nappealing, reassuring, disgusting, or annoying? We also wanted to explore \nconcepts of  fairness and what people saw as a good or a bad use of  data \nabout them. \nWithin three table groups of  four to five, participants were asked to \nengage with the stories one at a time, allowing notetakers to capture \nresponses to each story. Table facilitators introduced each of  the stories \nusing the context-setting section, before inviting them to role-play in pairs \nwith the scripted conversations. They were then asked to discuss what \nthey thought or felt, and how they interpreted the stories. \nAfter all the tables had fed back with their responses to the stories, we \npresented and discussed the different ‘types of  data about us’ graphic [as \nexplained in the previous section].\nAfter lunch, we invited each table to work on and present a way of  \nretelling one of  the stories. This could be a poster, a play, or a presenta-\ntion. We provided materials to help with generating something visually \ninteresting. \nThey each presented their new stories to the group, along with new \nparticipants who were brought in as ‘judges’. Not having heard the stories \nbefore these judges were intended to offer fresh, balanced judgement \naround the stories that most resonated with them, and increased their \n\nAbout Data About Us26 \ndemands for data rights and responsibilities. \nStory 1\nStory 1 was a made up conversation between two people about how data \nabout us can be used by our employers to make decisions about us in the \nworkplace. This was described by presenting the scenario of  a woman not \ngetting a job based on the interpretation of  a photo of  her that was posted \nby friends online, and a scenario of  data collected from someone’s ‘wear-\nable’ worn at work being used to monitor their productivity. \nThe intention of  the story was to test how people felt about the ways \nin which data about us can be interpreted, and to test concepts of  fairness \naround automated decision-making within employment practices. \nThis story had the most visceral reaction amongst participants, with \npeople expressing indignation, disgust, disbelief, discomfort, anger, a \nsense of  their privacy being invaded, and that the situations were “unfair”, \n“wrong”, and “sinister”.\nThe initial idea of  a person being defined by a photograph of  them \nonline without consideration of  context, or a conversation with them, \nwas of  concern to the participants. It led to a conversation about the \nresponsibility we have towards friends and family when we post things \nabout them online. \nOne person told us: \n“I’ve got one friend who says, ‘don’t put any pictures of  me up on \nFacebook, Instagram [...] Don’t put anything on it!’” \nAnother said: \n“It happened to my niece, someone put a photo on[line] and she didn’t \nwant it on[line], and it caused a lot of  problems. And in that case, you \nshould ask, you shouldn’t just presume they want the photo to go up”.\nGenerally people told us that they thought individuals should take \ninitiative in ensuring their safety and privacy, and take responsibility for \nhow and what data they were sharing online. But they also recognised that \npeople need a basic level of  education and awareness of  how social media \nworks, and how it can affect privacy. \n“I think people need to take a bit of  responsibility for themselves in using \nthose platforms, but it goes hand-in-hand with the company telling you \nhonestly as well, because there’s also a bit of  that Cambridge Analytica \nand Facebook using your data and people not being aware of  what it was \nbeing used for. I think it goes both ways [...] people have to take responsi-\nbility for using these platforms themselves, but I think the company has to \ntell you”.\nThe group discussed the importance of  boundaries between personal \nand work life, one person said they thought that monitoring via wearables \nis a misuse of  data, and that interpretations or judgements based on data \nout of  context are “wrong” and undermine privacy. There was a sense of  \n\nAbout Data About Us27 \nagreement that people shouldn’t be penalised for behaviour within their \npersonal lives that doesn’t have an impact on their work: \n“Why should an individual in their own spare time be penalised for what \nthey do outside their work? It’s nothing to do with in the workplace”.\nThe group suggested that without full context, assumptions can be \nmade about individuals that aren’t fair: \n“Computers are just brutal aren’t they? Whereas I suppose if  you had a \nboss who was a human, they might [think], ‘Oh she’s going to the loo, but \nI know her work, she’s a really hard worker, she’s not lazy”. \nAnother felt that: \n“If  you’ve got an algorithm looking at somebody, then the algorithm \nshould be challenged [...] it’s not necessarily true”. \nAnother person summed it up by saying that: \n“Data is not black and white. It’s how it’s read and it’s how it’s interpreted \n[...] We’re not all robots [...] Everybody’s got strengths and weaknesses”.\nThe concept of  data not being straightforward (“not black and white”) \nand the need for human interpretation and consideration was raised a \nnumber of  times across the workshop as a whole. \nIn the scenario, when the ‘opt-out’ option was presented as the deci-\nsion to quit a job (to escape its work-place monitoring), the group agreed \nit was only a viable choice if  other employment was available. \nOne said: \n“If  it was every single company, [so] you had to give away your privacy \nto get a job at all, then that would just be completely... just so alarming. \nThat’s an abuse of  human rights”. \nThe same participant went on to suggest that more should be done to \nhelp people opt out of  that kind of  treatment, especially as some groups \nare more vulnerable than others: \n“What about the people who really don’t understand it at all, [if] they \nwould like to opt out but they have no data literacy, they don’t understand \nthe tech, they don’t understand all of  those terms and conditions [that are] \nhard to read”.\nStory 2\nStory 2 was a conversation between two people describing how data about \nus (when anonymised, de-identified and aggregated) can be used for deci-\nsion making for public services. The example given was wi-fi connections \non people’s personal connected devices being tracked in order to track \nand analyse the flow of  people around a transport network. This story \n\nAbout Data About Us28 \nwas based in part on the tracking of  wi-fi connections on the London \nUnderground. \nThe intention of  the story was to test how people feel about data about \nus being used for decisions that aren’t about the individual, rather about \nbenefits to wider society. We wanted to understand what feelings this \nraised, and hear what protections (if  any) people wanted to see with the \ndata. \nThis story was found overall to be the least contentious. One partici-\npant said they felt \n“more at ease because it’s not as intrusive”. \nAnother said: \n“Yeah, they can know that for society and stuff, but not to just get more \nmoney out of  me and sell me stuff. I feel like for society for transport and \nenergy, I don’t mind them knowing”.\nWhile people were accepting and felt positive about data about them \nbeing used to make services better, they had some expectations about how \nit should be handled in order for them to feel completely comfortable. \nThey said the need for transparency is critical, and that they expect \norganisations or companies to be clear about what is being done with the \ndata. There was a feeling that if  we are not told what is being done then \nthe purpose may be harmful or unfair to us. \n“They should tell you. They should be transparent. And if  they’re doing it \ncovertly, what else are they doing with that information?” \nHow the ‘benefit to society’ would be defined was also an issue. In rela-\ntion to the use of  data about us in the transport story, one person asked: \n“Are they looking for where to make investment or looking for where to \nmake cuts?” Another said: “If  it [is] about knowing where people [are] \nto improve services that’s one thing, but we shouldn’t have to help them \ndecide [how] they get most money by where people stand [...] that’s \nanother thing, and I wouldn’t agree to that”. \nImprovements and efficiencies need to be visible and transparent for \npublic buy-in. One participant said that: \n“The thing I’m worried about is permissions”. Another said: “I would be \nworried about vulnerable people or children who struggle to understand \nwhat’s going on or what they’re giving permission for. I like the idea of  an \nindependent body overseeing what’s going on”.\nParticipants then suggested that there should be clear tracking notifica-\ntions or ‘requests’ that pop up on people’s phones; the ability to remove \ndata about them at a later point; an update after six months explaining \nwhat has been done with the data and any money generated if  the data is \nsold for any purpose. \n\nAbout Data About Us29 \nStorage and future use of  the data was also raised as a potential \nconcern. One person said: \n“It’s fantastic to be planning and making services better and having more \n[trains] at particular times. All that is absolutely fantastic. But once they \nhave this information, things might change. And I don’t like the thought \nthat data like this can be stored for years and years”. \nStory 3\nStory 3 presented a conversation between a couple who are planning a \nholiday. The scenario presented them searching online for the same flights \nand hotels, one using their phone, the other a laptop. The story sought \nto explain how our behaviour, the devices we use, and the way that data \nabout us is collected and analysed by online trackers such as cookies, can \nhave a direct impact on the content, offers, and prices that we are shown \nas individuals.\nThe intention of  the story was to test whether people were aware that \ncompanies collect behavioural data about us along with personal data \nabout us, and that this can directly affect what we are shown online. We \nwanted to find out how that made people feel, and whether they found it \nfair or unfair, or were indifferent about it.\nPeople’s initial reactions to the scenario were that they were confused, \nsurprised and curious. \n“I felt stupid, I didn’t realise what the companies do”, one person said. \nSome participants weren’t aware that companies may personalise pricing \nor content based on people’s profiles. One person said: “I’m astonished \nthat companies [...] are allowed to do stuff like that – purely based on [...] \nsomeone’s perceived wealth”. \nSome were confused as to exactly how the algorithms behind websites \nwork to create this personalisation. \nSeveral people related this story to analogies of  price discrimination \nin the non-digital world targetting people perceived as vulnerable or \nignorant, such as \n“Rogue traders [going] to an elderly person’s house and [saying] ‘Oh \nyou’re guttering’s broken, it’s going to cost £800,’” or the idea of  a car \nmechanic charging a woman more because “He’s thinking, ‘oh, I can take \nadvantage of  her because women don’t tend to know as much about cars \nas men’”.\nThe group discussed the difference between usual price-discrimination \nthrough peak supply-and-demand times (such as summer holiday prices \nunfairly penalising parents) and this situation, which they considered a \ngreater injustice. Price discrimination based on how far in advance some-\none booked a holiday wasn’t seen as controversial, as that was considered \na personal decision, but personalised tailoring of  prices was considered to \nbe unfair as the individual had no control over it: \n\nAbout Data About Us30 \n“Why is it more expensive? You’re obviously being watched on different \nwebsites”. \nIn particular, they thought this type of  activity targeted the vulnerable \nor ignorant. A couple of  people were surprised that the type of  device \nthey use can have an influence: \n“I wasn’t aware that not only [do] they take into account what websites \nyou’ve been on and what products you buy, but also the device you’re using \nand everything. I didn’t know that”.\nAdverts were contentious. One person said: \n“I don’t mind being recommended a different book, but I hate millions \nof  adverts”. Another said: “You can’t tell companies not to target ads at \nyou”. \nThis led to a conversation “the right to choose”, wanting to be able to \ngive a shop or service the ability to advertise to you. One person expressed \nconcern about her 12-year-old daughter being targeted by adverts, saying \nthe advertisers had no right to do that. In response, another person said: \n“We’re starting from the assumption that companies can do this, but [...] \nnone of  us have given permission”. Another person’s suggestion was that \n“Sometimes [if  I’m] sick of  the sight of  the same three t-shirts I’ve looked \nat or something, I’d rather have random adverts”.\nThe idea of  context being important – not just in terms of  accuracy, \nbut also fairness – was raised: \n“It’s not even accurate. They’ve got it wrong. Why can’t I challenge this? \nOh it’s so frustrating, they’re making guesses about me”. \nOne person said: \n“There’s loads of  people watching everything [I] do, and they’re all \nmaking a guess about what I am like [...] I just like to browse it. Sometimes \nI’m not even browsing for me. It’s for someone else”. Another said: “They \nneed a change in their business model. Right now, the more they know the \nmore they can make money second guessing you”.\nAnother told us: \n“I feel like I’ve got more of  an issue with this one because it’s just quite \ninaccurate. So it could be offensive [...] it’s unfair because it’s inaccurate. \nSo it’s just bad use of  data in this sense”.\nThe concern about how we are “judged” based on data about us was \nextended to the physical world, with one person saying, in relation to \ngetting a visa to move to the USA, that \n\nAbout Data About Us31 \n“They will check your political affiliation or your [online] posts. It’s just \nnuts to me”. \nAs to rights around data, one participant said: \n“I should have the right to decide whether my data is being taken and \nused”. Others outlined that the rights that exist already – such as opting \nout – don’t always seem to be working: “Not all cookies allow you to opt \nout of  it. Not all websites are clear about cookies”. \nFeelings of  resignation, that ‘this is just the way it is’, came up with \nsome participants. One said: \n“I actually think a lot of  things go on around us that we’re not even aware \nof”.\nAnother said: \n“I wouldn’t know what to do or what to change about my behaviour \nonline because there isn’t a control for the individual at the moment”. \nAnother said: \n“There’s no manual [for how to use the internet]. It’s literally trial and \nerror”. \nReaction to the types of data about us\nWhen we shared the four types data about us with participants, they all \nsaid the different categories (‘personal data’, ‘sensitive data’, ‘behavioural \ndata’, and ‘societal data’) were very useful in helping them understand \ntheir relationship with data, and place themselves within the three stories \nwe had talked about. One person said: \n“I’ve understood it more in that one slide than I’ve probably [ever] read \nabout [data] online, so that’s very good”. \nSeveral people picked up on the blurred lines between ‘sensitive data’ \nand ‘behavioural data’, and how one could be inferred from the other. \nOne person explained this by saying \n“They can make a guess on maybe different cultural events that you [go] \nto, or holidays that you [celebrate]. If  you were [to] Google Christmas \npresents, or Google something going on for Ramadan or something”.\nPeople said they wanted greater protections or a new social contract. \nOne said: \n“I think societal data is something separate, but the sort of  blurring \nbetween those different categories, particularly sensitive and behavioural \ndata, I mean, I get that one is protected under GDPR but like how... how \n\nAbout Data About Us32 \ndo you define and pull apart those things? Because you could infer some-\none’s political views from things they like on Facebook, and that is what \npolitical parties and lobbyists do”.\nAs a group, they agreed that societal data is useful and that they were \nhappy for data about them to be used if  it could be de-identified with a \nguarantee that there would be transparency of  how the data would be \nused, what the benefits were, and how it would be stored. One person \nsaid: \n“Yeah, they can know that for society and stuff, but not to just get more \nmoney out of  me and sell me stuff. I feel like society for transport and \nenergy, I don’t mind them knowing”.\nPresentations and feedback from the panel \nWe wanted to test what participants had learned during the course of  the \nday, whether the stories and the explanation of  the different types of  data \nabout us had had an impact, and what they considered important to share \nwith others. Therefore, in the last session of  the workshop, we brought in \na ‘panel’ of  four new members of  the public, to listen to the stories that \nthe other participants had developed in tables, and give their reactions.\nTable 1 \nTable 1 created a poster with presentation in relation to ‘Story 3’: infer-\nences based on data about us as consumers.\nThe group had categorised factors about the story into a ‘traffic light \nsystem’ from green to red. Green represented convenience, amber repre-\nsented data protection and red represented the tracking of  people.\nThe panel felt the presentation was informative and liked the balance \nprovided by the traffic light system, but felt it lacked emotion and context \nexplaining why the group had chosen the factors they had.\nWhen asked what feelings the story inspired and values it captured, \nthe panel said it made them realise how unclear it is how data is used and \nwhether it is done in a responsible way. One panellist said: \n“It seems that we don’t really have full control of  how our data is used. We \nare manipulated in a subtle manner by corporations”.\nA few panelists thought that some of  the examples in the story \nwere “random” and the approach didn’t tell a cohesive story. Instead \nthey wanted to see more detail about who is responsible for governing \ndata. One said in relation to whether it is companies or people who are \nresponsible: \n“Do we have responsibilities as well? Or maybe, because of  the flex-\nibility of  online services, we’ve just handed the responsibility to the \norganisations?”\nOur learning: the scenario of  inferences being made about us based on \ndata about us as consumers was found easily relatable by both the morn-\ning group and the afternoon panel. The scenario prompted discussion \n\nAbout Data About Us33 \nabout the level of  comfort and control people have about how data about \nus is used by companies. It also prompted conversation about what \nresponsibilities should be considered. The response of  the panel revealed \nthat, in terms of  capturing people’s imagination about how data about \nus is used, people wanted to see a story with characters and relationships \nrather than a presentation to help them engage more deeply\nTable 2 \nTable 2 created a poster and gave a group-presentation in relation to \n‘Story 1’: automated decision making in the workplace. \nThe group used a poster to outline a scenario where people were \nbeing monitored and automated decisions were being made in a fictional \ncompany. The poster and verbal presentation was designed to act as a \ncall-to-action to employees to stand together and challenge the company’s \napproach to the handling of  data about staff. \nThe panel felt that this story as the most emotive. They suggested it \nfelt as though the group were calling for a revolt. They liked the fact the \ngroup had used pictures and felt the poster approach engaging. Some of  \nthe feelings they said it raised were: \n“[It] makes me feel like information and data has been used in a deceitful \nway, and that there has been a betrayal of  trust”.\n“[It] reminds me that data capturing can be both positive and negative – if  \nnot used with the right context”.\nOne panelist said: \n“It made me feel that there is almost a data battle between modern \nmanagement and staff”.\nWhilst the story had been emotive, it was felt by the panel that the \ngroup hadn’t resolved the issues raised, and felt it was far from clear what \naction should be taken, or where the issue of  rights should fall. A couple \nof  people commented that they wanted to see a more collaborative ap-\nproach taken between employees and employers rather than the “us versus \nthem” framing.\nOur learning: all the panelists agreed the poster presented had been \nhelpful, and that it represented a clear sense of  the story. The emotion \nof  the story was referred to with panelists repeating the groups’ initial \nreactions of  anger, their sense of  deception and compromised personal \nagency, due to the ways that technology and automated decision-making \nwere used by the employer in the scenarios presented.\nTable 3\nTable 3 wrote a short play featuring characters represented by each \nmember of  the group in relation to ‘Story 2’: use of  data about us to \nhelp make societal decisions. The group incorporated the Types of  Data \nAbout Us graphic that we had previously shown to them, as part of  their \nperformance. \n\nAbout Data About Us34 \nThe group drew on elements from all three of  the stories that we had \npresented earlier. They modified the dialogue from ‘Story 2’ to incorpo-\nrate some of  their concerns about being tracked online, and somewhat \nhumorously also referred back to ‘Story 1’, with some actors wearing fake \nwearable wristbands they had made.\nThe panel: As a whole, the panel agreed this was the strongest, most \nengaging, informative, and structured story of  the three. The context and \ncharacters were felt to be engaging and they said that using Types of  Data \nAbout Us graphic helped give them a greater level of  understanding. On \nthe graphic, they said:\n“[It gave the] best clarity so far, [about] what can and cannot be used. This \nshould be more public”. \n“I think the presentation slide was excellent”.\nSimilar to the morning groups, panelists were supportive of  the idea \nthat data could be anonymised and used in ways to benefit society, al-\nthough some also questioned whether the real reasons for collecting data \nwould be beneficial to the public:\n“Companies are thinking about improving services, that’s the reason why \nthey seem to be collecting, but the concern is what else is the data used \nfor”.\nA few people highlighted a need for people to have a choice to opt out. \nAnother suggested we should have the right to delete all our activity at \nthe end of  an online interaction: “Every time you connect to it, it should \ncome up ‘do you want your data’, you know, ask the question. And then \nyou have the choice to say yes or no to it, and explain: ‘look, the data may \nbe used just for services and you may feel that’s safer, but be informative \nabout and let people know what you’re doing’”.\nOur learning: compared to the other two presentations, panelists felt \nthey had a deeper level of  understanding of  the types of  data about them. \nThey attributed this to the graphic the group had chosen to put in their \npresentation and the nuances of  presenting both positive and negative \nuses of  data.\n\nAbout Data About Us35 \nConclusion \nOur data lives are complicated. Never before has data played such an \nintegral and granular role in how we live.\nWe all take different approaches to our digital lives. Some people are \ncautious about how data about them is used. Others are willing to share \naccess to data about them with everyone and anyone. But, as we’ve found \nin the course of  our research, most people want to make a choice based \non how they feel at a moment in time, and be able to change their minds \nwhen they feel differently. \nThis nuance is often ignored or misinterpreted, particularly in quan-\ntitative research that is focused on people’s perceptions of  data and the \nvalue-exchanges around it. \nEveryone we spoke with in our focus groups and workshop made clear \nthat they had accepted the internet and their connected lives. Everyone \nsaid it gave them positive and beneficial experiences, be that more choice, \nbetter connectivity, ways of  keeping in touch, sharing, engaging, learning, \nand teaching. People liked to be able to shop online, and download music, \nfilms, and games. Some said how mobile technology helped their children \nnot to get lost; others said how it benefits their work and their hobbies. \nHowever, people also expressed discontent, worry, and feelings of  \nresignation. They worried about how much they understand, how well \nthey are educated in using connected technologies, how safe they are, and \ntheir lack of  control over how data about them is used. They also worried \nabout being subject to organisations – both private and public – making \ndecisions about them and decisions about society, which they fear might \nnot actually benefit them. \nIt was a commonly felt concern among the people we spoke with that \nwe are being misrepresented, misread, misinterpreted, and misunderstood \nbecause of  algorithms, automated decision making, and a simple lack of  \nhuman engagement.\nParticipants didn’t want to have assumptions made about them. They \nraised concern about the impact this can have on our mental health, our \nchildren’s mental health, and our general wellbeing. They said that they \ndidn’t want to be judged by how far they deviate from a statistic of  what \nis ‘normal’ in society, but be allowed to be the complex human, not a \nrobot. \nThe overall sense of  mistrust was strong and people expressed the \nfeeling that power is sitting in the wrong place, used for financial gain and \nnot sensitive to the impact it has on people. \nMost interesting of  all was the widespread engagement with data \nprotection. Everyone had heard of  the GDPR. Knowledge of  the detail \nwas varied, but it was legislation that people were familiar with and they \nspoke of  it with some confidence. \nWe posed general questions and presented basic stories to people to \n\nAbout Data About Us36 \ntest their feelings, knowledge, understanding, and thoughts. What we \nheard in return was a desire for clarity, knowledge, choice, and control. \nSuggestions from participants\nThe aim of  the work was not to present a series of  formal recommenda-\ntions. We wanted the people to speak for themselves and to make their \nsuggestions. \nSome of  the suggestions they made are things that already exist but \nclearly require greater signposting, strengthening, and improvement. \nSome are impractical but deserve consideration in terms of  how the data \nworld is seen by people and how government and business can approach \ntransparency and engagement in the future. Some are simply demonstra-\ntions of  how people want to be treated in their connected lives. \nThe following suggestions, which we have set out in bullet points under \nfive categories for ease, are a selection of  the collated suggestions made by \nthe people we spoke to in our focus groups and workshop. \nHonesty and transparency \nWe were told that: \n •\nPeople want to have continued and improved transparency and \ninformation about what data about them is being used, how, \nwhen, and for what purpose by all organisations both public and \nprivate. \n •\nPeople want honesty about how long data about them is being \nkept for and what it is being used for. In the focus groups we held \nit was felt by participants that if  a company couldn’t tell them \nhow data was being used then it was up to no good. \n •\nPeople want to be clearly told when tracking of  them is taking \nplace. Both on- and offline and to be asked to opt in rather than \nopt out. \nAgency and control\nWe were told that: \n •\nPeople want to see a more unified approach to how cookie \nconsents are displayed and handled – they don’t what to go hunt-\ning around a website and want better, clearer, understandable \ncommunication and explanation about what it is they are having \nto decide. \n •\nPeople want to see wider use of  opt in rather than opt out. \nOpting in was seen to give more agency over making decisions \nabout the use of  data about us.\n •\nPeople want to be asked clearly for their permission to share data \nfor societal purposes – not for it to be automatically assumed, \nand have it explained why data about them is needed.\n •\nSome people want the opportunity to choose which adverts they \n\nAbout Data About Us37 \nwant to see, and from which shops and companies, as opposed \nto being served adverts based on assumption or inference from \nbehavioural data. \n •\nParents want to have the opportunity to restrict and control \nwhat adverts are shown to their children.\n •\nSome people want to be able to choose which services are \npersonalised to them – rather than have organisations decide \nthat for them based on their behaviour. \n •\nPeople want to have continued access to education and more \nchoice of  services that strengthen privacy and security – exam-\nples given were encryption, virtual private networks (VPNs), and \nsearch engines that don’t track people.\n •\nSome people wanted to see more friction in place online. They \nwant to be asked “do you want to share?”, “do you want to \nupload this photo?”, “do you want to tag this person?”, “do you \nwant to delete your data now?” because it would enable a pause \nfor thought, and help them decide what they want. We were told \nthat even if  this was irritating it might help people engage with \nwhat was happening. \n •\nPeople want to have clearer signposting, such as a pop-up alert, \nwith clear instructions how to delete cookies at the end of  a \nsession on a device.\nRights and responsibility\nWe were told that: \n •\nSome people feel that individuals need to take responsibility \nfor communicating to others what they are comfortable with \nregarding posting of  photos, comments, and other content \nabout them on the internet. Others felt that there is also a place \nto ask before uploading or sharing data about others. \n •\nPeople want to see companies take greater responsibility in their \nrole of  communicating what is happening with data about us \nonline. This includes being clearer about how data about us is \nshared, sold, stored, and used to make decisions about us. This \nwill enable us to control how we are tracked and how data about \nus is used to make decisions about us. \n •\nPeople want to see government regulate companies to do things \nproperly, some want this regulation to be “light touch”, some \nwant regulation to be a level playing field where all companies \nhave to conform.\n •\nPeople want independent oversight: such as commissioners, \nombudsman or independent bodies to oversee the enforcement \nof  many legislative or regulatory moves made to improve or \nprotect how data about them is to be used.\n •\nPeople want government draft legislation with people who un-\nderstand data and the implications of  it rather than politicians.\n •\nPeople don’t want regulation or legislation to be influenced by \nfinancial motive. \n\nAbout Data About Us38 \nContext and fairness \nWe were told that: \n •\nPeople want to see legislation and governance that would prevent\nprejudices and bias from being replicated and scaled through\nbiased datasets.\n •\nSome people felt strongly that news should be general and not\npersonalised to them based on inference based on data about\nthem – personalisation of  news was considered unacceptable.\n •\nPeople want to have clearer signposting and meaningful ways to\nprevent automated decisions from being made about them.\n •\nPeople want the right to stop automated inferences from being\nmade about them.\n •\nPeople want an end to the idea that the value-exchange between\ndata about them for deals, convenience, or nuanced recom-\nmendations is the preferred approach. People were clear that it’s\nnot a one rule fits all and that decisions are made about comfort\nlevels of  access to data about them in that moment. A decision\non one day may not be the same decision the following day.\n •\nPeople want to see stronger and clearer reference to how data\nabout them will be anonymised and how they won’t be easily\nlinked/tracked/reidentified.\n •\nPeople want to see companies who use data about them to\ncontribute back into society by paying their taxes.\nCompliance and enforceability \nWe were told that: \n •\nPeople want companies to clearly tell them why an advert\nis being served to them, without the need to read lengthy\nexplanations.\n •\nPeople want to be given clear explanations of  why data about\nthem is being used, what the exact benefit is, and who has\ndetermined the benefit.\n •\nPeople want to have improved communication from a company\nthat they have been fully unsubscribed from a mailing list –\npeople raised concern that they still received rogue emails or\nhave to undertake lengthy unsubscribe processes.\n •\nPeople want to have improved communication that data has\nbeen deleted.\n •\nPeople want an end to data about them being sold to\nthird-parties.\nWhat next? \nSociety is still finding its feet in this relatively new data-driven and con-\nnected world. \nGovernments and businesses make decisions as to how the world \nshould look all the time. Sometimes these decisions provide immediate \nbenefit and little risk, sometimes however they are based on the principle \n\nAbout Data About Us39 \nof  ‘move fast and break things’. Both approaches have their benefits, but \nas we become more familiar with the role data plays in every facet of  our \nlives, both online and offline, moving fast and breaking things is becoming \nfar less tolerated legally and societally. How people respond, what they \nwill tolerate, and what they will simply oppose, are likely to become more \nnuanced as time goes on. \nIt has been a privilege for us to sit down and speak to people in the UK \nabout their feelings about their data lives, and the impact that the use of  \ndata about them have on them individually, as part of  a larger group and \nfor society as a whole.\nWhat we were told was eye-opening. It made clear that people had \nmuch more awareness and understanding than ‘the UK public’ had been \ngiven credit for, largely by politicians and press. It also showed us that \npeople were keen to express their wants, needs, and ideas for how data \nabout them should be protected. \nPeople are not naive or ignorant about data. We all understand – to a \ngreater or lesser degree – its impact, role, and importance. Give people \nthe chance to talk and they will engage in ways that will bring meaningful \ninsight into the development of  future rights, responsibilities, regulations, \npolicies, and products.\nThis body of  work should be the start of  a wider conversation between \npeople, governments, and businesses, as well as the more commonly held \nconversations between NGOs, interest groups, and think-tanks.\nWe’d like to hear what you think about data about us, about the rights \nwe have, about the responsibilities we should have, that governments \nshould have, and that businesses should have to strengthen our data \nrights.\nTweet your views using #WeAreNotRobots.\n\nAbout Data About Us40 \n\nAbout Data About Us41 \nThe RSA (Royal Society for the encouragement of Arts, \nManufactures and Commerce) believes in a world where \neveryone is able to participate in creating a better future. \nThrough our ideas, research and a 30,000 strong Fellowship we \nare a global community of proactive problem solvers, sharing \npowerful ideas, carrying out cutting-edge research and building \nnetworks and opportunities for people to collaborate, influence \nand demonstrate practical solutions to realise change.\nThe RSA has been at the forefront of social change for over \n260 years. Today our work focuses on supporting innovation \nin three major areas; creative learning and development, \npublic services and communities and economy, enterprise and \nmanufacturing.\nCentral to the RSA’s current work are the concepts of \n‘convening’ and ‘change making’. The RSA has also developed a \ndistinctive approach to change: ‘Think like a system, act like an \nentrepreneur’ which now runs through our projects.\nThe RSA: uniting people and ideas to resolve the challenges of \nour time.\n8 John Adam Street \nLondon WC2N 6EZ \n+44 (0) 20 7930 5115\nRegistered as a charity \nin England and Wales \nno. 212424 \nCopyright © RSA 2019\nwww.thersa.org\nISBN 978-1-911532-36-1","version":"1.10.100"}