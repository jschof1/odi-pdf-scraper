{"numpages":16,"numrender":16,"info":{"PDFFormatVersion":"1.4","IsAcroFormPresent":false,"IsXFAPresent":false,"Title":"#OPEN RDP8 Anonymisation and open data: An introduction to managing the risk of re-identification","Producer":"Skia/PDF m104 Google Docs Renderer"},"metadata":null,"text":"\n\n\n\nTable of contents\nAnonymisation and open data3\nAbout3\nOpen data is (sometimes) made of people4\nPersonal, sensitive and private data5\nHow does open data relate to this diagram?6\nRedaction and anonymisation6\nHow common is anonymised/personal open data?7\nAnonymisation, utility and risk7\nNot just a technical tool8\nThere is more than one way to do it8\nNot just how, but how much9\nExample: civil servants’ crumpet-eating habits9\nThe utility-risk trade-off11\nOpen data and re-identification risk11\nLies, damned lies, and statistics12\nWhat the future holds14\nDifferential privacy14\nGenerating synthetic data with deep-learning algorithms14\nAppendix 1: Going Further16\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification2\n\nAnonymisation and open data\nAn introduction to managing the risk of re-identification\nAbout\nThis report has been researched and produced by the Open Data Institute (ODI), and\npublished in April 2019. Its lead authors are Olivier Thereaux and Fionntán O’Donnell\nwith contributions from Sonia Duarte, Becky Ghani, Jared Robert Keller, Jeni Tennison\nand Peter Wells.\nThe report follows the recent implementation of the EU’s General Data Protection\nRegulation (GDPR) in May 2018. It is one of the most comprehensive pieces of\nregulation yet to deal with personal data, and is changing the way organisations collect\nand use data.\nGDPR is just one part of a growing trend towards people exercising more control over\ndata about them, and protecting their privacy in a data-hungry world.\nWe hope that this report will provide organisations with a better understanding of data\nanonymisation as a framework for risk management, and how responsible data\nmanagement can increase access to data while retaining trust.\nWe would like to thank Mark Elliott and Elaine Mackey from The University of\nManchester, Gemma Galdon Clavell and Carmela Troncoso from Eticas Research and\nConsulting, and Yves-Alexandre de Montjoye from Imperial College for their expertise,\ntime and advice to our team throughout our research. We would also like to thank\nJonathan Pearson and Forrest Frankovitch at NHS England, as well as the team at ODI\nLeeds, for providing us with a practical case to test our tools and understanding.\nApril 2019\nIf you would like to send us feedback, please get in touch\nby email atRandD@theodi.org.\nThis report is published under the Creative Commons Attribution-ShareAlike 4.0\nInternational licence. See:https://creativecommons.org/licenses/by-sa/4.0.\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification3\n\nOpen data is (sometimes) made of people\nOpen data initiatives have historically been motivated by increasing transparency about\ngovernments and support for economic growth. Typicalopen data describes things\n1\nlike bus stop locations, the weather forecast, or local authority payments. Open data,\nfrom a distance, is about things and organisations, not people.\nDEFINITION\nOpen datais data that is available for anyone toaccess, use, and share.\nThe ODI’s Data Spectrumillustrates how data accessand governance sits on a\n2\nspectrum of, from closed to shared to open. The current version of the spectrum\nincludes personal data at the closed and shared end of the spectrum.\nThis may lead to a perceived dichotomy: that personal data can never be open data,\nand vice versa – that open data never includes personally identifiable information.\nSometimes this perception extends to thinking that data derived from personal data –\nie anonymised, pseudonymised or synthesised data – cannot be open data.\nThis perception was apparent in the results of some user research we conducted in late\n2018. The interviewees had a very distinct mentalmodel: on the one hand, personal\n3\ndata which is meant to be kept closed and protected; on the other, open data, which\nalmost never includes personal data.\nThe reality, however, is more subtle than this perception suggests.\nFor example, the expenses and allowances for elected officials in the UK, such as\ncouncil membersor Members of Parliament (MPs),are open data.\n45\nThere are also many examples of open datasets for licensed practitioners, eg the\nGeneral Medical Council’s list of registered medical practitioners. Which professions\n6\nare included varies from country to country, from legislation to legislation. For example,\nas part of theLoi pour une république numérique(digital republic law),France has\n7\nrecently enshrined into lawwhich personal data oughtto be published as open data,\n8\nfor the public good.\n8\nEtalab (2018),Le décret fixant les catégories dedonnées diffusables et réutilisables sans anonymisation est\nparu,\nhttps://www.etalab.gouv.fr/le-decret-fixant-les-categories-de-donnees-diffusables-et-reutilisables-sans-anonymis\nation-est-paru\n7\nLegifrance.gouv.fr (2016), ‘Loi pour une république numérique’,\nhttps://www.legifrance.gouv.fr/affichLoiPubliee.do?idDocument=JORFDOLE000031589829&type=general&l\negislature=14\n6\nGeneral Medical Council (2019), ‘List of registered medical practitioners’,\nhttps://webcache.gmc-uk.org/gmclrmp_enu/start.swe\n5\nIndependent Parliamentary Standards Authority,\nhttps://www.theipsa.org.uk\n4\nLincolnshire County Council (2017)Members’ allowances,\nhttps://data.gov.uk/dataset/3b17b920-642a-4189-b1d9-18e420be9ef4/members-allowances\n3\nOpen Data Institute (2019)How do organisations perceivethe risks of re-identification?,\nhttps://theodi.org/article/how-do-organisations-perceive-the-risks-of-re-identification/\n2\nOpen Data Institute (2016)The Data Spectrum,\nhttps://theodi.org/about-the-odi/the-data-spectrum/\n1\nThe Guardian (2013),Obama to Berners-Lee, Snow toDomesday: a history of open data,\nhttps://www.theguardian.com/news/datablog/2013/oct/25/barack-obama-tim-berners-lee-open-data\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification4\n\nIn both cases – public office transparency and legal requirements – the open data\nincludes the names of officials or professionals, but also potentially reveals information\nabout their life or their habits. This is deemed an acceptable encroachment of their\nright to privacy, because that right is balanced against other rights, such as the right of\nthe public to hold those in public office to account, or to guard against fraud or\n9\nunlicensed practice.\nPersonal, sensitive and private data\nThere may be value in opening data about people, but this must be balanced with the\nrisk of harm.\nIn some instances, the case to make data public and open overrides concerns because\nthere are competing rights between the people who are potentially impacted by its\navailability and use. Sometimes societies decide that to protect people from harm we\nneed to either risk, or even cause, harm to some people. At the simplest level this is\nwhy nations have the power to punish people or put them in prison. Different societies\nmake their own decisions about which crimes receive which punishment. For example,\nthe USA publishes its sex offender registryas opendata with the public interest goals\n10\nof reducing instances of sexual violence and re-offending.\n11\nIn many other cases, open access to personal data could bring benefits, for example\nfor researchers or innovators, but the potential harm to people outweigh those benefits.\nBut is there a way to protect privacy while still realising the value associated with open\ndata?\nThe notions of personal and sensitive data are not just intuitive: they are reflected in\nofficial definitions used in laws and regulations.\nDEFINITIONS\nThe EU General Data Protection Regulation(GDPR)definespersonal dataas:\n12\nany information relating to an identified or identifiable natural person\n(‘data subject’); an identifiable natural person is one who can be\nidentified, directly or indirectly, in particular by reference to an identifier\nsuch as a name, an identification number, location data, an online\nidentifier or to one or more factors specific to the physical, physiological,\ngenetic, mental, economic, cultural or social identity of that natural\nperson\nThe UK’s Office for National Statistics definesprivateinformationas:\n13\ninformation that\n●\nrelates to an identifiable legal or natural person, and\n●\nis not in the public domain or common knowledge, and\n●\nif disclosed would cause them damage, harm or distress\n13\nOffice for National Statistics (2009),National Statistician’sGuidance on Confidentiality of Official Statistics,\nhttps://gss.civilservice.gov.uk/wp-content/uploads/2012/12/Confidentiality-of-Official-Statistics-National-St\natisticians-Guidance.pdf\n12\nEuropean Union (2016), Regulation (EU) 2016/679 (General Data Protection Regulation) Art 4. Definitions,\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32016R0679\n11\nNational Criminal Justice Reference Service (2011), ‘Evaluating the effectiveness of sex offender registration’,\nhttps://www.ncjrs.gov/pdffiles1/nij/grants/234598.pdf\n10\nDC Court Services and Offender Supervision Agency (2015) Sex Offender Registry,\nhttp://opendata.dc.gov/datasets/10e58174831e49a2aebaa129cc1c3bd5_20\n9\nCommittee on Standards in Public Life (2016), ‘Upholding the Seven Principles of Public Life in Regulation\nReport’\nhttps://www.gov.uk/government/publications/striking-the-balance-upholding-the-7-principles-in-regulation\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification5\n\nThis definition is very similar to what GDPR callsSpecial category data\n14\nBlack's Law Dictionary definessensitive informationas:\n15\nA sensitive asset that if compromised can cause serious harm to an\norganisation\nPersonal data can be sensitive data – in which case, in line with the definition above,\nwe call it private data. Or, in the case of a list of elected officials, personal data can be\ndeemed non-sensitive. Conversely, sensitive data can be personal data, but it does not\nhave to be: corporate confidential informationorenvironmental protection data (like\n16\nthe locations of endangered species, or animals protected from poaching) are all\nexamples of sensitive, but not necessarily personal, data.\nWe can visualise this overlap between sensitive, personal and private data as:\nHow does open data relate to this diagram?\nWhether data is personal is inherent to the data itself. Whether it is sensitive is\ndependent on both the content of the data and the wider information environment at a\npoint in time. Whether data is made open data is a choice. The examples above show\nthat personal data can often be made open – when in the public interest.\nWhen data is made open, by definition it is no longer sensitive. The definition of\nsensitive data is that it is ‘not already public’; open data is public.\n16\nOpen Contracting Partnership (2019). Mythbusting: confidentiality in public contracting\nhttp://mythbusting.open-contracting.org\n15\nBlack's Law Dictionary Free 2nd Ed, ‘What is sensitiveinformation?’,\nhttps://thelawdictionary.org/sensitive-information/\n14\nInformation Commissioner’s Office (2018), ‘Guide to Data Protection’\nhttps://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regul\nation-gdpr/lawful-basis-for-processing/special-category-data/\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification6\n\nRedaction and anonymisation\nIt is however possible to process data into a modified form that can be shared or made\nopen while significantly reducing (or eliminating) the possibility of anyone recovering\nsensitive or personal information from it. For sensitive data in general, this process is\ncalled redaction; for personal data it is called anonymisation.\nFor example, we can visualise how anonymisation of medical records would create\nmodified forms of data that could sit elsewhere in the Data Spectrum – shared/open\nrather than closed. This would allow research data to be shared with academics\nthrough data-sharing agreements, and health statistics to be published openly:\nHow common is anonymised/personal open data?\nSo far we have only partly challenged the perception that personal data and open data\nare mutually exclusive, by giving examples of open personal data and introducing the\nconcept of anonymised open data. Are those, however, anecdotal occurrences or more\ncommonplace?\nIn February 2019, our team conducted a small-scale audit of open data portals, taking\n17\na random sample of 114 datasets published to the Frenchand UK government open\n18\ndata portals, and assessing whether each of the selecteddatasets contained\n19\npersonal information, had been anonymised, or both.\nWe found that one in five datasets sampled on the French portal, and one in ten on the\nUK portal, included some personal data – often names of people, whether because\nthey were elected officials, registered professionals, or, in the case of the French portal,\nbecause the dataset listed sole traders or companies named after their owner, thus\nincluding personally identifiable information.\nThe proportion of datasets including anonymised data was around one in five in both\ncases, with one caveat: qualifying a dataset as ‘anonymised’ can be approximative and\nuncertain without information about how it was processed. We would need a full\n19\ndata.gov.uk,https://data.gov.uk/\n18\ndata.gouv.fr,https://www.data.gouv.fr/\n17\nOpen Data Institute (2019), ‘Auditing data portalsfor personal or anonymised data’,\nhttps://docs.google.com/spreadsheets/d/1fkmV5k8FANXXkLclu7pgZQQrxOgWeCsazXafr_EMUYE/edi\nt#gid=0\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification7\n\ndescription of the data processing that led to the open form of the data to fully verify its\nstatus as anonymised.\nSome cases were relatively easy to gauge, such as when a dataset included a ‘name’\ncolumn with values removed, or when the description of the data noted how it had\nbeen anonymised. In other cases, we considered personal data to be anonymised\nwhen aggregated or otherwise obfuscated so that no personally identifiable information\nremained.\nAnonymisation, utility and risk\nThere are several, sometimes contradictory, definitions of anonymisation and related\nterms such as de-identification, and how they relate to one another.\nDEFINITIONS\nFor the rest of this document, we will refer to the definitions used in the literature\nreview commissioned by the ODI and created by Eticas, a research and\nconsultancy company with expertise in security and responsible technology.\nAnonymisation:\nA process that alters a dataset to reduce the risk of re-identification as\nmuch as possible.\nAdversary:\nAn entity with access to an anonymised dataset that seeks\nre-identification of an individual or to learn more information about them\nNote that in this definition, the adversary does not necessarily intend to cause\nharm. Therefore the anonymisation processes described also cover the\nprevention of accidental disclosure or discovery of private information.\nNot just a technical tool\nIt is important to note that here, anonymisation is defined as a process – a series of\nactions. Not just a technique, not just a tool, and not just a set of mathematical\nequations.\nData practitioners may be tempted to approach anonymisation as a purely or\npredominantly technical activity. Once a team has chosen anonymisation as the best\nway to safely gather, process or share data, choosing an appropriate statistical\ndisclosure method may appear to be the logical next step. However, anonymisation is a\nmore in-depth process than this, involving research, legal and ethical considerations,\nrisk analysis and testing.\nIn a guide first published in 2016 (and to be updated in 2019), the UK Anonymisation\nNetwork (UKAN ) offered a practical and integrated approach: theAnonymisation\nDecision-Making Framework(ADF).\n20\nOne of the main messages of the ADF is: anonymisation is not done in a vacuum. To\nanonymise effectively, the process should include serious thinking about what happens\nto the data after anonymisation; with whom will it be shared; what could go wrong; and\nwhat mitigations can be put in place.\nThe right tools and techniques can only be employed effectively by going through this\nprocess of understanding the data ecosystem and data flow; engaging with internal\n20\nUKAN (2019),The anonymisation decision-making framework,\nhttps://ukanon.net/ukan-resources/ukan-decision-making-framework/\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification8\n\nand external stakeholders; understanding the legal and ethical context; and thoroughly\nconsidering the risks and their mitigations.\nThere is more than one way to do it\nAnonymisation is sometimes misunderstood as the act of simply removing personal\nidentifiers (mostly, people’s names) from data sets. This technique, often known as\n‘de-identification’ may be appropriate in some cases, but it is a very limited technique:\nremoving only explicit identifiers often leaves information within the dataset that can\nstill be used to identify an individual.\nThe field of anonymisation has however greatly evolved in the past three or four\ndecades. Contemporary anonymisation techniques are now many and varied, generally\nfalling into three categories:\n1.Suppression:removing identifiers or pieces of informationthat may lead to\nre-identification\n2.Generalisation:aggregating data points into a coarsergranularity, or\notherwise removing details to obfuscate data about people on an individual\nbasis\n3.Disruption:adding noise and changing values to theextent that it is\nincreasingly difficult to know how, or whether, information about specific\nindividuals can be recovered or inferred.\nNot just how, but how much\nThis document will not attempt to give a detailed explanation of the variety of the\ntechniques – but we would recommend the documents referred to throughout this\nreport and listed inAppendix 1.\nWe will, however, consider a simple case of anonymisation to illustrate how the\ntechniques can be applied.\nExample: civil servants’ crumpet-eating habits\nIn this example, an organisation ran a survey of all civil servants in the UK, collecting\ndata on how many crumpets the civil servants typically eat in one sitting. The\norganisation was commissioned to help the civil service optimise its crumpet purchase,\nbut is now considering whether to release an anonymised version of the survey results\nfor the benefit of the crumpets-and-other-foodstuffs-industry.\nThe raw data could look a little bit like this:\nFull nameDate of birthTypical number of crumpets eaten\nin one sitting\nPeter Watson1969-03-165\nLydia Okwanga1985-12-021\n.........\nAli Massa2000-05-262\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification9\n\nThe data steward (who collects, maintains and shares data)in charge of releasing an\n21\nanonymised version of this data may decide that the first technical step is to remove all\nnames (suppression), replacing them with pseudo-identifiers(pseudonymisation– a\ntype ofsuppression):\nIDDate of birthTypical number of crumpets\neaten in one sitting\n11969-03-165\n21985-12-021\n.........\n357692000-05-262\nWhile removing names is an obvious step, if you know a colleague’s birthday you could\neasily identify them in the data, and infer how many crumpets they ate. The data\nsteward may therefore decide to also replace the very precise dates of birth with age\nranges (generalisation):\nIDAge range, as of 2019-02Typical number of\ncrumpets eaten in one\nsitting\n140–505\n230–401\n.........\n3576910–202\nCrumpet-eating habits are quite sensitive, so they may also want to add some “noise”\nto the data, for example by swapping values for a certain proportion of the rows\n(disruption):\nIDAge range, as of 2019-02Typical number of\ncrumpets eaten in one\nsitting (up to 10% of the\nvalues have been\nswapped between rows)\n140–501\n230–405\n.........\n3576910–202\n21\nNote on language: In this document we use the term‘data steward’ to describe a person\nor organisation who collects, maintains and shares data. We may review the choice of\nwording in future iterations. Under GDPR, this role is defined as a ‘data controller’ or ‘data\nprocessor’. For GDPR definitions, see\nhttps://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-prote\nction-regulation-gdpr/key-definitions/controllers-and-processors/\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification10\n\nEach of those iterations results in a form of ‘anonymised data’ – as would an otherwise\ninfinite combination of anonymisation techniques.\nWith each of those steps, the data steward removed a certain amount of detail,\ngranularity and precision from the data. This means the data is less specific and less\naccurate. Anonymisation reduces its ‘utility’ – a measurement of the usefulness of the\ndata.\nUtilityis a relative concept: after the first two anonymisation steps above, the dataset\nwill be less useful to someone needing to answer the question: “how many crumpets\ndoes Peter typically eat?”, but utility would still be high for someone trying to answer a\nquestion about whether, in this dataset, people in their 20s eat more or fewer crumpets\nthan people in their 50s.\nUtility is typically at its maximum in the raw data because that data supports the widest\npossible range of uses, although there too it is relative: the full, non-anonymised raw\ndata will for example be more useful to someone with access to data about where\nPeter, Lydia and Ali live. At the other end of the spectrum, utility will generally be\nconsidered to reach a minimum if anonymisation is so heavy that it renders the data\nuseless, as in this (extreme) example:\nTypical number of crumpets eaten in one sitting\n(anonymised)\nSome people eat crumpets some of the time.\nThe utility-risk trade-off\nData stewards, whether they hold data about crumpet consumption or other topics,\nhave at their disposal an almost infinite combination of anonymisation techniques and\nparameters. They could apply different forms of anonymisation to the same raw data to\nproduce several different anonymised datasets, and release or share all of them. That's\na large amount of what census-based statistics do: they are different aggregations,\nfocusing on different features, a bit like showing only parts of the elephant in the\nparable of the six blind men.\nToo much choice can however be paralysing. Assuming they want to release a single\nanonymised dataset about crumpet consumption, how should our data steward decide\nhow much to anonymise this data, and how much utility should remain?\nTo answer this question, we need to get back to why they would go through a process\nof anonymisation in the first place: to address the risk of accidental disclosure of\nsensitive information – or of re-identification by an adversary, both of which could\ncause harm to the data subjects.\nRisk management is a whole discipline dealing with the assessment of the probability\nand impact of risk, and its mitigation. This document does not describe the discipline\nat length, but resources listed inAppendix 1addresshow to model threats, how to\ngauge the likelihood and impact of those threats, and how to manage associated risk.\nThe ADF, in particular, offers guidance on ‘building disclosure scenarios’.\nWhat is important to understand, however, is the correlation between utility and risk in\nthe anonymisation process. InBroken Promises of Privacy,professor Paul Ohm writes\n22\nthat: ‘Data can be either useful or perfectly anonymousbut never both’.\n22\nPaul Ohm (2010), ‘Broken promises of privacy: Responding to the surprising failure of\nanonymization’.\nhttps://paulohm.com/classes/infopriv13/files/week8/ExcerptOhmBrokenPromises.pdf\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification11\n\nIt would be tempting, but a mistake, to extrapolate from this statement that, since\nperfect anonymisation is impossible, anonymisation is pointless. Ohm explains further:\n‘No useful database can ever be perfectly anonymous,and as the utility of data\nincreases, the privacy decreases’. In other words,every decision in anonymisation is a\ntrade-off between risk and utility.\nOpen data and re-identification risk\nThis is where we finally answer the question posed earlier: why isn’t anonymised open\ndata more common?\nWe now understand that decisions about anonymisation of data are decisions about\nrisk and utility – alongside compliance with data protection laws – based on an\nassessment of threats in a specific context, such as who will the data be shared with,\nwhen, and how.\nOpen data is one such context, and it is one of the most complex:\n1.With open data, most of the factors used in assessing, planning, conducting\nand monitoring the anonymisation process are unknown by design. Open data\nis available for anyone to access, use and share, sometimes in unanticipated\nways. The adversary that needs to be considered may therefore be anybody, at\nany point in the future. They may also have accessto any data and any\n23\ncurrent or future technologies.\n2.Risk is therefore hard to assess. The process of risk assessment in\nanonymisation presumes that – through analysis and imagination – the data\nsteward is able to make assumptions about who the adversaries may be, what\nother data might be at their disposal, when and for how long the adversary\nmay attempt de-anonymisation, etc. Those assumptions cannot apply in the\ncase of open data.\nUtility is also harder to assess. Guides like the ADF involve a consideration of how the\ndata will be shared and used: it helps data stewards understand how ‘useful’ the data\nwill be post processing. But without full knowledge of possible uses – common in the\nopen data situation where serendipitous reuse is a feature – the understanding of utility\nis imperfect, and therefore anonymisation decisions are less certain.\nAnother perspective is to look at the Data Spectrum as a spectrum of governance and\ncertainty.\n-The closed end of the spectrum has the most certainty about who may access\nand use the data, and the strictest governance in terms of access, rights, etc.\n-In the shared space there are still rules and governance – regardless of which\nsharing model is chosen – and some clarity over who may access and use the\ndata and when, but there is some uncertainty, and therefore some risk, which\ncan then be assessed and mitigated through anonymisation.\n-The open end of the Data Spectrum creates the most potential for access, use\nand processing, but the more permissive the governance, the more uncertainty\nit creates.\nLies, damned lies, and statistics\nThe risk of re-identification is real. In a set of case studiescommissioned by the ODI,\n24\nEticas described a number of high-profile cases where anonymisation, albeit well\nthought through, resulted in re-identification or disclosure.\n24\nEticas Consulting (2019), ‘Responsible Open Data’\nhttps://www.eticasconsulting.com/responsibleopendata\n23\nThe life span of anonymised data is typically estimated at 100 years - after which most if not all\nof the individual data subjects are no longer likely to be alive, therefore mooting the risk of\nre-identifying a living person.\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification12\n\nIt is worth noting that many cases of re-identification or ‘breaking’ of anonymisation are\nnot necessarily a clever mathematical anonymisation technique being defeated by a\ncleverer mathematical de-anonymisation technique, but simply a result of inadequate\nor clumsy anonymisation.\nThis excerpt from a high-profile 2018 court caseprovides a rather comical example of\n25\nbad anonymisation (in this case suppression through pseudonymisation), literally\npointing to the single and very publicly available external data point required for\nre-identification, namely:who was president of theUnited States in January 2017?”\nThere is always a probability that an adversary with strong motivation and high skills\nand resources will be successful in attacking anonymised data. That said, poorly\nanonymised data – due to lack of skills or scrutiny – is where a significant amount of\nrisk lies.\nIn our exploration of open data portals, we found at least one example of a dataset\nwhich had gone through a process of anonymisation, but included people’s names in a\n‘notes’ field which had not been removed.\nConversely, while anonymised data remains a minority of the data released under open\nlicence, and most anonymised data is typically shared under much stricter governance,\nthere are plenty of examples of data being anonymised and published openly, while\nretaining significant utility.\nThe release of statistics is a major use case for ‘anonymised open data done right’.\nStatisticians routinely release open data, with risk reduced to a point that few would\nchallenge their release for fear of privacy harm. Based on our conversations with\nmembers of the UK’sGovernment Statistical ServiceOpen Data sub-group, including\n26\nmembers of the Data, Statistics and Digital Identity Division in the Scottish\nGovernment, this is mainly because of three things:\n1.First, statistics professionals have a deep understanding ofstatistical\ndisclosurecontrols: methods for ensuring aggregatestatistics do not\ninadvertently reveal information about individuals.\n2.Second, statistics bodies are keenly aware of the balance between privacy and\nother rights and outcomes. In its Guiding Principles for Data Linkage, the\n27\nScottish government clearly aims for the balance between risk and utility we\n27\nScottish Government (2012), ‘Joined-up data for better decisions: Guiding Principles for Data\nLinkage’,\nhttps://www.gov.scot/publications/joined-up-data-better-decisions-guiding-principles-data-linkag\ne/\n26\nGovernment Statistical Service (2018), Open Data,\nhttps://gss.civilservice.gov.uk/guidances/open-data/\n25\nUnited States District Court, southern district of New York (2018), ‘Sentencing Memorandum,\nUNITED STATES OF AMERICA v MICHAEL COHEN’\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification13\n\nmentioned earlier:\n“The law does not give absolute value to privacy andtherefore a balance is\nneeded between respect for privacy, through the proportionate mitigation of\nrisk, and the potential benefits to all through the use of data for statistical and\nresearch purposes.”\nIt also needs to balance other considerations, such as the cost and complexity\nof the process: “It is not in the public interest to undertake unnecessary\nanonymisation work if the risk is low, as it increases public spend with very\nlittle or no benefit.”\n3.Finally, one key reason why statistical bodies can routinely publish anonymised\nopen data is that they are dealing with such volumes of data that even with a\nbig drop in utility, what they release through aggregation methods is still\nextremely useful.\nThis way of thinking is not limited to statistics from public bodies: in a very similar\nsetting, corporations publish open dataabout theiroperations for transparency\n28\npurposes, but aggregated at such a level that any sensitive information is not visible.\nWhat the future holds\nWhile traditional anonymisation techniques have been around for decades, the age of\nlarge-scale data processing and machine learning has brought two important\ninnovations to the domain.\nDiffer  ential privacy\nOver the last 15 years, differential privacy has gone from theoretical academic research\nto being deployed by software multinationals in smartphones and browsers to protect\nthe privacy of hundreds of millions of users.\nDifferential privacy is a property of data systems that allows collection of aggregated\nstatistics about a dataset but obfuscates individual records. When queried, a small\namount of noise is added to the data such that if any one record were removed, the\nquery result would stay the same. This means those using the data can never be\nentirely certain about any single person’s data.\nThe first large commercial use of differential privacy was in 2014, with Google trialing it\nin the world’s most popular web browser, Chrome, to collect statistics on malware. In\n29\n2016 Apple added differential privacy to itsiOS 10operating system to collect\ninformation on keystroke usage and browser performance. Uber has also published a\n30\ntool it uses to study patterns of car rides without accessing passenger’s location\ninformation.\n31\nWhile obviously powerful, differential privacy should not be seen as the answer to all\ndata anonymisation problems. It works best in situations where one organisation has\n31\nUber/Github (2016), ‘Dataflow analysis & differential privacy for\nSQL queries’sql-differential-privacytoolkit onGithub.\nhttps://github.com/uber/sql-differential-privacy\n30\nApple (2017), ‘Differential Privacy’,\nhttps://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf\n29\nGoogle (2014), ‘Learning statistics with privacy, aided by the flip\nof a coin’,\nhttps://security.googleblog.com/2014/10/learning-statistics-with-pri\nvacy-aided.html\n28\nSyngenta (2018), ‘Open data agriculture’\nhttps://www.syngenta.com/who-we-are/our-stories/open-data-agriculture\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification14\n\nsole access to a large dataset collected from lots of people. It requires those who use it\nto predict the number of queries that will be made on a dataset, to know how much\nnoise to add. Also, it is mainly focused on simple data queries, where the answer is a\nbinary choice or a single number. For more complex data, differential privacy could add\ntoo much noise and render the results useless.\n32\nGenerating synthetic data with deep-learning algorithms\nSynthetic data is data that is created by an automated process such that it holds\nsimilar statistical patterns as an original dataset. Each individual record in the synthetic\ndataset may have no relation to reality but when viewed in aggregate the dataset is still\nuseful for certain analyses and for testing software. If done correctly, the synthetic data\ncan contain no personal data even though it is based on a dataset that holds personal\ndata.\nAcademics have researched the use of synthetic data since the early 1990susing\n33\nstandard data-analysis and machine-learning algorithms likeparametric modelling,\nlinear regression, andbayesian networks.\nHowever, in the past few years, a sub-branch of machine learning called deep learning,\nwhich uses large datasets and neural networks, has made huge advancements in the\ngeneration of synthetic data. Deep-learning algorithms have recently gained public\nfame and notoriety as they have been used to create fake audio, videos and images of\nfamous people, places and, of course, cats. Theycan be used to generate\n34\nrealistic-seeming numerical and natural-language data. That is, they can be adapted to\ncreate anonymised synthetic data from standard structured datasets.\nRecently there has been an emergence of new companies specialising in  synthetic\ndata generated through deep-learning, with some specifically focusing on\nanonymisation. Their services include helping clients get past the ‘privacy bottleneck’\nand share data without being blocked by the legal and ethical risks associated with\nprocessing personal data.\nIn the public sector, the UK’s Office for National Statistics has done some early\ninvestigative work to see if synthetic data generated through deep-learning could be\nused to increase access to datasets. It found promisingresults, saying the neural\n35\nnetworks are‘excellent at approximating the datadistribution of the original datasets,\nuncovering the underlying patterns in the datasets’.However, they warned that the\nmodels can be‘susceptible to statistical noise’andthat synthetic data‘should always\nbe mixed with reality to make informed decisions’.\n35\nChaitanya Joshi, ONS Data Science Campus (2019), ‘Generative adversarial\nnetworks (GANs) for synthetic dataset generation with binary classes.’\nhttps://datasciencecampus.ons.gov.uk/projects/generative-adversarial-networks-gan\ns-for-synthetic-dataset-generation-with-binary-classes/\n34\nJonathan Hui (2018), ‘ Some cool applications of GANs’,\nhttps://medium.com/@jonathan_hui/gan-some-cool-applications-of-gans-4c9ecca35\n900\n33\nIoannis Kaloskampis, ONS Data Science Campus (2019), Synthetic data for public\ngood,https://datasciencecampus.ons.gov.uk/projects/synthetic-data-for-public-good/\n32\nAccess Now (2017), ‘Differential privacy, part 2: It’s complicated.’,\nhttps://www.accessnow.org/differential-privacy-part-2-complicated/\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification15\n\nAppendix 1: Going further\nThe goal of this ‘primer’ is to present an honest picture of why anonymisation for open\ndata is hard, but not impossible, and to give an overview of the field of anonymisation.\nThere are many more resources available for the data practitioner – whether you are\naiming to publish or share data through anonymisation, or using data which may have\nbeen anonymised.\nHere we have listed resources which may help you go further, several of which were\nproduced, or commissioned by the ODI in the course of our project onmanaging the\nrisk of re-identification.\n1.Anonymisation: a literature review– read more onthe history and research on\nanonymisation\n2.Anonymisation: case studies– Learn from examplesof successful (and\nunsuccessful) anonymisation in this case studies document\n3.Anonymisation Decision-Making Framework(ADF) – TheUK Anonymisation\nNetwork offers a comprehensive guide to the anonymisation process\n4.The ODI has created aprototype of a simplified step-by-stepinteractive guide\nto the Anonymisation Decision-Making Framework, usingour eLearning\nplatform\n5.Anonymisation: A Short Guide– a high-level step-by-stepguide to\nanonymisation, focusing on techniques rather than process, is also available as\npart of Eticas’s work on responsible open data\n6.Guide to the General Data Protection Regulation (GDPR)- provided by the\nInformation Commissioner’s Office (ICO) in the UK\n7.Anonymisation: managing data protection risk code of practice– published by\nthe ICO in 2015, provides practical advice on methods for anonymising data\nand the associated risks\n8.Anonymisation: register of actorsThe ODI and Eticashave produced a list of\nactors in the field, from academia to the private sector, who can help with\nanonymisation.\nOpen Data Institute 2019                  Anonymisation and open data: an introduction to managing the risk of re-identification16","version":"1.10.100"}