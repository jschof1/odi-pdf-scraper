{"numpages":18,"numrender":18,"info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"Skia/PDF m78"},"metadata":null,"text":"\n\n \n\n \n \nContents \nExecutive summary2 \nThe changing role of archives and memory institutions3 \nThe challenges of digital preservation3 \nGuaranteeing the integrity of closed records3 \nThe additional challenge of format shifting4 \nAMIs in a post-truth culture5 \nUsing blockchains and machine learning to underpin trust in archives8 \nA proof-of-authority distributed ledger for immutable storage8 \nWhat data is stored on the ARCHANGEL blockchain?9 \nDoes this really need a blockchain?10 \nCould we have used a good old-fashioned database?11 \nCould we have used a merkle-tree solution such as Git?11 \nCould we have used durable storage instead of a DLT?11 \nA technological solution, a social contract13 \nTemporal content hashes for content-aware document fingerprinting13 \nReinventing national archives15 \nThe ARCHANGEL prototype15 \nAn international pilot15 \nResults and findings16 \nWe shape our tools, and in turn they shape us17 \nFurther reading and references18 \nAbout \nThis report has been created in collaboration between the University of Surrey’s \nCentre for Vision, Speech and Signal Processing (Surrey), the UK’s National \nArchives (TNA), and the Open Data Institute (ODI). It was published in August \n2019. It uses, summarises and refers to material created and published between \n2017 and 2019 as part of the ARCHANGEL project. \n1\n \nLead editor was Olivier Thereaux (Open Data Institute). Authors were Alex Green \n(TNA), Arindra Das (ODI), Daniel Cooper (Surrey), Jamie Fawcett (ODI), Jared \nKeller (ODI), Jez Higgins (ODI), John Collomosse (Surrey), John Sheridan (TNA), \nMark Bell (TNA), Olivier Thereaux (ODI), and Tu Bui (Surrey). Other contributions \nand review by Caley Dewhurst, Becky Ghani, Anna Scott, Jeni Tennison, Rachel \nWilson (ODI). \n \nTo share feedback or to get in touch, contact the lead editor Olivier Thereaux at \not@theodi.org​. \n \nThe ARCHANGEL project was publicly funded by the UKRI/EPSRC Digital \nEconomy Programme under grant reference EP/P03151X/1.   \n1\n ​Surrey Blockchain (2019), ‘ARCHANGEL - Trusted Archives of Digital Public Records’, \nhttps://blockchain.surrey.ac.uk/projects/archangel.html  \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   1 \n\n \n \nExecutive summary \n“ \nExploring blockchain technology together with some of the \nworld’s leading archives, the ARCHANGEL project has shown, \nfor real, how archives might combine forces to protect and \nassure vital digital evidence for the future.  \n-- John Sheridan, Digital Director, The National Archives \nRecords have been preserved for thousands of years, and the preservation of \narchives is a mature discipline. In the United Kingdom (UK), the National Archives \nholds over 120 miles of papers and documents, from the Domesday book to \nrecent UK government cabinet meeting minutes. \n2\n \nOver the past two decades, society has experienced rapid technological change, \nwhich has resulted in vast quantities of information being captured and stored on \nmedia other than paper. Although practices around digital preservation have \ndeveloped over the past 25 years, many of them attempt to replicate archival \npractices designed for paper collections.  \n \nOne of the unique challenges of this shift to digital preservation is that of \nguaranteeing integrity of the digital records. Digital records – that are transient, \neasy to copy and modify, and prone to corruption in copy and storage – often \nneed to be ported from one format to another, as technology evolves and \nsoftware used to read certain formats stops being available. In a context of \nincreasing mistrust in institutions and attacks on the notion of truth, this makes for \nan explosive, existential challenge for archives and memory institutions (AMIs).  \n \nThe ARCHANGEL project has been exploring the possibilities offered by \ndistributed ledger technology (commonly known as blockchain) and machine \nlearning and how they could address the challenges around trust, integrity and \nauthenticity that preserving born-digital material presents.  \n \nOver two years – between July 2017 and July 2019 – a team formed with \nmembers from the University of Surrey (Centre for Vision, Speech and Signal \nProcessing), the UK’s National Archives, and the Open Data Institute (ODI). The \naim of this team was to collaborate in exploring, developing and prototyping these \ntechnologies to underpin trust in digital archives, and the work culminated in the \npilot of a system across five countries. \n \nThe technology developed through the ARCHANGEL project, and the results of \nthe pilot study, were incredibly promising. It showed how the appropriate use of \nemerging technology could change digital archiving methodologies and create \nnew collaborations between institutions.  \n \nBeyond archives, the results of the ARCHANGEL project have potential to inform \nand support other domains where truth and integrity of information over time – \nsuch as journalism – are crucial to their long-term sustainability. \n  \n2\n ​The National Archives (2014), ‘Secrets Of The National Archives’, \nhttp://bookshop.nationalarchives.gov.uk/9780091943356/Secrets-Of-The-National-Archive\ns/ \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   2 \n\n \n \nThe changing role of archives \nand memory institutions \n“ \nIt is becoming easier and easier to manipulate digital records, \nwhich makes it crucially important for the institutions who take \ncare of those records to be able to demonstrate their \ntrustworthiness.  \n-- Jeni Tennison, CEO, Open Data Institute \nThe challenges of digital preservation \nTo fulfil their role, archives and memory institutions (AMIs) need to be both \ntrustworthy and trusted. They need to do everything they can to prevent the \ncorruption of historical records and be seen to be doing everything they can to \nachieve this. \n \nOne of the important challenges AMIs face is the shift from primarily physical \nobjects to primarily digital objects.  \n \nOrganisational practices, in government and beyond, are increasingly shifting from \nphysical to digital – from paper memos to emails, printed reports to PDFs, \noverhead projector transparencies to digital presentation slides. \n \nOne specific focus of digital-preservation researchers is guaranteeing the integrity \nof these born-digital objects – ie that they remain unaltered while stored in the \narchive. While changing physical objects without obvious evidence of tampering is \ndifficult, digital objects by their nature are relatively easy to change. This raises an \nimportant question: how can AMIs guarantee that a stored document is the same \ndocument that was originally archived? \n \nIn theory, public scrutiny can help: members of the public can compare \ninformation in one record against other sources, or in an earlier copy. However, \nthis is made particularly difficult for sensitive closed records that may not be \nreleased for decades – records that are nonetheless vital to historical scrutiny. For \nexample, records that could compromise government operations if published \ncontemporaneously are securely stored until a predetermined amount of time \npasses, at which point they become available to the wider public. In the UK, this is \nknown as the ‘20-year rule’.  \n3\nGuaranteeing the integrity of closed records \nWhile objects are being stored, only archivists with appropriate permission can \naccess them to ensure they are properly preserved. \n \nWhile the practice of keeping records closed for decades is not new, in the past it \nwould have been relatively straightforward to know whether a paper-based \nrecord, once opened, had been redacted or doctored. Much less so for digital \n3\n ​The National Archives (2015), ‘The 20-year rule’, \n‘​http://www.nationalarchives.gov.uk/about/our-role/transparency/20-year-rule/  \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   3 \n\n \n \nrecords: deleting a problematic paragraph can be done in a keystroke; and \nforensically examining electronic documents to find evidence of tampering is still \nvery much a complex art. \n \n \nExample of a closed record description​ in the catalogue of The National Archives \n \nThe additional challenge of format shifting \nThe repositories at The National Archives contain around 200km of shelving that \nholds millions of original paper, parchment and photographic documents. These \ndocuments are kept in very precise atmospheric conditions, with tightly controlled \ntemperature and humidity.  \n \nSimilarly, digital documents are held in conditions conducive to long-term \npreservation, minimising the degradation of tapes and spinning disks. Digital \ndocuments differ from paper in that they are not really the original: they have, at \nsome point in their lifetime, been copied from one medium to another.  \n \nFurther backup copies are made by the archive to de-risk the preservation \nprocess. In medieval times, scribes would make copies of documents and only \ncarefully comparing the copy and the original could verify a faithful copy. In the \ndigital world, we use methods originating in cryptography to automatically verify \nthat not a single bit is out of place in a copied file.  \n \nDigital files are made up of electronic ‘bits’ which encode their contents together \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   4 \n\n \n \nwith formatting instructions for the software that will be used to display – or render \n– them. We can use mathematical techniques (as used in cryptography) to reduce \nthese millions of bits down to a unique, short, alphanumeric string of characters \nfor every file. If even one bit of the file were to change, the unique string would \nchange. This unique string is sometimes called a cryptographic hash, a fingerprint, \nor a checksum.  \n \nWhen a file is received by the archive they generate the file’s hash and store it in a \ndatabase. Regular recomputation of file hashes are made and compared with the \noriginal hash to proactively identify corrupted files. If corruption is detected by this \nprocess, the file can be replaced with an uncorrupted backup copy. \n \nWhen archives present a user with a digital file, there are two options available: \nthey could download an exact copy of the original file, or download the file in an \nalternative format. One reason for the second option being presented is because \nas time goes by software becomes obsolete. It is replaced with updated versions \nor entirely new software, and the file formats change with them.  \n \nFor example, WordStar was a very popular word processor in the 1980s but there \nis no longer a version that runs on modern computers, although emulators, \n4\ncreated by enthusiasts, are available. A WordStar file may be opened in a modern \nversion of Microsoft Word after first installing a conversion add-in. This keeps the \nformat alive and usable for now, but can we guarantee that these files will load on \na standard computer in 20 or 50 years? Even if it can read a file, a modern word \nprocessor is not necessarily faithfully rendering the original.  \n \nIn the interests of long-term preservation, and for the convenience of users, the \narchive may create copies of these WordStar files and then convert them to an \nopen format which is more likely to still be readable decades from now. Similar \nactions may also be taken for formats such as high-definition video, converting \nthem to a compressed format to reduce the download time, again for user \nconvenience. \n \nChanging formats in this way introduces a problem: mathematically, a converted \nfile is different to the original – it contains different formatting bits even if the \ncontents are unchanged – and so the system of comparing checksums breaks \ndown and identifies these as non-identical files.  \n \nSoftware providers have used checksums for years to allow customers to verify \nthat they are downloading a genuine copy, and the AMI can use them to verify \nborn-digital files in the same way. By changing the format, however, we are \noffering a cryptographically different file to the one which was originally deposited.  \n \nHow can we assure the integrity of the file during the conversion process, \nespecially in cases where there may no longer be software available to render the \noriginal? Is there a way of demonstrating that two files in different formats are still \nthe same without comparing them side by side? Or, when applied to video \nmaterial, without resorting to a painstaking task of comparing two long videos \nframe by frame?  \nAMIs in a post-truth culture \nAMIs are continually developing ways of preventing errors, data corruption and \nother issues. The digital preservation domain had adopted and built on industry \nbest practice in this area, and is offering increasingly effective solutions to guard \nagainst error, data corruption and other effects of degeneration. \n \n \nBut the existential challenge to AMIs is not only technological – it is also societal, \n4\n Jenny Mitcham, ​What are the significant properties of a WordStar file? \nhttps://digital-archiving.blogspot.com/2018/08/what-are-significant-properties-of_85.html \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   5 \n\n \n \nand cultural. Many of the technological solutions to detect and prevent errors or \ndecay do nothing to prevent the deliberate modification of records. And while \nthere is no army of hackers intent on covertly corrupting the nation's record, the \n5\nattacks on archives are rather more indirect.  \n \nRecords could be changed from within, for example by order of new governments \nkeen to rewrite history in their favour – as the 20th century has shown time and \ntime again.  \n \n \nNikolai Antipow, Sergej Kirow and Nikolai Schwernik edited, over time, from the record for propaganda \npurposes. From ​https://commons.wikimedia.org/wiki/Category:Altered_Soviet_photographs  \n \nAt the time of writing, the public discourse frequently undermines societal trust in \npublic institutions. Heads of state label media organisations as ‘fake news’, and \nraise doubts about the competence or neutrality of intelligence and law \nenforcement services on matters of national security; politicians label judges as \n‘enemies of the people’; a UK minister states in his resignation letter that he \nbelieves the civil service is providing misleading briefings. We also live in a time \n6\nwhen easily-created synthetic content is capable of generating believable videos, \nputting words into politicians' mouths.  \n \nIn this atmosphere, AMIs do not need to be directly attacked. They are damaged \nsimply by the miasma of institutional mistrust. Indeed, AMIs are, perhaps, \nparticularly at risk. Their age and importance puts them at the heart of ‘the \nestablishment’. Many of the records they hold are not immediately available for \npublic view. By policy or by law, many documents are redacted or even entirely \nwithheld from the public for a period of time.  \n \nThose wishing to challenge the integrity of released records could plausibly \nsuggest records are incomplete or altered in some way.. As the documents in \nquestion are, in this modern age, entirely digital – having never existed as physical \nobjects – how could an archive demonstrate otherwise? \n5\n Or rather, there probably is a motley array of hackers, but major archives are well \npracticed in both network and physical security. You can't tamper with a file that's  not \nnetwork connected unless you're willing to engage in Mission Impossible-style shenanigans \nwhile deep underground in an otherwise disused salt mine. \n6\n “Unfortunately, I do not believe the briefings you have received on these matters recently \nhave reflected all they have achieved or the preparations our European partners have \nmade” - Chris Heaton-Harris resignation letter, April 2019 \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   6 \n\n \n \nUsing blockchains and \nmachine learning to underpin \ntrust in archives \n“ \nBy combining blockchain and artificial intelligence \ntechnologies, we have shown that it is possible to safeguard \nthe integrity of archival data in the digital age.  \nIt essentially provides a digital fingerprint for archives, making it\npossible to verify their authenticity.  \n-- Prof. John Collomosse, University of Surrey \nA proof-of-authority distributed ledger for \nimmutable storage \nThe ARCHANGEL system, created through this project, uses distributed ledger \ntechnology to guarantee that document fingerprints cannot be altered, and \nmachine learning to create fingerprints that can withstand format shifts. \n \nBlockchains, or distributed ledger technology \n \nOften considered synonymous with Bitcoin, blockchain is the technology that \nunderpins a number of digital currencies but it has the potential for far wider \napplication.  \n \nAt its heart, it is the digital equivalent of a ledger, like a database but with \nthree features that set it apart from standard databases.  \n \n●First, a distributed ledger technology is immutable – or ‘append \nonly’ – meaning that data cannot be overwritten, amended or \ndeleted; it can only be added to. \n●Second, it is distributed. No central authority or organisation has \nsole possession of the data. Instead, a copy of the whole \ndatabase is held by each member of the network and they \ncollaborate to validate each new entry before it is written to the \nledger. As a result, there is no centralised authority in control of \nthe data and each participant has an equal status in the \nnetwork: equal responsibility, equal rights and an equal stake.  \n●Third, it is transparent. All entries in the ledger are visible to all \nwho have a copy. \n \nThe main feature of the ledger used for ARCHANGEL is its immutability: because \ndata on the ledger can not be amended, it prevents tampering of the fingerprints \nafter the fact. \n \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   7 \n\n \n \n \n \nARCHANGEL uses what is called a ‘permissioned blockchain’ – anyone can keep \na copy of the ledger, but only participating AMIs are given the ability to write \noperations into the chain.  \n \nPermissioned blockchains have particular advantages over typical public \nblockchains. The most common criticism of blockchain is that it is sensitive to \n‘51% attacks’. An operation on the blockchain is validated when a majority of \n7\nnodes confirm the hash of the block to be written to the ledger. As there are \ntypically so many members, it is deemed almost impossible to add a fake \ntransaction, as more than 50% of the network’s processing power would have to \nverify the transaction. However, with enough resources – ie by members \ncombining their processing power or one member joining multiple times – any \nanonymous actor can take over the network by reaching 51% of stakes or ‘mining \npower’. This is a higher risk in a public, anonymous blockchain. In a permissioned \nblockchain, the risk of ‘51% attacks’ is lessened because every member has been \ninvited, and their identity is known.  \n \nBecause ARCHANGEL uses a permissioned blockchain, each member of the \nnetwork gets a vote when ‘sealing’ the record, meaning that more than half of \nparticipating archives would have to collude to somehow falsify a record.  \n \nARCHANGEL also uses a proof-of-authority consensus mechanism. This also \n8\nputs all the participating archives on equal footing in providing assurance and \naccountability for each other, nationally and internationally.  \n  \nWhat data is stored on the ARCHANGEL blockchain? \n \nARCHANGEL needs to prove to the public that records shown to them are the \nsame as those received by the archives. To do so, the system stores the \nfingerprints of all the records in the blockchain. It does not, however, store the \nrecords in the blockchain, mainly for one reason: the information on a blockchain \nis visible to anyone who has a copy, therefore keeping the records on the \nblockchain would make it impossible to use it for closed records.  \n \nThe sensitivity of public records sometimes extends to their filenames or \ndescriptions. Adding these metadata fields to the blockchain would therefore not \nbe appropriate. As a team, we settled on a selection of fields that included an \narchival reference and the record’s checksum: a unique alphanumeric string \ngenerated by a mathematical algorithm that changes completely if even one bit is \naltered in the file.  \n \nIn this way, a researcher can lookup the archive reference in the blockchain to \nfetch its original checksum – recorded perhaps decades ago – and compare it \nwith the checksum of the record they have just downloaded.  \n \nIn summary, the ARCHANGEL blockchain enables:  \n \n●an archive to upload metadata that uniquely identifies specific records \n●that data to be sealed into a ‘block’ that cannot be altered or deleted \nwithout detection \n●a copy of the data to be shared with each of the other trusted members of \nthe network for as long as the AMIs maintain it.  \n \n \n7\n Open Data Institute (2016), ‘Applying blockchain technology in global data infrastructure’ \nhttps://theodi.org/article/applying-blockchain-technology-in-global-data-infrastructure/ \n8\n Unlike proof-of-work blockchains, which rely on members of the chain computing \nintensive mathematical challenges (‘mining’) as a way to prevent 51% attacks. This makes \nsuch blockchains extremely energy-hungry. The proof-of-authority type of distributed \nledger used in ARCHANGEL does not rely on such a mechanism, and is therefore unlikely \nto ever consume as much energy as a small country. \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   8 \n\n \n \n \nIllustrating the storage of metadata about closed records in an immutable ledger \nFrom ODI Lunchtime Lecture: ​Can technology help reinvent national archives for the 21st Century? \nDoes this really need a blockchain? \nThe ARCHANGEL blockchain seems to be a good solution to the digital \npreservation needs we have identified earlier. As an immutable storage solution, it \ncreates a barrier against tampering of the record after the fact. Its transparency \nand the ability for anyone to keep a copy makes the record verifiable. And for the \nsake of performance, the ledger is separate from storage. This also enables use \nfor closed records. \n \nBut if the point of the ARCHANGEL project is to try to defend AMIs from \nallegations of improper conduct, is an infrastructure based on a blockchain the \nbest vehicle?  \n \nThere is a public perception that blockchains go hand in hand with \ncryptocurrencies, a domain of wild speculation and rampant crime.  \n9\n \nIts reputation also suffers from a tendency by technology vendors to oversell \ntechnical solutions to complex problems. For example, IBM's widely publicised \nblockchain work purports to be a groundbreaking example of secure new \ntechnology. Its electronic bill-of-lading proof-of-concept claimed, for example, \nthat it reduced transaction time from five-to-seven days to under one second, \nfailing to mention that the five-to-seven days was for a paper-based system that \nwas rendered obsolete decades ago by EDI systems that IBM itself played a large \npart in designing. \n10\n \nCan ARCHANGEL help strengthen trust in AMIs using a technology which is itself \nsometimes seen as untrustworthy, or at least over-hyped? \n \nThe core feature ARCHANGEL provides is the ability to verify the integrity of an \nelectronic artefact produced by an archive. It does this by providing a \ntamper-proof log of artefact checksums. Are there alternative technologies we \nmight be able to use to achieve the same ends? \n \n9\n Coin Telegraph (2019), ‘Report: Indictment Reveals Connection to Bitfinex, QuadrigaCX’s \nShadow Banking Services’, \nhttps://cointelegraph.com/news/report-indictment-reveals-connection-to-bitfinex-quadriga\ncxs-shadow-banking-services  \n10\n European Paten Register (1991), ‘European patent EP0507717A2: Method and apparatus \nfor interchange of customization characteristics of formatted business data’, \nhttps://register.epo.org/application?number=EP92480032 \n \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   9 \n\n \n \nCould we have used a good old-fashioned database? \n \nDo we need distributed ledger technology at all? Could ARCHANGEL simply take \nthe form of additional metadata, published in the archive's catalogue database? \n \nConsider how the process of verifying a document would work when using a \ndatabase which is neither distributed nor immutable. Having obtained a document \nprovided by an archive and generating the document fingerprints, we are then \ngoing to compare the fingerprints against a database provided by that selfsame \narchive. If the fingerprints match, it actually does nothing to establish the \nauthenticity of the document.  \n \nWhat if a third party produces another copy of the database which differs from the \narchive's current database? We are then in the same position as trying to \nestablish the true copy of the artifact in question. There is no way to prove which, \nif either, of the two versions is a true copy of the database. \n \nIn a situation like this, those disinclined to trust an archive will not be reassured \nand will remain unconvinced that integrity can be guaranteed. \n \nCould we have used a merkle-tree solution such as Git? \n \nRather than a conventional database, perhaps a more unconventional datastore \nmight be more suitable. Perhaps something built on a Merkle tree (a tree in which \nevery leaf node is labelled with the hash of a data block, and every non-leaf node \nis labelled with the cryptographic hash of the labels of its child nodes). \n11\nSomething like Git perhaps, the source-code control system used by millions of \n12\nprogrammers around the world, notably in the popular GitHub platform. \n \nWithin a Git repository each entry – typically a set of software changes, but \ndocument metadata in the ARCHANGEL case – forms part of a journal. New \nentries are linked to one or more previous entries, and will in turn, be linked to by \nsubsequently entries. Entries can be cryptographically signed, allowing their origin \nto be verified against published public keys.  \n \nThe features sound very similar to those offered by a blockchain-based solution, \nwhich should not be a surprise: merkle trees are also used as the tamper-proof \ndata structure in blockchain.  \n \nHowever, an ARCHANGEL built around Git, or a system that is similar, would still \nsuffer the same fundamental flaw as the previous database example. It would \nagain simply require institutional trust in a master copy for copies to stay \nconsistent with each other. This is because, while the core data structures are \ntamper-proof, the design of Git is such that additions to the journal are not \nautomatically duplicated to the copies. Instead the owner of one copy must ask \nfor updates from a chosen peer in the network to synchronise with each other.  \n \nEven making the Git repository publically available and signing entries is not proof \nagainst deliberate tampering. Any publically available Git repository is a snapshot \nin time and, in any case, is not necessarily the 'master' copy.  \n \nCould we have used durable storage instead of a distributed \nledger technology? \n \nThere are a number of long-term durable storage providers, perhaps the most well \nknown is Amazon's S3 Glacier product. Amazon describes S3 Glacier as \"a \nsecure, durable, and extremely low-cost cloud storage service for data archiving \nand long-term backup\". ‘Secure’ here means secure against theft or tampering, \nwhile ‘durable’ means the data will be preserved as-is without loss or corruption.  \n13\n11\n ​Wikipedia (2019), ‘Merkle tree’, ​https://en.wikipedia.org/wiki/Merkle_tree \n12\n ​https://git-scm.com/ \n13\n Amazon claims 99.999999999% durability. \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   10 \n\n \n \n \nThe service offers a variety of access policies, including a write-once read-many \npolicy that provides protection against tampering even by the data owner – data \ncan be written once, but then can not be altered or deleted. S3 Glacier is \nsufficiently well developed, both in terms of technology and process, to achieve \ncompliance with the UK Government's G-Cloud and the US Department of \nDefence Data Processing requirements, among others. \n \nIn a system based on Glacier, we could generate fingerprints of our archived \nrecords, and write them into our Amazon S3 Glacier storage. At a future time \nwhen the record is produced, archives can provide a pointer to the corresponding \nGlacier entry. Interested third-parties could find the fingerprint within Glacier and \nverify the record against it. \n \nThis would certainly be a workable system from a technological point of view. \nHowever, it fails a number of our non-functional requirements:  \n \n●This system is neither transparent nor open. Interested third parties \ncannot see what has been written to Glacier storage, which reduces \npublic scrutiny \n●Once our third parties do know what they want to retrieve, then that data \nis not open and available to them.  \n●They need to pay a data-retrieval fee to access the data. \n \nS3 Glacier, and services like it, are built to serve a particular market for long-term \ndata archival with infrequent retrieval which doesn’t need to be fast – such as \nauditable accounts – and thus solve a different problem.  \n \nFor example, a financial institution has a number of reporting and data retention \nobligations set by the industry regulator. It needs to report the specified data \npromptly, perhaps at the end of the tax year, and then store it securely for a \nperiod of time. In practice, so long as the financial institution operates in a normal \nway, within the bounds of accepted business practice, it does not attract the \nattention of the regulator and so may never need to retrieve data from the store.  \n \nS3 Glacier  – and the clue is in the name – provides ​cold storage – ​a data storage \nsystem that is accessed less frequently and doesn't require fast access. The \nexpectation is that the data is rarely, if ever, needed again. Consequently, the fee \nstructure actively discourages data retrieval: writing and storage is cheap (indeed, \nvery cheap), but data recovery is comparatively expensive. Data retrieval can take \nhours to complete – unless you pay an even bigger fee – and the retrieved data is \navailable for only a limited period. \n \nHash verification requires immediate access. If someone wants to verify the \nauthenticity of a document, they should be able to do it immediately, not many \nhours later after paying a fee. Observers should, if they wish, be able to access \nthe entirety of records. Records need to be transparent and this form of durable \nstorage hides the record. \n \nSetting this problem aside – perhaps an ARCHANGEL-like system could come to \nsome suitable commercial arrangement with a storage provider. But the presence \nof a commercial third party presents another barrier to trust. Public trust in large \ntechnology companies is eroding and this is unlikely to change in the near future. \n14\nIt is not particularly farfetched to argue that if an archive is paying a service \nprovider to store the artefact fingerprints, that archive might also pay that provider \nto change or delete fingerprints if necessary.  \n \nLastly, there is the additional question of organisational longevity. ARCHANGEL \naims to be a system for the very-long term. The average lifetime of an S&P listed \ncompany (widely-used gauge of performance for US companies, often used as a \n14\n EY (2019), ‘Why trust in tech giants is eroding, and how it can be rebuilt’, \nhttps://www.ey.com/en_gl/trust/why-trust-in-tech-giants-is-eroding--and-how-it-can-be-re\nbuilt \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   11 \n\n \n \nproxy for ‘the market’) is 15 years. ARCHANGEL would need to rely only on \n15\norganisations guaranteed to exist for decades and beyond. \n \nA technological solution, a social contract \n \nNone of the technologies we considered above fit the bill for ARCHANGEL, but \nnot only for purely technological reasons. One could build the most secure system \nyet, but unless that system can be comprehensively examined and critiqued by \nothers, we are reliant solely on the system builder's assurance whom we then also \nneed to trust.  \n \nBlockchain technology provides the transparency and openness demanded by \nARCHANGEL by enabling the following: \n \n●The ARCHANGEL blockchain is, by design, publicly readable to provide \nscrutiny; while the ability to write to the blockchain is limited to \nparticipating AMIs \n●Various tools – many of them open source –  can be used to access and \nkeep a copy of the chain.  \n16\n●The consensus mechanism replaces a single institution's assurance with a \ncollective assurance.  \n●No one participant can change the record, no matter how much they may \nwish to.  \n●In the unlikely event that a cohort attempted to subvert the record, that \nattempt would immediately be detected by others in the consortium.  \nTemporal content hashes for content-aware \ndocument fingerprinting \nBuilding and delivering this system is complicated by another challenge in digital \narchiving – changing digital formats. Digital formats shift over time – new ones are \ncreated, old ones are retired – and new software might no longer support the \nsame formats supported by previous versions. For example, some modern video \nplayers will not open older video files. This presents a fundamental challenge for \ndigital archivists striving to preserve documents for the future. \n \nVideo formats rapidly become obsolete, motivating format shifting (transcoding) as \npart of the curatorial duty to keep content viewable over time. This leaves video \npreservation at risk of modification – either due to direct attack (tampering), or due \nto accidental corruption, such as truncation or frame corruption due to bulk \ntranscoding errors. \n \nCryptographic hashes operate at the bit level. They are effective at detecting video \ntampering, but not for videos undergoing transcoding: a bit-level fingerprint of a \nvideo using format A would be entirely different to that of the exact same video in \na different format B. This means that, to guarantee the integrity of video records \n(such as video proceedings of the UK Supreme Court, which are deposited and \nkept at the National Archives), one has to find a solution involving content-aware \nand format-agnostic hashing of the audio-visual stream itself.  \n \nOur approach for ARCHANGEL was therefore to explore and prototype the \ncreation of hashes using machine-learning methods, particularly for image and \nvideo content, rather than ‘traditional’ bit-level hashes. \n \n15\n The situation varies around the world. In Japan, a number of businesses are over 1000 \nyears old. However, none of those are large technology outfits that could provide this kind \nof service. \n16\n The ARCHANGEL prototype was built on the Ethereum stack, which provides a number \nof tools and libraries at ​https://geth.ethereum.org/downloads/ \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   12 \n\n \n \nContent-aware hashing: not new, still a challenge \nA number of technologies and solutions already exist that attempt to \ntackle the challenge of content-aware hashing for video. One of the \nmost well-known ones is ContentID, used to detect and prevent \ncopyright infringement on the Youtube video platform. While Google \nsays they have invested over $100m into ContentID, the system is \n17\nnot infaillible.  \n18\nUsing a content-aware hashing to guarantee the integrity of records in \na national archive is extremely challenging: \n●False negatives are unacceptable: failing to detect tampering \nwould make trust in the system collapse, and make the whole \nthing useless \n●False positives are also highly undesirable: if the system over \ndiagnoses tampering, trust collapses too – or a trusted party has \nto take on manual checking, limiting the usefulness of the \nautomated system. \n \nThe solution proposed by the research team on the ARCHANGEL project uses \nDeep Neural Networks to create a ‘temporal content hash’ that is trained to \n19\nignore transcoding artifacts, but is capable of detecting tampers of a few seconds \nduration within typical video clip lengths within a short amount of time (ie minutes \nor hours). \n \nBy using both original video as well as derived, transcoded videos as training \nmaterial, the machine-learning system is able to accurately differentiate whether \nglitches and noise – in either the audio or video signal – are caused by transcoding \nand format-shifting, or are caused by any undesirable process, including \ncorruption of the files in storage, or tampering of the record. \n \nIt is worth noting that this system is very different, in its approach and its purpose, \nfrom other kinds of content-aware digital fingerprinting systems. Systems like \nContentID attempt to answer the questions like: “Does this piece of content \nclosely resemble anything else in our catalogue”; the temporal content hashes in \nARCHANGEL aim to answer the question: “Can we say with high certainty \nwhether differences between content pieces A and B are artefacts of \nformat-shifting or the result of tampering?”.  \n17\n ​Protecting what we love about the internet: our efforts to stop online piracy​, Google \nPolicy blog, Nov 2018 \nhttps://www.blog.google/outreach-initiatives/public-policy/protecting-what-we-love-about-i\nnternet-our-efforts-stop-online-piracy/  \n18\n ​Sorry professor, old Beethoven recordings on YouTube are copyrighted​, Ars Technica, \nMarch 2018 \nhttps://arstechnica.com/tech-policy/2018/09/how-contentid-knocked-down-decades-old-r\necordings-of-beethoven/ \n19\n More on the algorithm at ​https://arxiv.org/abs/1904.12059 \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   13 \n\n \n \nReinventing national archives \nThe ARCHANGEL prototype \nTo test the technology solution proposed in the ARCHANGEL project, a prototype \nwas created to explore the feasibility of the technology, as well as the desirability \nand suitability of the solution for AMIs. \n \nThe prototype consisted mainly of two intertwined systems: \n \n1.A permissioned network built on the Ethereum blockchain stack – the \nARCHANGEL distributed ledger.  \n \n2.A digital preservation tool, implemented both as a simple web application \nand a desktop app. Designed as closely as possible to the typical user \nexperience of the digital archivist, the tool uses the language and \nmetaphors of the Open Archival Information System (OAIS).  \n \nThe tool integrates with another system typically used by archives such as \nthe National Archives’, the DROID (Digital Record Object IDentification) file \nformat characterisation tool. We used it to help the digital archivist \ndescribe the digital records being deposited, create fingerprints, and store \nthose in the ARCHANGEL blockchain network. \n \nThe tool also included the feature described above of creating temporal \ncontent hashes for video. Given the resource-intensive process of training \ndeep neural networks, the feature was achieved not directly in the \napplication, but instead integrated through an API, by software deployed \non the University of Surrey digital infrastructure. \n \nMost of the software developed for the prototype is available on GitHub at: \nhttps://github.com/archangel-dlt/​. \n \nEarly tests of the prototype demonstrated the technical feasibility of the system. \nBut – given that such a distributed system requires several archives to use it and \nparticipate in hosting nodes of the blockchain network – we still needed to \ndiscover whether external digital preservation practitioners would: understand \nwhat the prototype did; see value in the methodology; and have an interest \nbeyond a simple pilot. \nAn international pilot \nTo address this question, in April 2019, the ARCHANGEL project team conducted \na pilot of the prototype with national AMIs in five different countries: the UK, \nAustralia, Norway, Estonia, and the US. \n \nThe aim of the pilot was to gather insights on the ARCHANGEL concept and its \nprototype from participating institutions as they used the prototype system in \nparallel with their routine archival process for a limited period of time. \n \nTo better understand the viability, desirability, feasibility and usability of the \nprototype, the project team ran user research with pilot participants. Some of the \nquestions covered by the user research included: \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   14 \n\n \n \n●Did they engage with the prototype? Was it appealing? Did it fit \ntheir usual mental model? Did the prototype help solve problems \nand pain points known to the archivists? \n●Did the archivists understand the underlying technology of \nARCHANGEL (distributed ledger technologies, hashing and \ncomputer vision)? Would that understanding matter in order for \nthem to have trust in the system and, importantly, to see it as an \nimprovement on existing processes and structures? \n●Did the participating archivists understand that they are mutually \nunderwriting the integrity of other archives in return for others \ndoing the same? Did they assign value in being part of a \nblockchain?  \n●Was it more important to them that all tampered videos should be \nreported (with the occasional false positives); or should videos be \nreported as ‘tampered with’ only when the system is fully \nconfident? \n \nResults and findings \n“ \nThe key thing for us is blockchain environment really. [...] \nWhere I work there's a 20-year closed period. That's a long \ntime for files and metadata to be sitting around. If we can \nprove authenticity and integrity through a tried and tested \ntechnology, [...] then that's going to help us a lot just through \nmultiple generations of technology. \nThe user research-generated insights and gave us access to a wide range of \nideas, and a broad perspective on the experiences of various kinds of archivists.  \n \nOverall, the participants of the pilot showed positive sentiment towards the \nARCHANGEL concept and prototype. They found the interface clean and simple. \nHowever, some were not clear about the inner workings of the prototype. \n \nThe main differentiator in the response to the pilot was whether the participants \nhad prior understanding and knowledge of blockchain technology.  \n-Those who did, understood the underlying value of mutually underwriting \nthe integrity of other archives and trusted the system.  \n-Those who did not, while appreciating the value of software that creates \nfingerprints for digital records, understood ARCHANGEL’s trust framework \nas inherently residing with the participating organisations.  \n \nThe pilot also generated insights that would be useful for future iterations, such as \nmaking the software more modular to enable configurable workflows, and an \nadditional interface for independent validation of checksums. Those insights are \ndiscussed in detail in the research report. \n20\n  \n20\n ARCHANGEL pilot - User Research Report, ODI 2019 \nhttps://docs.google.com/document/d/1GQL23E9aQNpc5vB1UtETEFz0tt8hYAb_qNlmldDQ\nY8Y \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   15 \n\n \n \nWe shape our tools, \nand in turn they shape us \n \nCollaboration is essential to the value offered by blockchain technology. In the \nproof-of-authority system chosen for ARCHANGEL, every participating \norganisation has an equal role in providing assurance of integrity.  \n \nIn a large enough network, this means that collusion and tampering would be \ndifficult to achieve and relatively easy to notice. And since every participating AMI \nkeeps a copy of the ledger, this creates a form of insurance against crises such as \npolitical turmoil in a given country or region – outsourcing some of the \npreservation effort to the network.  \n \nThis means the success of the ARCHANGEL blockchain relies on a minimum \nnumber of participants – we think at least seven – from as many different \ninstitutions as possible. Without a minimum number of participants the trust that \nthe technology engenders is in danger of being lost.  \n \nIn a live, future environment we would hope to involve participants beyond the \narchive sector such as news organisations, and other transparency-minded \ngroups who –  as well as providing external oversight – also have a stake in the \nassurance of public records. \n \nA distributed approach to assuring trust is another step in the long-running trend \nof memory institutions relying on each other. Archival capability is distributed and \nshared in terms of know-how and in the development and maintenance of tools \nand archival resources (for example PRONOM and LOCKSS). In the case of \n2122\nweb archives, the collections themselves overlap and content is shared, as \narchives supply each other with content to fill gaps in their collections. \n \nBecause the challenge is great and as institutions’ archives are quite small, this \napproach is the key to winning the technology arms race between archives and \nthose parties who use the tools to falsify our digital inheritance.  \n \nIf this approach to distributed services is successful, it is exciting to think about \nwhat else could be distributed in the future.  \n  \n21\n National Archives (2006), ‘PRONOM’,​http://www.nationalarchives.gov.uk/pronom \n22\n Stanford University (2019), ‘LOCKSS’, ​https://www.lockss.org \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   16 \n\n \n \nFurther reading and \nreferences \nThe project team created papers, articles, blog posts and presentations in the \ncourse of the two-year ARCHANGEL project. Several of them were used in part \nand summarised in this document, but they typically go much further, especially in \nexplaining the technical aspects of the system. \n \n●ARCHANGEL: Trusted Archives of Digital Public Documents – ​ACM \nDocument Engineering ​2018. \nhttps://arxiv.org/abs/1804.08342 \n●ARCHANGEL: Tamper-proofing Video Archives using Temporal Content \nHashes on the Blockchain – CVPR Blockchain Workshop 2019. \nhttps://arxiv.org/abs/1904.12059 \n●A Blockchain For Archives: Trust Through Technology, \nhttps://www.archivoz.es/en/a-blockchain-for-archives-trust-through-techn\nology-2/ \n●Underscoring archival authenticity with blockchain technology​, Insights, \nhttps://insights.uksg.org/articles/10.1629/uksg.470/ \n●Blockchain’s potential role in the future of archiving, ​ODI blog \nhttps://theodi.org/article/blockchains-potential-role-in-the-future-of-archiv\ning/ \n●Challenges in using blockchains to build trust in digital archiving​, ODI blog. \nhttps://theodi.org/article/challenges-in-using-blockchain-to-build-trust-in-\ndigital-archiving/ \n \n \n \nOpen Data Institute, The National Archives, University of Surrey 2019ARCHANGEL Project Report   17 ","version":"1.10.100"}