{"numpages":44,"numrender":44,"info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"Skia/PDF m86"},"metadata":null,"text":"\n\n \n\n \nContents \n \n \nContents1 \nAbout3 \nForeword3 \nExecutive summary4 \nRecommendations5 \nIntroduction7 \nBackground9 \nWhat do we mean by evaluation?9 \nWhat do we mean by new models of data stewardship?10 \nThree use cases12 \nRelevance for Covid-1913 \nUse case #1: Digital-first primary care15 \nUse case description and background15 \nFindings15 \nWhat an evaluation would aim to assess or demonstrate15 \nData an evaluation would need16 \nChallenges or barriers to accessing necessary data16 \nRecommendations and next steps16 \nEmbed mandatory access to data for evaluation purposes in procurement \ncontracts17 \nEnable institutional efforts to improve access to NHS data to support \ntechnology evaluation17 \nEngage with existing initiatives and consider piloting a data institution to \nfacilitate access to NHS data for the purpose of evaluations18 \nExplore the potential for existing organisations to take on new data \nstewardship roles19 \nAdvocate for technologies to be implemented in a more evidence-based way, \nwith early input from evaluators19 \nCovid-19 context20 \nUse case #2: Online misinformation and vaccine hesitancy21 \nUse case description and background21 \nFindings21 \nWhat an evaluation would aim to assess or demonstrate21 \nData an evaluation would need22 \nChallenges or barriers to accessing necessary data22 \nRecommendations and next steps23 \nExplore the use of data portability to collect contextual data from offline \nsources23 \nExplore the use of plugins and data portability to collect data from online \nsources24 \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 1 \n\n \nConsider piloting a data institution aimed at facilitating access to data held \nby online platforms25 \nEngage with stakeholders to identify their views and needs26 \nRethink the scope of evaluations on this topic26 \nCovid-19 context27 \nUse case #3: Patient flow automation28 \nUse case description and background28 \nFindings29 \nWhat an evaluation would aim to assess or demonstrate29 \nData an evaluation would need29 \nChallenges or barriers to accessing necessary data29 \nRecommendations and next steps30 \nConsider piloting a data institution aimed at enabling comparative and \nlong-term evaluations30 \nInvestigate the form and scale of a data institution31 \nCovid-19 context32 \nConclusions and recommendations33 \nRecommendations for evaluators33 \nRecommendations for funders34 \nRecommendations for innovators34 \nRecommendations for health and care providers35 \nRecommendations for patient and practitioner groups35 \nAppendices36 \nAppendix 1: Project methodology36 \nAppendix 2: Detailed findings37 \nUse case #1: Detailed findings37 \nWhat an evaluation would aim to assess or demonstrate37 \nData an evaluation would need37 \nChallenges or barriers to accessing necessary data38 \nUse case #2: Detailed findings39 \nWhat an evaluation would aim to assess or demonstrate39 \nData an evaluation would need40 \nChallenges or barriers to accessing necessary data40 \nUse case #3: Detailed findings41 \nWhat an evaluation would aim to assess or demonstrate41 \nData an evaluation would need42 \nChallenges or barriers to accessing necessary data42 \n \n \n \n \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 2 \n\n \n \n \n \n \n \n \nAbout \nThis report has been researched and produced by the Open Data Institute and \npublished in July 2020. It was commissioned by the Health Foundation, an \nindependent charity committed to bringing about better health and health care for \npeople in the UK. The lead authors are Jared Robert Keller, Pauline L’Hénaff and Jeni \nTennison. If you want to share feedback by email or would like to get in touch, \ncontact us at ​research@theodi.org​.  \n \nTo share feedback in the comments, highlight the relevant piece of text and click the \n‘Add a comment’ icon on the right-hand side of the page. \n \nWe would like to thank all organisations that took part in interviews and workshops to \nhelp us put together this document. \n \n \n \n \nHow can it be improved? We welcome suggestions \nfrom the community in the comments. \n \n  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 3 \n\n \nForeword \nThe outbreak of the coronavirus (Covid-19) has amplified and accelerated the need \nfor an effective technology ecosystem that benefits everyone’s health. The pandemic \nhas been accompanied by a marked increase in the use of digital technology, \nincluding introduction of remote consultation in general practice, new data flows to \nsupport the distribution of food and other essentials, and applications to support \ndigital contact tracing. While technology can have huge benefits, these don’t always \noccur. If poorly designed or delivered, technology can drive up costs while adding \nlittle value, distract from other priorities, or worsen social inequalities. \n \nWhile innovation seems to be everywhere, solid information about the impacts of \nthese technologies is much harder to find. Evaluation is essential to help innovators \nlearn and improve over time, to help health care organisations identify technologies \nthat are having a positive benefit and spread those. But robust evaluation does not \nalways happen as the data required to support it is often locked up within \norganisational silos. \n \nThis report explores models of ‘data stewardship’ (the collection, maintenance and \nsharing of data) required to enable better evaluation. It argues everybody involved in \ntechnology has a shared responsibility to enable evaluation, whether that means \ninnovators sharing data for evaluation purposes, or healthcare providers being \nclearer, from the outset, about what data is needed to support effective evaluation. \n \nThis report reenvisages the role of evaluators as data stewards, who could use their \npositions as intermediaries to encourage stakeholders to share data, and help \nincrease access to data for public benefit. \n \nHealthcare systems are seeking to accelerate the adoption of digital and data-driven \ntechnologies. While medicines and drugs are tested within randomised controlled \ntrials, the approach for digital technologies is that they are tested ‘in the wild’ and \nadapted over time in response to learning. The approach is pragmatic, and matches \nthe way that software developers work, but it means there is increasing reliance on \nthe post-market assessment of benefits. \n \nUnfortunately, the system does not always encourage the necessary data sharing to \ntake place – meaning the issues examined in this report are becoming critical. \n \nBy analysing specific use cases, and with the help of a wider range of stakeholders \nand experts in the field, the Open Data Institute has made recommendations for \nimproving data stewardship to enhance evaluation.  \n \nHealthcare providers or innovators cannot tackle this challenge alone, nor can \nevaluators, funders or patients. We call on all parties involved to act on the \nrecommendations in this report, and to work together to create an environment that \nwould allow digital and data-driven technology to reach its potential in benefiting \neveryone’s health. \n \nAdam Steventon  ​Director of Data Analytics, the Health Foundation \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 4 \n\n \nExecutive summary  \nThere is much uncertainty about the impact of recent developments in technology on \nour health, and health and care services. New technologies offer significant potential, \nbut active, and timely, monitoring and evaluation is crucial so that we know what \nimpact they have on people’s health and the health system as a whole, and whether \nit is positive or negative.  \n \nThe Health Foundation and the Open Data Institute have worked together to explore \nhow to improve access to data to support the evaluation of health technologies.  \n \nWe wanted to see how data generated through digital services could be brought \ntogether with health and care data so that interventions can be evaluated in a way \nthat is trustworthy, fair and ethical. \n \nWe explored three specific use cases: digital-first primary care technologies; online \nmisinformation and vaccine hesitancy; and patient flow automation. Challenges were \nidentified across these use cases in terms of data access and we examined the \nsuitability and feasibility of new models of data stewardship to address these \nchallenges.  \n \nKey challenges identified were related to the data needed for evaluation not being \ncollected (sometimes due to a lack of resource), as well as the data not being \naccessible. Access issues can be due to a variety of reasons, such as a lack of \nincentives to share, worries related to sensitive data, and a lack of clarity on what is \npermissible. When accessed, the data can also sometimes not be as useful as \nexpected (this can be due to a lack of quality, consistency or standards).  \n \nWe explored where data institutions could be beneficial, and where they should be \ncoupled with other elements to be put in place such as the need to convene key \nstakeholders and explore data needs; a push for new rules related to procurement; \nand the development and adoption of new standards.  \n \n \nRecommendations \nWe have identified recommendations and next steps for key stakeholders such as \nevaluators, funders, innovators, health and care providers, and patient and practitioner \ngroups. \n \n●Evaluators should:  \n○consider themselves data institutions and look for ways to steward data and \nincrease access to data for public benefit \n○use their position as an intermediary to encourage stakeholders to share data, \nand enable and support them to improve their capability and trustworthiness \n○act as convenors in the sector to create standards for benchmarking \ntechnologies \n \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 5 \n\n \n●Funders should: \n○explore new ways of increasing access to data through scoping and piloting \nnew models of data stewardship such as data institutions.  \n○explore other use cases in the sector such as precision medicine or the \nimpact of social media on mental health \n \n●Innovators should:  \n○get evaluators in the room early to arrange the data collection  \n○be prepared to share data for research and evaluation purposes \n○explore best practices around collecting sensitive data about who uses digital \nservices \n○work together to develop standards for benchmarking and comparison \n \n●Health and care providers should: \n○convene innovators and healthcare practitioners to align understanding \nrelated to data collection for evaluation purposes  \n○build in evaluation from the start when piloting or deploying new health \ntechnologies \n○clarify the ways the data will be collected, accessed, used and shared at the \nprocurement stage and in some cases embed mandatory access to data for \nevaluation purposes in procurement contracts. \n \n●Patient and practitioner groups should: \n○explore cooperative models for collecting data about their experience. \n \n  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 6 \n\n \nIntroduction  \nIn recent years there has been a dramatic increase in the pace of innovation and the \nuse of data and technology in health and care services. The impact of these \ntechnologies on health and care (in terms of delivery, experience and outcomes) is \nsignificant. Effective monitoring and evaluation of such technologies is critical to \nsupport confident innovation at pace. \n \nHowever, there are a number of challenges related to evaluation.  \n \nThe drive to innovate at pace and to avoid delays can mean new technologies are \nadopted before their impacts have been fully established. \n \nThe technologies themselves are largely developed by the private sector and adopted \nby the NHS. This means evaluation needs to combine public and private sector data, \nand there are both technical and governance challenges in brokering access to that \ndata.  \n \nThe evaluation of the impact of these technologies requires access to sensitive \npersonal data. This has to be done in trustworthy and trusted ways. Existing \n1\nframeworks and structures do not always enable this. \n \nThere are also some blind spots in data collection, making evaluation difficult. For \nexample, data about some parts of the system – such as social care – might be \n2\nmissing or be of poor quality. Some data related to outcomes for patients (quality of \nlife, satisfaction) is also not consistently collected.  \n \nThe impact of technologies and other interventions can be different in different \ncommunities, as there are varying risks and levels of adoption due to protected \ncharacteristics such as age and ethnicity, or levels of digital inclusion. The Open \n3\nData Institute (ODI) has previously explored how the protected characteristics of \npeople using a digital service can be collected, to understand how they might be \naffecting excluded communities.  \n4\n \nThe current Covid-19 crisis is also disrupting demands on the healthcare system and \nthe use that is made of some technologies. The pandemic will influence the future \nshape of health and care services in ways we cannot predict. Evaluations need to \n5\ntake account of this changing context. \n \nThe Health Foundation and the ODI have worked together on this project to explore \nhow to improve access to data to support the evaluation of health technologies. We \nwanted to see how data generated through digital services could be brought together \nwith health and care data so that interventions can be evaluated in a way that is \ntrustworthy, fair and ethical. \n \n1\n Open Data Institute (2020), ‘​Designing trustworthy data institutions​’. \n2\n Future Care Capital (2020), ‘​A new joint project on data and analytics about social care​’. \n3\n Public Health England (2020), ‘​COVID-19: review of disparities in risks and outcomes​’. \n4\n Open Data Institute (2020), ‘​Monitoring Equality in Digital Public Services (report)​’. \n5\n The Health Foundation (2020), ‘​Four key questions on COVID-19​’. \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 7 \n\n \nIn particular, we have examined the suitability and feasibility of new models of data \nstewardship. These models have the potential to bring benefits to both the public and \nthe private sector, by facilitating and incentivising data exchange and access, while \nalso benefiting the public, both in terms of better data governance and through the \nadoption of well-evidenced innovations. \n \n  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 8 \n\n \nBackground  \nIn this section we describe the role of evaluation in the health sector, and some of the \nexisting work on new models for data access. \nWhat do we mean by evaluation? \nAn evaluation can be defined as the process of determining the merit, worth \nor value of something. An evaluation of an intervention, such as the use of a health \n6\ntechnology, can be designed to help form or shape the design or implementation of \nthat technology, or address particular questions about its impact. The design of the \nevaluation needs to reflect the nature of the intervention being evaluated and the \nspecific constraints of time, expertise and resources – including people, finance and \ndata – available for the evaluation itself. These and other factors are used by an \nevaluator to determine which evaluation methodology to use, which in turn \ndetermines which questions can be answered and to what level of confidence.  \n \nA framework underpinning many evaluations is the ‘trident’, or three-pronged, \napproach. In this approach, evaluators seek to: measure the outcomes; describe and \n7\nanalyse the process; and sample multiple stakeholder perspectives. This involves \naddressing the questions: did it work, what happened and what did stakeholders \nthink? Measuring the ​outcomes​ of technology in health and care involves assessing \nquality and safety, effectiveness of care and the impact on the efficiency of the \nsystem as a whole. Describing and analysing the ​process​ involves assessing how \nthe technology was or is being implemented, and understanding the context in which \nit is used. Sampling multiple stakeholder ​perspectives​ involves examining the \nexperience of patients and gathering feedback from health and care staff about their \nuse and views of the technology.  \n \nRobust evaluations help identify whether an intervention worked, why and how. This \nallows lessons to be learned, existing interventions to be improved, successful \ninterventions to be spread and scaled, and new ones developed.  \n8\n \nEvaluations of health technologies also help identify if and when a solution might be \nadopted more widely, for example at a national level. They also help the provider to \nadapt the solution being offered, based on recommendations identified.  \n \nEvidence standards are in place to help understand what good levels of evidence for \ndigital healthcare technologies look like, and to ensure new technologies are clinically \neffective and offer economic value. These standards look at ways to assess aspects \nsuch as credibility, effectiveness, equality and acceptability.  \n9\n \n6\n The Health Foundation (2015), ‘​Evaluation: What to consider​’.  \n7\n Ellis R, Hogard E (2006), ‘​The Trident: A Three-Pronged Method for Evaluating Clinical, Social and \nEducational Innovations​’.  \n8\n Ibid. \n9\n National Institute for Health and Care Excellence, ‘​Evidence standards framework for digital health’. \ntechnologies \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 9 \n\n \nA code of conduct for data-driven health and care technologies has also been \ndeveloped by NHSX, to proactively and objectively evaluate current technologies, \nand ensure best practices are developed and implemented in an evidence-based \nmanner.  \n10\n \nThe Health Foundation has identified some key challenges for evaluators of health \ntechnologies: \n11\n \n●Are some patients more likely to be excluded because of their complex \nneeds or experience using digital services? \n●As technologies evolve through self-learning, how often do they need to be \nevaluated, and how reliable can the findings be over time? \n●How can we ensure that the understanding of how data is collected and used \nis common across all evaluators, and build this capability? \n●How can we ensure the evaluation considers the context in which \ntechnologies are introduced, so that we can understand what might be \ncontributing to their impact? \n \nThere is also a key challenge – common to all evaluations – around collecting the \nright data and making it accessible to evaluators. Evaluations can be thought of late \nin the process, which means delays in identifying the questions to be asked and the \ndata necessary to answer them. Evaluators may not be able to access the data they \nneed to perform a robust evaluation, particularly within the time constraints of an \nevaluation. \nWhat do we mean by new models of data \nstewardship? \nStewarding data involves collecting, maintaining and sharing it, and as part of this, \nmaking decisions about who has access to it, for what purpose and to whose benefit. \nWhen data is stewarded responsibly, data is available to those who need it in ways \nthat are trustworthy and sustainable.  \n \nAs part of its work to build an open, trustworthy data ecosystem, the ODI has been \nexploring different approaches to stewarding data. Other organisations, such as ​The \nGovLab​, the ​Aapti Institute​ and ​Nesta​, and networks such as those run by ​MyData \nGlobal​, ​Mozilla​ and the ​Centre for International Governance Innovation​, are also \nexploring this topic.  \n \nAt the ODI, we think data institutions have an important role to play in the \nstewardship of data. Data institutions are organisations whose purpose involves \nstewarding data on behalf of others, often towards public, educational or charitable \naims. The ODI’s initial work has found that data institutions play a number of vital \nroles in different sectors and contexts, including: \n \n●holding data on behalf of an organisation or person, or group of them, and \nsharing it with others who want to use it for a particular purpose \n●combining or linking data from different sources, and providing insights and \nother services back to those that have contributed data \n10\n Morley J, Joshi I (2019), ‘​Developing effective policy to support artificial intelligence in health and care​’.  \n11\n The Health Foundation (2019), ‘​Evaluating digital first primary care – the challenges ahead​’.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 10 \n\n \n●maintaining common data infrastructure for a sector or field, such as by \nregistering identifiers or publishing open standards. \n \nThere are many existing data institutions, including in the health sector. The UK \nBiobank is one example. It was set up in 2006 to steward genetic data and samples \nfrom around half a million people, and continues to support their use for health \nresearch. The data being stewarded by the Biobank is available to health scientists \nfrom academia and industry for research purposes, via an application process. The \nobjective is to improve the prevention, diagnosis and treatment of a wide range of \nserious and life-threatening illnesses.  \n12\n \nThere are many different models for facilitating access to data for public benefit. In \nthe context of this report, the need to share potentially sensitive data from both the \npublic and private sectors, in a trustworthy way, towards the specific purpose of \nevaluating the impact of health technologies, meets many of the criteria for a \nparticular form of data institution known as a ‘data trust’. However, appropriate data \n13\nstewardship and access models are context sensitive. Their design is dependent on \nthe details of a given data ecosystem and the motivations and capabilities of different \nstakeholders. \n \nAdopting new models for data stewardship does not always mean creating a new \ndata institution. Often, it may be more appropriate for an existing organisation to take \non a new role or to recognise its status as a data institution. In this report, we \nconsider both gaps where new data institutions might provide trustworthy access to \ndata, and places where existing organisations could or should adopt this role. \n  \n12\n UK Biobank, ‘​About UK Biobank​’.  \n13\n Open Data Institute (2019), ‘​Data trusts: lessons from three pilots (report)​’.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 11 \n\n \nThree use cases  \nThis report focuses on three use cases related to the evaluation of emerging \ntechnologies and their impact on health and care in the UK. By focusing on specific \nuse cases and technologies, we have been able to take an in-depth look at the data \necosystems that surround these new technologies, their potential value, the relevant \nstakeholders involved, and the challenges and barriers to accessing and sharing data \nfor evaluation purposes. In particular, we have been able to identify where within \nthese three use cases there is potential for new forms of data stewardship to help \nremove barriers to data access and sharing.  \n \nThe insights and lessons we have drawn from this research are applicable beyond \nthese specific technologies and contexts. This report therefore offers \nrecommendations to evaluators and other organisations working with these specific \ntechnologies, alongside overarching guidance and lessons for organisations working \non emerging technologies in the health and care sector more broadly.  \n \nWe chose three complementary use cases:  \n \n●Assessing the safety, effectiveness and efficiency of ​digital-first primary \ncare​ services designed to advise patients about their symptoms and direct \nthem to other, non-digital primary care services. \n●Understanding the spread of ​information and misinformation​ online, and \nthe impact of misinformation on ​vaccine hesitancy​ and public health. \n●Evaluating ​patient flow automation​ systems which are designed to improve \nclinical pathways and operational efficiency within hospitals and across \nregions.  \n \n \nThe use cases have contrasting and complementary characteristics, as summarised \nin the table below. \n \n  Digital-first primary \ncare \nMisinformation and \nvaccine hesitancy \nPatient flow \nautomation \nLocation within \nhealth sector/society \nFirst point of contact \nwith the health sector \nOutside the health \nsector but impacting \nhealth \nEmbedded within the \nhealth sector \nLevel and manner of \nengagement with \npeople \nPatient-facing for \nspecific purposes  \nFrequent contact with \npeople but outside the \nhealth sector \nPrimarily ‘behind the \nscenes’ \nExtent of adoption A handful of \nimplementations \nLarge-scale adoption Dozens of \nimplementations  \nType of technology Specific technology \nused for particular \npurposes when \nSuite of technologies \nused for a range of \npurposes \nSpecific technology \nused regularly \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 12 \n\n \nneeded \nPrimary level/ scale \nof impact \nImpact on individual \nusers as well as care \nproviders \nImpact on society at \nlarge \nImpact on hospitals or \nregions \n \n \n \nRelevance for Covid-19 \nThe three use cases were selected in early 2020, before social distancing \nmeasures were put in place to prevent the spread of Covid-19, but each \ntechnology has played an important role in the health sector’s response to the \nCovid-19 pandemic.  \n \n●Digital-first primary care​: In the weeks after social distancing measures \nwere put in place, virtual GP consultations markedly increased. The need \n14\nto facilitate continued access to health services has highlighted the value of \nvirtual consultations, as well as being able to triage the types of people and \nsymptoms who should be seen in person. \n●Misinformation and vaccine hesitancy​: Much misinformation has spread \nonline during the pandemic, highlighting the importance of being able to \n15\ndirect people toward helpful, authoritative sources of health information and \naway from potentially harmful misinformation. This is likely to take on even \ngreater importance if/when a Covid-19 vaccine is produced.  \n●Patient flow automation​: During the current pandemic, hospitals and the \nhealth systems have been stretched thinly. The need to use available \nresources effectively and efficiently has highlighted the value of having a \nreal-time view of resources and capacity across a system, and the ability to \nmodel and anticipate peaks of activity and potential bottlenecks. \n \nWe have been able to observe the impact of the pandemic on such technologies \nand on efforts to share and access data more broadly. New temporary \ndata-sharing procedures and agreements have been put in place at a rate that \nwould have been difficult in normal circumstances. For each use case, we draw \nout important lessons learned from the pandemic and provide recommendations \nand next steps related to efforts to battle Covid-19. \n \n \nThe following three sections of this report examine these use cases in more detail, \nbased on our desk research, interviews and workshops with experts and \nstakeholders. We provide background on the specific technology, the context and \nthe results of our research. Specifically, we outline: \n \n●the types of things an evaluator would look to assess or demonstrate through \nan evaluation of that technology  \n14\n Financial Times (2020), ‘​Lockdown drives boom in healthcare apps​’.  \n15\n BBC (2020), ‘​Coronavirus: Fake news crackdown by UK government​’​.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 13 \n\n \n●the types of data that an evaluator would need access to in order to perform \nthat evaluation \n●the challenges or barriers that currently exist to accessing those types of \ndata.  \n \nAt the end of each section we outline recommendations and next steps, including \nindicating where some of the challenges and barriers identified can be addressed by \na data institution. Where appropriate, we also note where challenges and barriers can \nbe addressed through other interventions, including non-institutional forms of data \nstewardship.  \n \nMany of our findings and recommendations apply more broadly to multiple use cases \nor to the health sector in general. We outline these overarching findings and \nrecommendations after discussing each of the three use cases.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 14 \n\n \nUse case #1: Digital-first \nprimary care \nUse case description and background \nDigital-first primary care (DFPC) refers to models of general practice where patients \nuse smartphone and desktop applications to book and conduct consultations \nremotely. These digital and online tools make it easier for some patients to access \n16\nthe advice, support and treatment they need.  \n \nThere are a number of approved DFPC providers, offering different models, including \ntriage only, triage with consultation (either video or online) and full digital primary care.\n Examples of such services include ​Babylon GP at hand​, ​eConsult​ and ​Ada Health​.  \n17\n \nThe NHS Long Term Plan commits that every patient will have the right to DFPC by \n18\n2023/24, with online consultations by April 2020 and video consultations by April 2021. \n19\n \nHowever, there have been relatively few formal evaluations of the impact of DFPC in \ngeneral practice – on patients, the workforce and the wider health and care system. \nFor example, it is currently difficult to assess whether the decisions made when using \na DFPC service are causing added strain on a particular section of the health and \ncare system, or whether a DFPC provider is creating a digital divide. \n20\nFindings \nThis section contains a summary of our findings. For further details see the ​research \nfindings appendix​. \nWhat an evaluation would aim to assess or demonstrate \nBased on our research, an evaluation of DFPC technologies would pursue a \nthree-pronged evaluation, similar to many other evaluations of emerging health \ntechnologies.  \n \nAs outlined in the ​Background section​, within such an approach, evaluators would \nseek to measure the outcome of the introduction of a DFPC technology (what are the \npatients’ experiences and pathways, and their views on the quality of service?); \ndescribe and analyse the process by which the technology was introduced (does the \nservice create inequalities in terms of patients access?); and sample multiple \n16\n The King’s Fund (2019), ‘​Digital-first primary care: helpful disruptor or unnecessary disruption?​’. \n17\n NHS England, ‘​Digital First Primary Care​’.  \n18\n NHS England (2019), ‘​NHS Long Term Plan​’.  \n19\n NHS England, ‘​Digital First Primary Care​’.  \n20\n Ibid. \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 15 \n\n \nstakeholder perspectives on the use and value of the technology (what are the \npractitioners’ experiences using it? Does the service provide value to the healthcare \nsystem and make it more efficient?).  \nData an evaluation would need \nIn order to assess or demonstrate the impact of a DFPC technology, evaluators \nwould need to be able to access and link a wide range of data:  \n \n●Data from general practitioners (GPs) and other health and care staff related \nto their experience of using DFPC technologies. \n●Data about patients (including demographic data) and how they use and \ninteract with DFPC technologies, how they are directed through the health \nsystem after they have used it, and what their experience of using the \ntechnology was.  \n●Operational data drawn from commercial providers as well as from health \nproviders. This data would include aggregated usage data related to how and \nwhen the tool was used, NHS system-level data about waiting times and GP \nworkflows, and data about any decision-making algorithms used within those \ntechnologies and any data used to train those algorithms.  \nChallenges or barriers to accessing necessary data \nAccording to the stakeholders and innovators we interviewed within this area, \nit can often prove difficult to access necessary data..  \n \nSometimes, the data needed to perform an evaluation is simply not collected. This \ncan be due to a lack of resources (time and capacity), high data-collection costs, or \nbecause the need to collect a specific type of data was not identified or \ncommunicated from the start.  \n \nOther times, the data exists or has been collected, but is not accessible to the extent \ndesired by evaluators; whether due to concerns about commercial sensitivity, or user \nor patient privacy. This barrier to accessing data appeared frequently in our research \non DFPC technologies. \n \nIn some cases, the necessary data has been collected and is accessible, but is not as \nuseful as desired. According to our research, this is often because the data is not \ngranular or real-time enough, is of poor quality or is anonymised to the point that it is \nless useful for the purposes of evaluations. At times, the necessary data lacks \nconsistency or shared standards, making it difficult to link disparate datasets or \nperform analyses across aggregated datasets.  \nRecommendations and next steps \nThe two main challenges and barriers identified within this use case are gaining \naccess to data held by commercial innovators and technology providers; and \naccessing necessary data from across the NHS and the health sector. Here, we \ndiscuss these barriers, offer recommendations for addressing them, and then outline \nsuggested next steps.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 16 \n\n \nEmbed mandatory access to data for evaluation purposes in \nprocurement contracts \nThe people and organisations we spoke to identified several specific commercial \ndatasets and types of data that are currently difficult to access, including data about \npeople’s use of DFPC tools and services; the algorithms and models used to make \ndecisions about recommended treatments or where to direct patients; and data used \nto train algorithms and models, if applicable. \n \nThe main barriers to accessing commercial data are a potential lack of trust between \ncommercial providers and evaluators (though not in all cases); and a perceived \ninability to incentivise commercial organisations to share potentially sensitive data, \nbecause it contains either personally identifiable or commercially sensitive \ninformation. \n \nBuilding or empowering a data institution would help to address some of the \ncurrent barriers to accessing commercial data, but would not be sufficient on its \nown​. It would be possible to build or install a data institution to sit between \nevaluators and commercial technology providers to balance competing interests, act \nas a trusted third party and ensure the security of personally and commercially \nsensitive data. But a data institution on its own would not be able to address the \ninability to incentivise or compel commercial organisations to share data with \nevaluators. For this, another approach is necessary.  \n \nIn this use case, the data holders – ie the innovators and DFPC providers – want \naccess to the NHS market and the potentially lucrative contracts that may come from \nhaving their technology or solution adopted by healthcare providers. Having their \ntechnologies evaluated is a crucial part of that process. Therefore, evaluators and \nNHS bodies do have a degree of leverage. Drawing on that leverage, it should be \npossible to include specific requirements in contracts with DFPC providers that \noblige them to share data with evaluators for the purpose of evaluation. Evaluators \nwill need to work with NHS organisations to draw up terms that are amenable to all \nparties and to ensure that these obligations are added to contracts early in the \nprocess of implementing, piloting and procuring new technologies.  \n \nSuch an approach would enable evaluators to gain access to data currently held by \ninnovators and commercial technology providers. Any data institution is unlikely to \nsucceed without it.  \nEnable institutional efforts to improve access to NHS data to \nsupport technology evaluation \nAnother challenge we identified in this use case (and others) is that ​it is difficult to \ngain access to necessary data from across the NHS and the health sector more \nbroadly.​ In some cases, this data is not collected; in others, the data is collected but \nis not available in the desired form or is not linkable with other sources. The people \nand organisations we spoke to identified various reasons for this, including but not \nlimited to: \n \n●a lack of sufficient technical infrastructure \n●a lack of consistent and suitable standards \n●poor data quality \n●concerns about patient privacy. \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 17 \n\n \n \nThere is a role for data institutions to play in increasing access to data across the \nNHS and the health sector, and there are initiatives already underway that aim to \naddress many of these challenges and barriers. \n \nFor instance, Health Data Research UK (HDRUK) is currently working to unite, \nimprove and use data from across the health sector for research purposes through \nthe ​UK Health Data Research Alliance​ and the ​Health Data Research Innovation \nGateway​, and to make health data more accessible and useful through the \nestablishment of a series of ​Health Data Research Hubs​. HDRUK is also working to \n21\nensure that health data is put to good use through funding regional and national \ninnovation programmes with organisations such as the National Institute for Health \nand Care Excellence (NICE) and the Health Foundation.  \n22\n \nNHS Digital, which is responsible for standardising, collecting and publishing data \nfrom across the health sector in England, is also conducting work that seeks to \naddress many of the challenges and barriers identified. Its work aims to develop \n23\nnational and international standards for health data, improve the quality of health \ndata, and increase the accessibility of health data. \n24\n \nThese two organisations are not focused specifically on bringing together data for the \npurposes of evaluations of digital health technologies, but they are working to \naddress many of the same barriers to access that currently confront evaluators. \nOngoing initiatives by these and other organisations within the health sector should \nengage with evaluators to ensure that they can get timely access to data for \nevaluation purposes. For instance, in an interview with HDRUK, we discussed ways \nthat one of the research hubs, Discover Now, may be able to facilitate access to data \nfor the purposes of evaluations.  \nEngage with existing initiatives and consider piloting a data \ninstitution to facilitate access to NHS data for the purpose of \nevaluations \nIt is currently unclear whether existing initiatives to increase access to health sector \ndata, such as those outlined above, will meet the needs of evaluators, although it \nshould still be possible to learn from them.  \n \nIf these central initiatives do not meet the needs of evaluators – for instance, it may \nbe that these approaches will not be agile enough to meet the needs of evaluations \nthat need to move at pace – then evaluators should ​explore the potential of taking \non the role of a data institution to facilitate access to data in a way that does \nmeet their needs​. For example, this institution could facilitate access to data held by \nindividual NHS Trusts or clinical commissioning groups rather than going via the \nmore national bodies. We describe what such a data institution might look like and \nprovide potential next steps for evaluators in use case #3 and in the conclusions. \n \n  \n21\n Two relevant examples are the ‘​Health Data Research Hub for Real World Evidence​’ and the ‘​Health Data \nResearch Hub for Clinical Trials​’. \n22\n Health Data Research UK, ‘​How we use health data​’.  \n23\n NHS Digital, ‘​Data and information​’. \n24\n NHS Digital, ‘​Data, insights and statistics​’.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 18 \n\n \nExplore the potential for existing organisations to take on new \ndata stewardship roles \nWe believe there are several important data stewardship roles that existing \norganisations working in this space should and could perform. Here we draw a \ndistinction between ​creating​ new institutions to perform specific roles and \nempowering existing organisations​ to take on new roles or to recognise their status \nas a data institution. Performing these roles well will help increase access to health \ndata for the purpose of evaluating DFPC technologies, while also increasing access \nto health data more broadly.  \n \nThrough our use case workshops and interviews with organisations such as HDRUK, \nthe National Data Guardian and NHSX, we identified areas where further work is \nneeded. An organisation wishing to facilitate access to data for evaluation purposes \nwithin this space could action the following points: \n \n●Push for the development and adoption of new procurement terms and \nconditions for services that include the requirement to provide access to data \nand other materials for the purpose of evaluating the performance of the \nservice. \n●Push for the development and adoption of new standards, certifications or \naccreditations of organisations involved in the collection, processing, sharing \nand use of health data. This can help organisations build and demonstrate \ntheir own trustworthiness, while helping them assess the trustworthiness of \nothers.  \n25\n●Push for new policies or regulations where necessary, for instance, policies \nthat address challenges related to the legal basis for accessing patient data. \n●Help provide clarity for organisations around what is and isn’t permissible in \nterms of the collection, use, linking and analysis of potentially sensitive \ndatasets. \n●Develop a framework to enable consent-based access to data across \ncommercial and NHS organisations for evaluation purposes. Some \nevaluations will require access to many different types of data from many \ndifferent sources, but some can be performed with less data from fewer \nsources. In these cases, a consent-based approach may be workable and \nmanaging those consents could be a role taken on by an evaluator acting as \na data institution. \n●Engage with commercial organisations to advocate the benefits of sharing or \nopening data for the purposes of evaluations. Though we encountered some \nresistance from commercial organisations to sharing certain types of \nsensitive data, it may still be possible in some circumstances to persuade \ncommercial providers to increase access to data they hold, especially if the \npublic health and societal benefits are made apparent.  \nAdvocate for technologies to be implemented in a more \nevidence-based way, with early input from evaluators \nThere should also be advocacy for emerging health technologies to be implemented \nin a more evidence-based way. This advocacy could be led by an evaluator or by an \nNHS body, but regardless, this type of initiative would likely require contributions and \ncooperation from numerous organisations inside and outside the health sector.  \n25\n Open Data Institute (2020), ‘​Designing trustworthy data institutions​’. \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 19 \n\n \n \nApproaching the implementation of emerging health technologies in a more empirical \nmanner would enable evaluations to be conducted in a more rigorous, controlled, \nscientific way than is currently possible. Evaluators are often consulted or contracted \nafter​ a new technology has already been rolled out, meaning it is difficult – sometimes \nimpossible – to collect the types of data necessary or control for important variables. \nIncluding evaluators in discussions early on in the roll out of a new technology would \nmake it possible to identify important things to evaluate, important data to collect, \nand potential challenges or barriers to collecting that data. This would also enable \nevaluators to communicate to health personnel the necessity and value of collecting \nthat data, which should increase buy-in. \n \nBy working with the NHS to implement emerging health technologies in this way, \nevaluators can help the NHS to continue to innovate at pace, while making it easier to \nevaluate the safety, efficiency and effectiveness of these technologies.  \n \n \nCovid-19 context \nThe uptake and roll out of digital-first primary care solutions has increased \nmassively due to Covid-19. The pandemic has resulted in a move towards \ndelivering a ‘total triage’ model of care through online, video and telephone \nconsultations. In May 2020, online consultations were available in 85% of general \npractices, covering 86% of the population; and video consultations were available \nin 99% of practices, covering 99% of the population (while in 2019 less than one \n26\nin every 100 of all GP appointments was carried out by online video consultation).\n \n27\n \nSome questions, however, still remain in terms of: \n  \n●understanding which parts of health care services are still better addressed \nin person and which can best be conducted digitally.  \n●understanding the impact these technologies can have on some groups that \nare excluded (creating a digital divide). Some technologies were identified \nas being used mostly by younger and healthier people, while older people \nwith more complex health needs were less frequent users.  \n28\n \nEvaluating these issues would be necessary before adopting such technologies \nfurther and fully transitioning to digital care in a post-Covid-19 context.  \n  \n26\n Bakhai M, NHSX (2020), ‘​The use of online and video consultations during the COVID-19 pandemic - \ndelivering the best care to patients​’. \n27\n ​The guardian (2020), ‘​GPs told to switch to digital consultations to combat Covid-19​’  \n28\n Ipsos Mori (2019), ‘​Evaluation of Babylon GP at hand​’. \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 20 \n\n \nUse case #2: Online \nmisinformation and \nvaccine hesitancy \nUse case description and background \nThere are growing concerns about the impact of the online spread of information and \nmisinformation on many areas of life, several of which might impact health. For this \nuse case, we looked at the impact of online information and misinformation on \nvaccine hesitancy.  \n \nOnline information includes that found on social media websites and applications \nwhere users can create or share content. It also includes other sources of information \nonline. For this use case, we also considered messaging applications (that is \nmisinformation might be flowing quickly through messaging applications rather than \non platforms). The information may or may not be accurate, and inaccuracies may be \nintentional or unintentional.  \n \nHesitancy in relation to vaccination may affect people’s motivation to be vaccinated, \ncausing people to reject it for themselves or their children. This hesitancy can be \n29\ncaused by a number of factors, including complacency about the need for \nvaccinations, difficulties with access to and lack of confidence in vaccinations. The \nWorld Health Organization identified vaccine hesitancy as one of the top 10 global \nhealth threats of 2019.  \n30\n \nMisinformation around vaccinations is a long-standing problem, however social \nmedia presents unprecedented risks around the amplification and spread of \nanti-vaccination messages. \n31\nFindings \nThis section contains a summary of our findings. For further details see the ​research \nfindings appendix​. \nWhat an evaluation would aim to assess or demonstrate \nEvaluating the impact of online information and misinformation on vaccine hesitancy \nis a difficult and complex task. This is in large part due to the diffused but widely \n29\n World Health Organization (2019), ‘​Improving vaccination demand and addressing hesitancy​’.  \n30\n World Health Organization (2019), ‘​Ten threats to global health in 2019​’.  \n31\n Burki T (2019), ‘​Vaccine misinformation and social media​’.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 21 \n\n \nadopted nature of the technology being evaluated. Whereas the previous use case \nfocused on a fairly discrete type of technology in the early stages of adoption, this \nuse case focuses on a suite of related but different technologies that are widely used \nand deeply embedded within many aspects of people’s lives. Because of this, the \nthree-pronged approach to evaluations used in the previous use case would be \ninsufficient.  \n \nTo assess and demonstrate the impact of online information and misinformation on \nvaccine hesitancy, evaluators would need to, among other things: identify and \nmeasure the types of misinformation about vaccines circulating online; understand \nthe motivations and methods of people propagating misinformation about vaccines; \nunderstand the role that online platforms play in either limiting or circulating \nmisinformation; track the types of misinformation about vaccines people have been \nexposed to; understand what factors influence a person’s susceptibility to \nmisinformation; understand the influence of offline sources of information in shaping \na person’s views of vaccines; and ultimately be able to measure the extent to which \nexposure to online content influences people’s actions and behaviours. This last step \nis crucial to proving causality rather than correlation.  \nData an evaluation would need \nBecause the answers sought by evaluators are complex and multi-faceted, a \ncomprehensive evaluation will require access to many different types of data from \nmany different sources.  \n \nEvaluators would need data from sources within the health sector related to visits to \nGP practices and vaccine clinics, data about messaging and communication about \nvaccines, and data about the overall rates of vaccine uptake across the country. \nEvaluators would also need contextual data about people’s demographics, \nsocioeconomic backgrounds, beliefs and offline activities.  \n \nCrucially, evaluators would need data from online platforms and services, including \ndata about the types of vaccine information and misinformation available online, and \ndata about how people view, share and comment on misinformation. Evaluators \nwould also need data related to any algorithms trained to prioritise and circulate \ncontent online.  \nChallenges or barriers to accessing necessary data \nBased on our discussions with stakeholders and innovators within this area, there are \ncommon challenges and barriers to accessing the data needed to assess the impact \nof online misinformation on vaccine hesitancy.  \n \nAs with the previous use case, sometimes the necessary data is not collected due to \na lack of resources, concerns about privacy or because the data was not identified as \nimportant for evaluations and was therefore not collected. There is also often a lack \nof clarity about whether collecting some types of data is legally permissible.  \n \nWhen data does exist, it can be difficult to access – especially data that is held by \nonline platforms and commercial tech companies. Often, these companies are \nunwilling to share data due to concerns about commercial sensitivity and intellectual \nproperty, protecting the privacy of their users, and the ethics of sharing data about \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 22 \n\n \nusers. Some of our interviewees noted that because the data is of a personally or \ncommercially sensitive nature, it is very difficult to find a way of incentivising \norganisations to share that data.  \n \nEven when it is possible to access data from online sources, a lack of consistency or \nstandardisation across platforms for flagging, labelling, reporting and removing \nmisinformation tends to make it difficult to perform cross-platform analyses. \nRecommendations and next steps \nNot surprisingly, we identified many of the same challenges in this use case as in the \nprevious use case around digital-first primary care. In some cases, the particulars of \nthis use case have led us to outline different recommendations or add nuances to the \nrecommendations outlined in use case #1. In others, our recommendations remain \nlargely the same, in which case we refer back to the previous use case rather than \nrepeating our recommendations here.  \n \nThe challenge of accessing and linking data from across the NHS and the health \nsector came up often within this use case. For more on our recommendations for \naddressing this challenge, see ‘​Use case #1: Recommendations​’. \nExplore the use of data portability to collect contextual data from \noffline sources \nAn important challenge in this use case is that ​it is difficult to gain access to \ncontextual data and data about people. ​A few of our interviewees and workshop \nattendees noted the potential for evaluators and health organisations to conduct \nsurveys to collect demographic details, data about people’s activities offline and \nattitudes toward vaccines. This is a worthwhile avenue to pursue and sits firmly within \nthe standard research methodologies of social scientists and public health \nresearchers.  \n \nAnother potential avenue would be to develop ways for people to participate in ‘data \naltruism’: leverage their right to data portability to provide evaluators with access to \nimportant types of data related to misinformation and vaccine hesitancy. Data \nportability allows individuals to obtain and reuse personal data about them for their \nown purposes across different services (they can move, copy or transfer personal \ndata easily from one IT environment to another in a safe and secure way, without \naffecting its usability). People could port relevant records from health systems, local \n32\nauthorities, social services or schools to provide evaluators with demographic details \nor details about a person’s interactions with various offline services. This type of \ninformation would help evaluators gain a clearer understanding of the wide range of \ndifferent factors that influence vaccine hesitancy and might even help health and care \nofficials target interventions.  \n \nData portability could also be used to gain access to data held by online sources, but \nthere are additional potential limitations in this case. For example, there are limits to \nthe type of data a person can request from an online platform. In particular, a user \nwould only be able to receive data that they themselves created or contributed to the \n32\n Information Commissioner’s Office, ‘​Right to data portability​’. \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 23 \n\n \nservice, for example, data about the posts ​they​ have made, but would not be able to \nreceive data about all the posts they had seen, or about the advertising directed at \nthem. Nor would a user be able to receive access to algorithmic decision-making \nrelated to ​why​ certain posts or adverts were directed at them. \n \nA data portability initiative, whether aimed at gathering data from offline or online \nsources, would probably rely on people volunteering to take part. Whether it would \nbe hindered by the self-selected nature of the cohort who sign up would need to be \nexplored. It is unclear, for instance, whether the people most likely to reject vaccine \nadvice would be the people signing up for this type of data portability scheme. This \ntouches on a wider point about ‘crowdsourced’ or ‘patient-led’ initiatives: though \nthey have proved useful as a means of increasing access to data for a range of \nresearch purposes, there are questions as to how reliable, representative and timely \nthey can be expected to be and therefore whether evaluators should make them a \ncore part of their evaluations. \n \nHowever, as Gary King of ​Social Science One​ has said, researchers are adept at \nusing facts we know to learn about facts we do not know. So, despite the potential \nlimitations of such an approach, we recommend exploring the potential of data \nportability further. In a use case such as this – as in many others – every little bit of \ndata is likely to help.  \nExplore the use of plugins and data portability to collect data from \nonline sources \nAnother challenge is that ​it is difficult to gain access to data from online sources, \nin particular from online platforms and social media companies.​ There is a \nperceived inability to incentivise or compel social media companies to provide \naccess to this data, as well as concerns over commercially sensitive data and user \nprivacy. As such, the challenges are similar to the challenges identified in the first use \ncase, but with an important differentiating factor. In use case #1, evaluators and the \nNHS had leverage in the form of access to the NHS market which they could use to \nincentivise digital-first primary care providers to share necessary data. In this second \nuse case, since online platforms exist outside the health sector and do not rely on \nNHS contracts, evaluators do not have the same degree of leverage or ability to \nincentivise cooperation. Therefore, whereas in use case #1 we recommended \npursuing contractual access to data, in this use case we recommend exploring a few \ndifferent approaches.  \n \nOne approach would be to explore data portability in order to access data held by \nonline platforms, but there are limitations to this approach, as outlined above.  \n \nA related approach would be to explore the potential of browser plugins to gather \ninformation about people’s interactions with online services. For instance, a ​browser \nplugin developed by ProPublica​ enabled people to “see exactly how Facebook users \nare being targeted by advertisers”. These types of tools would not give evaluators a \n33\nview into algorithmic decision-making, but they would enable them to track and \ncatalogue adverts and see which groups advertisers are targeting with which \nadvertisements. The legality of these tools is currently contested, however, so \norganisations should explore these with caution. For instance, Facebook “urged” \nProPublica to shut down their plugin. \n33\n ProPublica (2019), ‘​Facebook Moves to Block Ad Transparency Tools - Including Ours​’. \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 24 \n\n \n \nIn both these cases, the data collected could be held by a data institution that would \nthen provide access to that data to a range of researchers and evaluators. \nConsider piloting a data institution aimed at facilitating access to \ndata held by online platforms \nGiven the necessity of accessing data held by online platforms and the limitations of \nthe two approaches outlined above, we think that ​a data institution could play an \nimportant role in facilitating access to data held by online platforms​. \n \nOur research into similar data institutions suggests that a data institution aimed at \nfacilitating access to data held by online platforms would be able to incentivise these \nplatforms, while addressing concerns about commercial sensitivity and user privacy.  \n \nIn terms of incentives, collaborating with a data institution set up to study the impact \nof misinformation on vaccine hesitancy would provide online platforms with:  \n \n●An opportunity to share data for public benefit​. Many online platforms \nalready share data for such purposes, so an initiative aimed at helping to \nevaluate the impact of online misinformation on vaccine hesitancy and public \nhealth would fit firmly within those existing initiatives and motivations. \n●A chance to garner good public relations​. Helping to answer such an \nimportant question would give online platforms a chance to show their \ncommitment to promoting public health, while also potentially countering any \ncriticism of the way that misinformation spreads on their platforms.  \n●An opportunity to forestall regulation​. Some data institutions and data \naccess initiatives like Uber Movement have arguably been set up as a way of \nheading off regulated access to commercial data that might have been on \nterms less generous to the commercial data holder. \n \nAn independent data institution would be better placed to address concerns about \nuser privacy and commercial sensitivity than other actors. By sitting between private, \npublic and third sector organisations, data institutions can serve as trusted third \nparties and help to increase access to data for research purposes, while ensuring \nthat nothing commercially or personally sensitive is accessed by the wrong parties. \nBy utilising tools and approaches like ​differential privacy​ or secure research \nenvironments, a data institution could help commercial data holders – and the people \nthat use these platforms – have confidence while sharing data. \n \nThis data institution could take a form similar to ​Social Science One​, the \nindustry-academic partnership set up to explore “the effect of social media on \ndemocracy and elections”. Or it could resemble other data-sharing initiatives \ninvolving major data-holding companies such as Facebook’s ​Ad Library​, \nSharedStreets​ or ​Uber Movement​.  \n \nBefore the exact structure and focus of this potential data institution can be decided \non, however, a number of questions about the remit and breadth of the institution will \nneed to be answered. Most of these questions are a consequence of the fact that the \nanswers sought by evaluators in this use case are quite complex (proving causality \nversus correlation is a notoriously difficult task) and will require access to many \ndifferent types of data from many different sources. These open questions include:  \n \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 25 \n\n \n●Should this data institution focus on a single health topic like vaccine \nhesitancy or on public health more generally?​ Social Science One, as a \ncomparison, is aimed broadly at questions of democracy and elections, but \nthere may also be value in starting with a smaller focus and expanding from \nthere.  \n●Should this data institution start by aiming to facilitate access to one \nonline platform or to many? ​It is unclear whether online platforms would be \nwilling – or able – to cooperate with each other on such an endeavour.  \n●Should this data institution exclusively focus on facilitating access to \ndata from online sources?​ Or should it set its sights on facilitating access to \ndata from all the different sources necessary to perform an evaluation of this \ncase, that is data from the health sector, contextual data, data from people, \nacademic research data, and so on.  \nEngage with stakeholders to identify their views and needs \nWe recommend conducting a scoping process to confirm the need for a data \ninstitution and explore what such an institution would look like. Our report, ‘​Data \ntrusts: lessons from three pilots​’, has resources that can help evaluators in this effort.\n In particular, our recommendation for evaluators is continued research and \n34\nstakeholder engagement on three fronts: \n \n●Convene a range of online platforms and companies to discuss their \nviews in this area.  \n○What are their views on data sharing and cross-industry \ncooperation?  \n○What type of data institution would be agreeable and what types of \nincentives could convince them to take part?  \n \n●Engage with researchers – be they social scientists, evaluators or \nepidemiologists – to discuss their needs around this use case in more \ndepth.  \n○What types of questions do they want to answer and what types of \ndata would they need in order to do so? \n○In what form would they need this data? Would differential privacy or \na secure research environment suffice? \n○What types of questions would be answerable if the data institution \nonly facilitated access to data from online platforms? What if all the \nnecessary types of data were made available? \n \n●Engage with similar initiatives exploring similar terrain.  \n○For instance, research into ​online harms​ or calls for regulated access \nto social media data in order to enable research into the impact of \nsocial media on mental health​.  \nRethink the scope of evaluations on this topic \nA final recommendation is for evaluators and public health researchers to ​focus on \nusing available data to answer smaller aspects of the larger question, ​rather than \naiming for a single source of all possible relevant data. Answering the ultimate \nquestion of the impact of misinformation on vaccine hesitancy will require access to \n34\n Open Data Institute (2019), ‘​Data trusts: lessons from three pilots​’.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 26 \n\n \nmany different types of data that are currently not available, but in our workshops and \ninterviews we discussed the potential of using the few bits of relevant data that are \ncurrently available to begin answering smaller questions related to vaccine hesitancy \nand misinformation, as is the standard scientific process. Once researchers have \nanswered a number of smaller questions in this area, the answers can be pieced \ntogether to provide a view of the larger question.  \n \nThese individual studies are useful not only in shining a light on the research \nquestions themselves, but also in highlighting the utility of certain datasets and \ntherefore prioritising investment. By examining existing and recent research, \nevaluators should be able to identify which datasets from which sources have proved \nthe most useful in order to prioritise investment and effort aimed at increasing access \nto those datasets.  \n \n \nCovid-19 context \nIn the context of the Covid-19 pandemic, vaccine hesitancy remains an important \nissue, both in terms of maintaining scheduled vaccinations during periods of \n35\nsocial distancing, but also to ensure access to high quality and accurate \ninformation as Covid-19 vaccination research – and hopefully eventual \ndeployment – is underway. With reduced access to health professionals, this \nchallenge is even more present as people might rely more on other sources of \ninformation.  \n \nThere is a form of easy-to-reach consensus around the need to fight the spread of \nCovid-19 worldwide. In this context, people may have fewer privacy concerns \nwhen the sharing of data about them is for uses that the public think are \nimportant. Platforms might be more easily pushed (both by the public and \ninternally) towards taking an active role in the fight against the virus.  \n \nAs research begins on potential vaccines for Covid-19, a consensus might be \nharder to reach in the future. Opinions might differ as to how best to treat the \nvirus, and some might strongly oppose vaccination. This might make it more \ndifficult for people to agree on the use of data about them, and platforms might be \nmore cautious about playing an active role. \n  \n35\n The Colonel Group (2020), ‘​A future vaccination campaign against COVID-19 at risk of vaccine hesitancy \nand politicisation​’.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 27 \n\n \nUse case #3: Patient \nflow automation \nUse case description and background \nImproving the flow of patients, service users, information and resources within and \nbetween health and social care organisations can play a crucial role in coordinating \ncare around the needs of patients and service users, and driving up service quality \nand productivity. \n36\n \nPoor patient flow is not only a source of significant waste and delay, but it can also \nbe devastating for patients and service users, and deeply frustrating for people \nworking in health and social care. Poor patient flow contributes to crowded and \n37\nunsafe emergency departments; patients being admitted to wards that are not best \nsuited for managing their care; poor clinical outcomes (especially for frail patients); \npoor patient experience (for example being moved between wards or delayed care); \nand poor staff satisfaction. \n \nPopulation health management has highlighted the importance of understanding \n38\nhow patients flow through different pathways and between different care providers \nacross the health and care system. \n \nPatient flow management tools can help with making these pathways more efficient \nand can lead to better health outcomes. They can help map flows, and so enable \nanalytics and modelling, for example:  \n \n●real-time views of patient pathways and capacity within hospital wards for \nbed management \n●opportunity to link in information from others (for example, primary care or \ncommunity) and to share information on care records \n●opportunity to model patient pathways to help understand patient flow and \nperform ‘what-if’ analyses to identify more effective and efficient service \nconfigurations. \n \nPatient flow management tools are developed both ‘in-house’ by NHS analysts and \nby external organisations. They allow better reporting and enable managers and \nclinicians to access information closer to where the decisions get made.  \n39\n \nExamples of such tools include the patient tracker and the operational control \n40\ncentre developed by ​Beautiful Information​ (helping to see performance or capacity \n41\n36\n The Health Foundation (2016), ‘​The challenge and potential of whole system flow​’. \n37\n Ibid. \n38\n NHS England, ‘​Population Health and the Population Health Management Programme​’.  \n39\n The Health Foundation (2019), ‘​Untapped potential: Investing in health and care data analytics​’.  \n40\n Beautiful Information, ‘​Patient tracker live​’.  \n41\n Beautiful Information, ‘​Performance​’.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 28 \n\n \nwithin a hospital, in real-time) and ​Clinithink​ (helping to manage patient pathways and \nimprove resource management).  \nFindings \nThis section contains a summary of our findings. For further details see the ​research \nfindings appendix​. \nWhat an evaluation would aim to assess or demonstrate \nBased on our research, an evaluation of patient flow automation tools would follow \nan approach similar to the one outlined in use case #1; focusing specifically on \nassessing the impact of these tools on patients and the health system as a whole. \nEvaluators would seek to measure patient experience, patient outcomes and any \nunintended consequences in the treatment that patients receive.  \n \nIn terms of the impact on the whole system, evaluators would seek to assess the \nimpact that an implementation within a hospital might have on social care services, \nGP practices and community care services; focusing on whether the tool is causing \nadded strain on a particular part of the system. \nData an evaluation would need \nTo assess or demonstrate all of the above, evaluators would need to be able to \naccess and link data from a number of sources across local and regional health \nsystems. They would need data on how and when a specific patient flow automation \ntool was used and any actions taken as a result of using that tool. They would need \ndata from across the system on where patients were directed and what capacity was \nwhen they were directed there. Evaluators would also need data about any \ndecision-making algorithms used within these technologies and access to any data \nused to train those algorithms. \n \nFinally, they would need data collected from patients related to their experience and \noverall satisfaction. \nChallenges or barriers to accessing necessary data \nOur research identified several challenges and barriers to accessing necessary data, \nmany of which are similar to those identified in the previous two use cases. As in the \nfirst use case, we identified challenges related to accessing, linking and analysing \ndata from across the health sector, as well as challenges related to accessing data \nheld by commercial innovators, often due to concerns about personal and \ncommercial sensitivity.  \n \nSimilar to use case #2, there can often be a lack of clarity around what is legally \npermissible when it comes to the collection, sharing, linking and use of certain types \nof data. This lack of clarity can extend to control of data as well. Several people we \nspoke to noted that it is often difficult to know who controls different datasets and \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 29 \n\n \ntherefore who to approach to request access, especially when a hospital or health \nsystem is using digital technologies from multiple different providers.  \n \nAn additional challenge raised in many of the interviews was that some teams within \nhospitals are hesitant to share data because they are concerned that it will be \nmisunderstood by other departments and parties, and that misunderstandings could \ncome back to impact them in a negative way. \nRecommendations and next steps \nThe main challenge with this use case is ​gathering the data necessary to evaluate \nand compare numerous implementations of similar technologies.​ This is in large \npart a consequence of the fact that patient flow automation tools and services are the \nmost developed and widely adopted of the health technologies examined for this \nproject. Though digital-first primary care tools are gaining wider adoption – and are \nlikely to be implemented more widely following their use during the Covid-19 \npandemic – patient flow automation tools are already embedded in NHS processes.  \nConsider piloting a data institution aimed at enabling comparative \nand long-term evaluations  \nThe systems currently in use in the health sector include some that have been \ndeveloped by teams within NHS hospitals and trusts, and some that have been \ndeveloped by commercial companies. For evaluators, the difficulty in evaluating and \ncomparing these stems from the fact that each implementation is bespoke with \ndifferent technical setups, standards and legal agreements with data sources. The \nbespoke nature of the implementations requires evaluators to work in a similarly \nbespoke manner in order to collect, analyse and work out the legal agreements to \naccess necessary data. Comparative evaluations are therefore possible, but require \nlarge amounts of time, effort and resources. \n \nThere is a potential role for a data institution to play here. In short, this data institution \ncould work with public and private sector innovators to collect and steward relevant \ndata for the purposes of evaluating patient flow automation tools; while in return, the \ndata institution could provide innovators with aggregated insights such as \nbenchmarking and safety reports that help them improve their services.  \n \nThrough such an institution, evaluators would receive access to data about multiple \npatient flow automation tools, which would help them evaluate them individually as \nwell as comparethem to assess efficiency, safety and effectiveness. For their part, \ninnovators who contribute data about their patient flow automation system would \nreceive aggregated insights drawn from the patient flow automation market as a \nwhole. This could help incentivise public and private sector innovators to take part. If \nthat incentive is not enough, cooperation could be also compelled through the \ninclusion of specific requirements in contracts with patient flow automation providers \n(for more on this, see the recommendations in use case #1). \n \nThis type of data institution would also make it easier for evaluators to perform \nlong-term or periodic evaluations. In this use case, as well as in the others, we \nidentified challenges related to how to perform evaluations not just of a single \ntechnology at a single point in time, but how to perform evaluations of many \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 30 \n\n \ntechnologies and/or over long periods of time. Periodic evaluations will be needed for \nmany technologies, in particular technologies that include algorithms that learn ‘on \nthe job’ and therefore modify their decision-making over time. A data institution \nwould provide the stability necessary to enable these types of long-term evaluations. \n \nThe data institution would be able to do more than facilitate evaluations, incentivise \ncooperation and distribute aggregated insights, however. The institution should also \nbe able to help improve the operation of patient flow automation tools and ultimately \nhelp the ecosystem function more effectively. By leveraging its position as an \nintermediary in the ecosystem, the data institution could help to develop and \ndistribute standards – thereby improving the interoperability of the different systems, \ndatasets and technologies, as well as spreading best practice. These standards \nwould need to be developed collaboratively by organisations and stakeholders \nacross the ecosystem, and could aim to define minimum technical, legal and ethical \nstandards that enable interoperability and cross-comparison, while retaining the \ndynamism that is crucial to efforts to innovate at pace. \nInvestigate the form and scale of a data institution \nWe recommend continued research into the form and scale of the proposed data \ninstitution, in particular to investigate the following points: \n  \n●To what degree would the data institution need to be independent from \nthe health sector?​ It is unclear, for instance, whether the teams developing \npatient flow automation tools and the hospitals that would be asked to report \nstatistics about those tools would be comfortable reporting to an \norganisation with close ties to the NHS. During our research, some of our \ninterviewees referred to ‘parent-child syndrome’ and questioned whether \norganisations, be they public or private sector, would be as willing to report \nstatistics about the operation of their services if there is a perception that the \nbody they are reporting to could use that information to negatively impact \nthem in some way – such as through the loss of contracts or loss of funding. \nIf that is found to be the case, then the independence of the proposed data \ninstitution will be of paramount importance. An additional question, then, \nwould be whether the data institution will need to be a newly formed body or \nif the same roles can be played by an existing, independent organisation. \n \n●At what scale should the proposed data institution operate and what \nshould be included in its remit? ​As with the data institution proposed in use \ncase #2, there are questions related to whether a data institution of this sort \nshould focus on a specific emerging technology, a general type of technology \nor data technologies more broadly. Similarly, there are questions related to \nwhether such an institution should focus on a specific region or on the UK as \na whole. It should be noted that this type of data institution may prove \nvaluable for evaluating many emerging health technologies once they reach a \nlevel of adoption where comparative analyses become necessary. Digital-first \nprimary care systems, for instance, are likely to reach this level soon – if they \nhaven’t already. By piloting a data institution focused on patient flow \nautomation tools, it may be possible to design an approach that could be \nuseful for other technologies such as digital-first primary care.  \n \nSeveral people we spoke to suggested that organisations interested in exploring or \npiloting a data institution along these lines should get in touch with ​Clinical Practice \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 31 \n\n \nResearch Datalink (CPRD)​ to see if their approach to data collection and stewardship \ncould provide insights or be adapted for the purposes of evaluations. In brief, CPRD \ncollects data from GP practices across the UK and, working with NHS Digital, links \nthat data to a range of other health-related data. They then enable access to that \ndata for various stakeholders (universities, research organisations, charities, \npharmaceutical companies), while providing some data and benchmarking insights \nback to the contributing practices.  \n \nAlong similar lines, some interviewees suggested that a next step could be to contact \nthe ​Research Surveillance Centre​, which collects data from a range of different \nplatforms and software systems used by GP practices, and therefore has experience \nin helping to standardise data and data practices.  \n42\n \n \nCovid-19 context \nIn the context of Covid-19, having a high-level view of patients and resources \nwithin a health system has proved to be crucial.  \n \nThe government used data from various sources, including confidential UK patient \ninformation, in order to obtain this high-level view and as part of their response to \nthe outbreak. Rules were adapted to allow for easier sharing of such confidential \n43\ninformation. \n44\n \nHowever, it is unclear whether patients will still be comfortable with data about \nthem being shared so easily in a post-Covid-19 world.  \n \nIt is also unclear whether this level of sharing should become standard practice in \nthe future. A ‘top-down’ approach has proved to be valuable in a time of crisis, \nhowever going back to a ‘bottom-up’ approach might be the solution in the long \nterm.  \n \n \n \n  \n42\n Royal College of General Practitioners, ‘​RCGP Research and Surveillance Centre​’. \n43\n The Guardian (2020), ‘​UK government using confidential patient data in coronavirus response​’. \n44\n British Medical Journal (2020), ‘​Covid-19: Rules on sharing confidential patient information are relaxed in \nEngland​’. \n \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 32 \n\n \nConclusions and \nrecommendations \nThrough our research, we have identified a series of challenges, and the use cases \nhave helped us to dig deep into these challenges. Difficulties in accessing, linking \nand analysing data from across the health sector, as well as accessing data held by \ncommercial organisations, were highlighted.  \n \nDifferent forms of data institutions are relevant or useful for different types of \nchallenges. In some cases, we have identified a data institution as a way to address \ncurrent barriers to accessing commercial data, as well as data from across the NHS \nand the health sector. In other cases, a data institution could help enable \ncomparative and long-term evaluations. We have also explored other areas such as \ndata portability to collect specific types of data.  \n \nWorking to enact change within one use case can help address similar problems in \nother use cases. Many of the recommended steps for increasing access to data for \nthe purposes of evaluating the use cases in this report would have value far beyond \nthe use cases themselves.  \n \nBelow are recommendations for specific groups of stakeholders. \nRecommendations for evaluators \n1.Evaluators should consider themselves data institutions and look for \nways to steward data and increase access to data for public benefit. \nEvaluators regularly collect and acquire access to a wide range of different \ndatasets (public health, academic research, commercial, contextual) in order \nto do their evaluations. There is potential for evaluators to adopt the role of a \ndata institution, focusing on bringing together various sources of data, and \nactively looking to repurpose that data – either for use in further evaluations \nor to support wider research. This would require effort to clean, describe and \nmake the data they currently hold findable for other researchers who would \nbe interested in accessing that data. Some evaluators may also be able to \noffer analytical expertise or insight. An important next step along this route \nwould be to engage with patients, data holders and potential reusers of data \nto discuss their views of such an institution. \n2.Evaluators should use their position as an intermediary in many \ninteractions to encourage stakeholders across the ecosystem to share \ndata, and enable and support them to improve their capability and \ntrustworthiness. ​This should facilitate access to data for evaluation \npurposes, as well as for research purposes, and enable the deployment and \ninnovation of new tools and services. \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 33 \n\n \n3.Evaluators should act as convenors in the sector to create standards for \nbenchmarking technologies​, in cooperation with innovators, policymakers \nand funders. \nRecommendations for funders \n4.Funders should further explore new models for data access through \nscoping and pilot studies in the areas outlined above. ​Our research has \nscratched the surface of the challenges of accessing data in these use cases. \nThere are many aspects of new models of data stewardship for these \npurposes that still need to be explored through testing.​ ​The next stages of \nexploring these models – scoping and piloting – will take time, effort and \nresources. Seed funding is important to take these efforts to the next stage.  \n45\n5.Funders should explore other use cases in the sector such as precision \nmedicine or the impact of social media on mental health.​ For precision \nmedicine, it would be interesting to look at the impact of technologies such \nas AI-based symptom assessment on the health of patients or the work and \nefficiency of doctors, while also evaluating whether such technologies place \nadditional, unforseen strain on other parts of the health and care system. \nEvaluating the impact of social media platforms on the mental health of \nyoung people would allow the exploration of the impact that technologies not \ntraditionally understood as ‘health technologies’ have on the health of the \npopulation and on the delivery of health and care within the UK. \nRecommendations for innovators \n6.Innovators should get evaluators in the room early to arrange the data \ncollection.​ Challenges have been identified around a lack of relevant data \nbeing collected for the purpose of evaluation. This can be due to evaluation \nbeing thought of or put together too late.  \n7.Innovators should be prepared to share data for research and evaluation \npurposes.​ There can be some fears related to sharing data as an innovator. \nHowever, a key incentive is that sharing of data is necessary for evaluation \nand research to be successful and insightful, to improve products and to \ndemonstrate their effectiveness for customers. \n8.Innovators should explore best practices around collecting sensitive \ndata about who uses digital services. ​Collecting, using and publishing data \nabout who uses digital services is important for demonstrating their \neffectiveness across different communities and ensuring new technologies \ndo not increase existing inequalities.  \n46\n9.Innovators should work together to develop standards for benchmarking \nand comparison. ​This would allow for evaluations to be even more insightful \nwhen they can compare different technologies.  \n45\n Open Data Institute (2020), ‘​Designing sustainable data institutions​’. \n46\n Open Data Institute (2020), ‘​Protected Characteristics in Practice​’. \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 34 \n\n \nRecommendations for health and care providers  \n10.Health and care providers should convene innovators and healthcare \npractitioners. ​This would allow alignment in understanding and expectations \nrelated to data collection for evaluation purposes.  \n11.Health and care providers should build in evaluation from the start when \npiloting or deploying new health technologies.​ This would allow the sector \nto keep innovating at pace, while ensuring safety and effectiveness. \n12.Health and care providers should clarify the ways the data will be \ncollected, accessed, used and shared at the procurement stage​. \nRequirements should be made clear during procurement, to allow for \n47\naccess to data for evaluation as well as other purposes. In some cases, it \nmay be necessary to embed mandatory access to data for evaluation \npurposes within procurement contracts. \nRecommendations for patient and practitioner \ngroups \n13.Groups representing the users of health technologies (including \npatients, carers and healthcare practitioners) should explore \ncooperative models for collecting data about their experience. ​Data \nabout stakeholder perspectives is an important part of any evaluation. The \nformation and stewardship of datasets that capture these experiences and \nperspectives would allow expectations and concerns to be heard, and for \nevaluations to be more insightful. Presenting data and facts can be an \nimportant way of being heard. These approaches have been explored in \nother sectors, for example, ​Workers Info Exchange​ helps Uber drivers to \ncollate data about their experience. \n \n \n \n \n \n \n \n \n \n  \n47\n Open Data Institute, ‘​Guide - How to embed open data into the procurement of public services​’.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 35 \n\n \nAppendices \nAppendix 1: Project methodology \n \nThe ODI started with some desk research on the following topics: \n \n●types of new health technologies \n●evaluation approaches for health technology \n●data needs and challenges to support evaluation \n●health policy data landscape \n●data stewardship approaches for health data. \n \nThree workshops were organised to explore each use case. During these workshops, \nparticipants were asked to reflect on:  \n \n●what they would want to assess or demonstrate through an evaluation, and \nwhat data would be necessary to do so \n●what the limitations, pain points or challenges of current systems and \nprocesses are for gaining access to data, and why they exist. \n \nA wide range of participants took part in these workshops:  \n \n●Workshop 1: Babylon Health, AskmyGP, eConsult, Ada Health, UseMyData, \nNHS England, NHSX \n●Workshop 2: Alan Turing Institute, University of Essex, Carnegie Trust, \nUniversity College of Dublin, King’s College London, FullFact, DEMOS/ \nCentre for the Analysis of Social Media, Department of Health and Social \nCare, Economic and Social Research Council, Royal Society for Public \nHealth, Nesta, Ada Lovelace Institute, Ipsos Mori \n●Workshop 3: Beautiful Information, Future Care Capital, Cardiff University, \nGoogle Health, Liverpool Clinical Commissioning Group, Sheffield Teaching \nHospitals NHS Foundation Trust, East Midlands Academic Health Science \nNetwork. \n \nThe ODI also ran some interviews to explore the use cases a bit more and to test \nsome findings. The following organisations took part in the interviews: \n \n●Academic Health Science Networks \n●National Data Guardian \n●University of Siegen  \n●Beautiful Information \n●Salus Coop \n●HealthBank Cooperative \n●Alpha Health  \n●NHSX \n●Use My Data \n●Ada Lovelace Institute \n●Facebook \n●Social Science One  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 36 \n\n \n●Mumsnet \n●Understanding Patient Data \n●Health Data Research UK \n●Clinical Practice Research Datalink \nAppendix 2: Detailed findings  \nUse case #1: Detailed findings \nWhat an evaluation would aim to assess or demonstrate \nOur research found that when evaluating digital-first primary care (DFPC) \ntechnologies, evaluators would want to perform a three-pronged evaluation: the \nmeasurement of outcomes; the description and analysis of the process; and the \nsampling of multiple stakeholder perspectives. \n48\n \nOutcomes would cover efficiency of the system as a whole, quality and safety, and \neffectiveness of care. They could also cover economic aspects (translating impact \ninto economic terms). Analysing the process would mean evaluating how the \ntechnology was or is being implemented. Looking at key stakeholders’ perspectives \nwould mean looking into patients’ experiences as well as staff feedback on the use of \nthis particular technology.  \n49\nData an evaluation would need \nEvaluators would need access to a wide range of data: \n \n●General practitioner (GP) data \n○Feedback from GP staff on their experience once the technology was \nimplemented \n \n●Patient data \n○Data on patients’ backgrounds in relation to their access and use of \ntechnology in general (for example, deprivation, digital literacy, \ninternet connection) \n○Patient demographic data (for example, age, gender, ethnicity) \n○Usage data (data related to how the patients used the technology) \n○Patient pathways (unnecessary accident and emergency attendance) \n○Patient outcomes (what happened after the online consultation? Did \nthe patient receive health advice? Was it followed by a face-to-face \nor telephone consultation?) \n○Feedback on patients’ experience using the technology (user \nsatisfaction and understanding) \n \n●Operational data  \n○Aggregated utilisation data related to how and when the digital \naccess was used by patients (for example, aggregate data from all \nuses of the technology in order to understand how representative \nthey were) \n48\n Ellis R, Hogard E (2006), ‘​The Trident: A Three-Pronged Method for Evaluating Clinical, Social and \nEducational Innovations​’.  \n49\n The Health Foundation and the Open Data Institute, ‘​Workshop 1 notes​’. \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 37 \n\n \n○Number of online consultations and triage data \n○NHS system-level data (for example, waiting times, GP workflow) \n○Data about the decision-making process used for the technology \n(algorithms as well any data used to train them). \n \nThere is also a need to link these datasets together, to understand the pathways \ntaken by patients through the system, and the wider impact of the service, for \nexample: \n \n●Data between the provider, GP practices and other health services (eg \nSecondary Use Service) to understand outcomes and show how they might \n50\nhave changed. \n●There could also be an ability to link familial records if patients are happy to \nconsent to it.  \nChallenges or barriers to accessing necessary data \nAn evaluation would require the gathering of data from multiple sources. It can prove \nto be difficult to access this data, for the following reasons. \n \nSome data is not currently being collected: \n \n●There may be a lack of data, limiting the effectiveness of evaluation of DFPC \ntechnology. This can be related to a lack of resources (time and capacity), \nhigh data collection costs or even a particular data collection not being set as \na priority (for example, surveys, or user or GP experience statistics).  \n●The collection and/or analysis is often not financially feasible. \n \nAccessing the data can be difficult: \n \n●There may be concerns related to data sharing, depending on the data \nsteward, such as: \n○Tech providers may have legal and intellectual property concerns.  \n○GPs may be concerned about who is accessing their patient data \nand for what purposes.  \n○Local authorities may have concerns about privacy and may face \ntechnical barriers to sharing.  \n○Individuals may have concerns about their own privacy.  \n \n●Data may not be shared or accessible to a certain extent because: \n○there may be a lack of real-time data \n○the data may not be granular enough \n○the data may be anonymised to the point that it is less useful \n○the data may only be shared for a brief period of time, however \nevaluation requires regular or continued access. \n \nWhen accessing the data, it can lack consistency: \n \n●There may be gaps in the datasets (collected with a different aim in mind). \nThere is no single, national primary care dataset that can be used for \nresearch or evaluation.  \n●The data may not be of a high enough quality (and therefore difficult to \ncombine or interoperate). \n50\n NHS Digital, ‘​Secondary Uses Service (SUS)​’.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 38 \n\n \n●Data may not linkable (for example across a disease rather than across an \nindividual; between DFPC provider and GP clinical system; across primary \nand secondary care) \n○DFPC providers are unable to link their data with national datasets \nsuch as Secondary Uses Service/Hospital Episode Statistics. \n51\n○Different GP practices use different patient record systems. Not all \nDFPC systems necessarily link in to GP-held patient records. \nUse case #2: Detailed findings \nWhat an evaluation would aim to assess or demonstrate \nUnderstanding correlation and/or causation: \n \n●What information is present online? \n●What information have people been exposed to? \n●What external factors influence people’s interactions with such information \n(eg socioeconomic background, the source)? \n●What is the correlation between what people see online and their beliefs/ \nbehaviour? \n \nUnderstanding the people actively playing a role in spreading misinformation and the \nones being impacted by it: \n \n●Understanding the intent of people propagating misinformation about \nvaccines. \n●Understanding the communities within which the information is flowing:  \n○What makes people more susceptible to misinformation? \n○What is their exposure (information related to their identity, social \norganisation, the power relationships they are involved in)? \n \nMeasuring and understanding the impact misinformation has on people and how to \nchallenge misinformation: \n \n●Identifying its direct impact on vaccination rates. This could help identify \nwhat the highest impact interventions could be (for instance, how effective \nare counter messages, do they make a difference to behaviours?). \n●Understanding what steps have been proven capable of combating either the \nspread of misinformation or user beliefs/behaviour. \n●Understanding where trust exists (for instance with authority figures such as \nlocal GPs, central authorities): what factors impact the level of trust that \npeople place in information and the source of that information (for example, \nwhen the information is coming from the NHS, versus a politician, versus a \nfriend)?  \n \nEvaluators may also want to have a view on what platforms and other sources of \nonline information are doing that impacts the spread of misinformation and user \nbehaviour: \n \n●What, in the design and algorithms being used for instance, makes the \nspread of misinformation easier? \n51\n NHS Digital, ‘​Hospital Episode Statistics​’.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 39 \n\n \n●When misinformation is identified, what actions are put in place to fight \nagainst it? \nData an evaluation would need \nTo assess some of the elements listed above, various types of data would be \nneeded: \n \n●Data about the content \n○Data related to misinformation being available online: measuring what \ncontent actually exists on platforms, how much and where. \n \n●Data about engagement with misinformation \n○Data about how people engage with misinformation: metadata about \nhow it is viewed, shared and commented on.  \n○Data on engagement with offline sources of official information (eg \nGP and health visitor interactions, public health communications), as \nwell as data related to how misinformation online interacts with \nmisinformation offline. \n \n●Data about people \n○Data about people’s background: information about identities \n(belonging, social organisation underlying beliefs). \n○Data on the impact and profile of the personal relationships that have \nthe most influence: “friends of friends who think x”, “mums \nnetworks”. \n○Data about the impact on behaviours and health outcomes: is \nmisinformation leading to a change in attitude? Is it leading to a \nchange in vaccine uptake? \n \n●Granular data related to geographies (to allow for an analysis at city level for \ninstance). \nChallenges or barriers to accessing necessary data \nThere are various challenges related to accessing the data that would help assess the \nimpact of online misinformation on vaccine hesitancy.  \n \nSome data is currently not being collected on this topic: \n \n●There can be various reasons for some data not being collected at the \nmoment.  \n○Organisations may: \n■just not want to collect data  \n■lack time and/or resources  \n■lack infrastructure  \n■be barred from collecting it  \n■not realise it would be useful  \n■be uncertain about whether they can collect it \n○People may not be willing to have some data collected. \n \n●There is currently limited follow up with people who do not present for routine \nvaccinations.  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 40 \n\n \n●There is a lack of insight and knowledge (from government, NHS, public \nhealth organisations) on patients’ behaviours and motivations around \nvaccination. \n \nThe topic is complex and so the data needed to analyse it is complex: \n \n●There are challenges related to knowing what data exists, who holds it and \nhow to access it. \n●The activity on an online account does not capture the experience of a \nperson. The context both across the online space and offline is important. \n●Data on vaccination behaviours has limitations. Understanding all the factors \nleading to vaccine hesitancy can be challenging. \n \nAccessing the data can be difficult: \n \n●Data on online behaviours and the algorithms used is held by private tech \ncompanies, who may not always be willing to share the data.  \n○This may be due to: \n■commercial sensitivity and intellectual property (some data \nmay contain strategic information, such as advertising \nefficacy, or data about the algorithms used for instance)  \n■privacy concerns \n■ethics concerns related to the use of personal data (it may be \nthat the platforms are very wary of sharing any information of \na personal nature and users are wary of them doing so). \n○It might also be due to a lack of incentives: why would a platform \ngive such deep access if it is not required? What is the benefit for \nthem? This is especially the case if the data requested is core to their \nbusiness, intellectual property or unique selling proposition. \n \n●There is a lack of clarity around what is and is not permissible. Some \nstakeholders may need clarity on what can be done and what data can be \naccessed. Otherwise, data may not be shared when it could legally and \nethically be shared, and data stewards may be uncertain and therefore wary \nof running afoul of laws/regulations that are not clear. \n●Data can also be held by individuals, and they may have privacy concerns \nrelated to the use of data about them.  \n●Accessing the data may be costly.  \n \nWhen accessing the data, there may be a lack of consistency: \n \n●There is a lack of consistency or standardisation across platforms for \nflagging, labelling, reporting and removing misinformation, which will \ncomplicate any analysis across different online sources (and such processes \nmight change rapidly within single platforms). This is because most \nmisinformation is likely to be contested or have subjective aspects.  \n●The format in which data is accessible might not always be useful. \n●Linking data between online platforms and health services is difficult.  \nUse case #3: Detailed findings \nWhat an evaluation would aim to assess or demonstrate \n●Impact on patients:  \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 41 \n\n \n○What is the patient experience?  \n○What are the patient outcome consequences (for example, cancer \npathways are normally set within timeframes and relate to patient \nflow across a health system from primary to secondary care)? \n○Are patients accessing other services (for example, are mental health \npatients also accessing community services?) and what is the \nbroader health profile of patients accessing these services? \n○Are there any unintended consequences (for example, fairness)? \n \n●Impact on the whole system:  \n○How does it impact social care, GPs and community care (for \nexample, are decisions made by an AI-backed patient pathway tool \ncausing added strain on a particular section of the health and care \nsystem)? \n○How does it impact the quality of care and where are the anomalies \n(for example, where are the 3% of patients suffering an extraordinary \nwait)?  \nData an evaluation would need \nTo assess or demonstrate all of the above, a wide range of data would be needed. It \nwould come from various services such as hospitals, mental health services, \ncommunity GPs, as well as providers such as ​EMIS​, and national services such as \nthe Office for National Statistics (ONS; for census data).  \n \n●Patient data \n○Measures of patient satisfaction. \n \n●Operational data \n○Data from across the system (for example, were patients shunted \nelsewhere? Would the receiving service have capacity?). \n \n●Provider data \n○Data on use of the tool \n○Training data (eg data about the original context), particularly if an \nalgorithm is involved and has been trained in a specific context for a \nspecific purpose. \n \nSome identifiable data would be needed, especially to be able to link data together. \nData would also need to be comparable across hospitals/trusts.  \nChallenges or barriers to accessing necessary data \nThere are various challenges related to accessing the data that would help assess the \nimpact of patient flow tools.  \n \nAccessing the data can be difficult: \n \n●Obtaining data from various companies can be an issue. This can be due to:  \n○a lack of time and resources \n○a lack of incentives \n○a cultural reluctance to share data (within care providers and across \nthem) \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 42 \n\n \n■Patient safety teams would want their data to be understood \nin the right way. Some might have worries around the use of \ntheir data. \n■There can be patient and public reluctance as well. \n■There can be trust issues between patients and the public \nand private sectors. \n \n●Data may not be shared or accessible to a certain extent \n○Time lags can be a challenge. This can be due to coding and data \nquality processes needed before the data can be used. \n \nWhen accessing the data, it can lack consistency: \n \n●There are various clinical systems and trusts, where data collection is \ninconsistent and not standardised. \n●There are data quality risks related to manual input (which could be related to \nresources and technical issues), for example, patient data being manually \nentered on spreadsheets is prone to lag, duplication and error. This is the \ncase for care homes for instance, where community care data is often \nrecorded manually and not shared. \n○Related to this, there is a lack of analytical expertise to represent \ndata in such a way that facilitates improvement measures. \n \n●There is no defined terminology or taxonomy for data collectors to describe \nthe data to consumers. \n●Data without context can be problematic: without understanding the original \npurpose, interpreting it for secondary use can be challenging. \n \nThere can also be challenges related to people not knowing what is permissible (for \nexample, people not being comfortable with linking data, as they are not confident \nthey are doing it right). It is also sometimes difficult to know who controls different \ndatasets – for instance, is it the hospital that collected the data or the software \nprovider that built the data-collection system? \n \n \nOpen Data Institute 2020Applying new models of data stewardship to health and care data 43 ","version":"1.10.100"}