{"numpages":29,"numrender":29,"info":{"PDFFormatVersion":"1.5","IsAcroFormPresent":false,"IsXFAPresent":false,"Producer":"Skia/PDF m68"},"metadata":null,"text":"\n\n \n\n \n \nContents \n \nWhat are registers and why are they important?4 \nAn introduction to registers5 \nThe Register Society’s Register of Shipping7 \nHow the internet has changed registers8 \nRegisters as web services9 \nFrom web service to data infrastructure9 \nRegisters and abstractions10 \n1. Type of list11 \nPrimary or secondary source11 \nContents11 \nTechnology12 \n2. Authority13 \nSources of authority13 \nReality15 \nCompleteness16 \nAccuracy16 \n3. Stewarding17 \nWho will maintain this register?18 \nHow is an entry added to the register?18 \nHow are errors in the register corrected?18 \nWho is paying to maintain this register?18 \nCommunicating stewarding18 \nCollaboration makes better registers20 \nFunding registers21 \nState funding21 \nService revenue21 \nHM Land Registry22 \nGrants and donations23 \nBuilding trust when stewarding24 \nModels for collaboration25 \nCustodial discretion25 \nSuggested edits26 \nFood Standards Agency Sampling Classification Register26 \nDirect access27 \nRegistering a company in England and Wales28 \nAggregator registers29 \nUK Postal Addresses30 \nCrowdsourcing31 \nDemocracy Club Candidates32 \norg-id.guide33 \nMixed models34 \nAppendix 1: report methodology34 \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust2 \n\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nAbout \nThis report is about registers or lists, and ways of publishing and maintaining them. \nWe examine models of stewarding registers and review case studies of different \nregisters. We argue that transparency is necessary, and collaboration desirable, in \nmanaging registers that are trusted by their users. \n \nThis report has been researched and produced by the Open Data Institute, and \npublished in May 2018. Its lead authors were David Miller and Sym Roe. If you want \nto share feedback by email or would like to get in touch, contact the Registers \nproject lead Leigh Dodds at ​Leigh.Dodds@theodi.org​. \n \n \n \n \n \n \nHow can it be improved? We welcome suggestions \nfrom the community in the comments. \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust3 \n\n \nWhat are registers and \nwhy are they important?  \nRegisters are accurate, complete lists. However, not all lists are registers. This \ndocument explores some of the key properties of registers and discusses the \nimplications of these. It does not provide a hard definition or set of criteria for what is \nor is not a register. Such exercises have tended to produce criteria that exclude lists \nthat it is helpful for maintainers and users to consider as registers, or the criteria have \nbeen so vague that they cease to be meaningful. \n \nWhat are the key properties of registers beyond accuracy and completeness? \nRegisters are only useful when we trust them. This means that we need a reason to \ntrust the process of keeping the list up to date, ideally because that process is clear \nand transparent.  \n \nWhat do registers contain? There are no hard rules about what they contain, but \nbroadly speaking, the contents of registers are important enough for many people to \ncare about them being accurate and complete. For us to think of a list as a register, \nan institution must have promised to keep it up to date.  \n \nDigital registers reduce the cost of services that require data to operate and reduce \nthe burden on institutions that have to manage data. Therefore, publishing registers \nthat take advantage of digital technology can unlock space for new products and \nservices that have previously been too expensive or too risky to build and maintain. \n \nAlmost all registers use some degree of collaboration. Embracing that collaboration \noften increases their quality and usefulness. Today, there are tools that can make \nthat collaborative process faster, cheaper and more accessible. Taking advantage of \nthem is an opportunity to strengthen our data infrastructure, at reduced cost. \n1\n \nWe define the process of maintaining a register through collaboration as stewarding. \nThis is the process of maintaining a list by a group or institution and the promise they \nmake to maintain that list. That process is co-ordinated by a custodian. Transparent \nstewarding of a list means being clear and open about how it is maintained. \n  \n1\n Open Data Institute, 2018, ‘Data Infrastructure’, \nhttps://theodi.org/topic/data-infrastructure  \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust4 \n\n \nAn introduction  \nto registers \nThe word register is both a noun and a verb. The word register also implies that there \nis an official quality to the list or act of listing. \n \nFor many of us, our first introduction to registers comes as we sit in a classroom and \nthe teacher takes the register. The teacher reads out a list of names and we sit \nwaiting for our own. When we hear it, we answer “yes”. The teacher makes a mark \nagainst our name and our attendance is recorded. This simple example actually \ncontains several of the key features of registers – that they are recorded, accurate \nand official – as well as some less-obvious characteristics.  \n \nThere are three uses of ‘register’ — two nouns and a verb — in the act of taking the \nregister. Our teacher reads from the register of pupils in this class (1) in order to \ncreate the register of attendance (2). When the pupils answer in the affirmative, the \nteacher will register them (3) in the register of attendance (2). These different uses of \n‘register’ will be used throughout this report, though we will avoid using register as a \nverb except where it has a specific legal meaning. Instead, we talk about creating, \nupdating and removing entries from a register. \n \nThe classroom example demonstrates many of the characteristics of registers we \ndiscuss in this report. The primary register of the pupils in a class is a subset of the \nsecondary register of pupils at a school. Both of these registers are issued by the \nschool, an institution with authority over the contents of the lists. We know the \nregister of pupils in class 1B is a complete register that contains all the pupils in the \nclass. These registers define reality. When your name appears on the school’s \nregister of pupils, you have become a pupil at this school. Similarly, when a pupil’s \nname is added to the register of class 1B, they are in class 1B.  \n \nThe daily register of attendance describes reality. It is a record of who is present that \nday (and who is not), but it does not change who is in the room or who is a pupil in \nthe class. The register of attendance is created by the teacher, who has the \nspecifically assigned responsibility to undertake the process of researching reality \nand recording the results of their understanding. This register is also verifiable by \nanyone else in the room: they too could mark off the names of the pupils and create \nthe register independently. The school will aggregate these individual registers of \nattendance, creating a list of all the pupils who were at the school on any given day. \n \nWe create registers in order to do something with the information they contain. When \na pupil arrives on the first day of school, members of staff will check for their name \non the register of pupils in a class to find out which room they should go to. Later on, \nthe register of pupils at the school will be used to find out about overall attendance. \nThis can be divided into individual pupils (who can be given additional help if their \nattendance is low) or represent the school overall (which contributes to its quality \nassessment by Ofsted, which in turn is used by parents to work out where to send \ntheir children). \n \nRegisters have a long history outside the classroom. Some of these registers are very \ndifferent kinds of lists, particularly with regard to how hard it is to maintain their \nobjective completeness and accuracy. \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust5 \n\n \nCase study: The Register Society’s Register \nof Shipping \nIn the late 18th century, The Register Society created a register of shipping that \n“classified and registered vessels according to certain criteria of physical structure \nand equipment, to enable underwriters, shipbrokers, and shipowners more easily to \nassess commercial risk and to negotiate marine insurance rates”. \n2\n \nBecoming Lloyd’s Register of Shipping in 1834, this register has been published \nannually since at least 1775. For most of this time, the register was published as a \npaper book. \n \nThe Register of Shipping differs from school registers in several ways. While a \nregister of pupils in a class contains very little information – just the name of the pupil \n– the Register of Shipping records many different attributes about each ship. \nAlthough some of this information is reproducible and verifiable, such as tonnage and \ndimensions, and can be obtained by surveying, some items in the register are less \nobjectively defined. Over time, the characteristics of ships that people considered \nimportant to record has changed: ‘guns’ are no longer a standard ship classification.  \n \nMuch of the Register of Shipping is a list of opinions that underwriters and owners of \nships had about the quality of ships. Because entries in the register were not always \nthings that could be objectively agreed, a dispute between the two groups caused \ntwo registers to be published. Called The Green Book and The Red Book, each \n3\ncontained different lists of ships and different information about those ships. \n \nThe design of the Register of Shipping meant that it could never be completely \naccurate. It would be legitimate for two people to hold different opinions about a \nsingle value in the register. Even with these potential discrepancies, it proved \nimmensely useful to the shipping industry. This official list, which different parties \nagreed to use as the standard, allowed owners to insure their ships.  \n2\n Michael Palmer (1999), ‘Lloyds Register of Shipping’, \nhttp://www.mariners-l.co.uk/ResLloydsRegister.htm \n3\n University of Glasgow Archive Services (2011). ‘Lloyd’s Register of Shipping’, \nhttps://www.gla.ac.uk/media/media_67062_en.pdf \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust6 \n\n \nHow the internet has \nchanged registers \nThe internet and the web introduced new possibilities for registers.  \n \nPublishing and updating became easier and quicker. But publishing formats and \nprocesses have been slow to adapt to new opportunities and needs. Progress from \nregisters on private databases or (even) paper to registers as part of trusted data \ninfrastructure, available to help other web services deliver reliable online services, is \nstill uneven. \n \nStoring registers in a database allows us to both store lists and to perform complex \noperations like searching, aggregating and filtering in fractions of a second. Many \nregisters were moved to databases decades before the invention of the web. \n \nWhen the web became widely available registers started to be published online. They \ntended to be published as static ‘snapshots’, often in parallel with printed ‘editions’ \nand in formats such as PDF which can be read easily by humans but not \nby machines. \n \nA bigger step was needed to make registers part of the web, key components of a \ndata infrastructure which enables a host of new, simpler, cheaper and faster services \nfor everything from renewing a passport to global shipping insurance brokerage. \nRegisters as web services \nRegisters have started to become web services rather than static collections of data. \nThe register itself, rather than a frozen-in-time snapshot of it, is available online. It is \nalways up to date. And access to it uses the natural patterns and protocols of the \ninternet: register entries have their own addresses on the web and can be \nmanipulated, used or linked to by third parties. \n \nTurning a register into a usable web service requires substantial change by its \ncustodians, affecting its publishing methods and systems and, often, the design and \nstructure of the register itself. \nFrom web service to data infrastructure  \nTreating registers as infrastructure requires that we can reliably expect and inspect \nspecific standards of authority and continuity. This allows the creation of tools and \nservices built on the data. It makes it easier to curate, maintain and consume data. \n \nInfrastructure, particularly when operated by the state, implies a long-term \ncommitment. Users can assume that there will continue to be an accurate list and \nthat curating it will continue to be someone’s specific responsibility. \n \nReliable data of a known provenance is critical to people who build services using \nthe data held in registers. Being able to rely on a register as infrastructure for their \nservices means they only ever need to work with one reliable and current source of \ndata – what is known in information systems theory as a ‘single source of truth’. They \ncan concentrate on building the service rather than spending time checking if the \ndata they are using is still accurate.  \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust7 \n\n \n \nWhen registers can be easily accessed by other products or services, they are more \nwidely useful than when they were books in council basements. Combining internet \ntechnology for publication with a commitment from the state to maintain registers \nmeans society can reap increased benefits from these official lists of things, \nconsuming and using them in ways that were not previously possible. \n  \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust8 \n\n \nRegisters and \nabstractions \nBefore exploring the details of specific registers, it is worth looking at some of the \nabstract aspects of lists. We are deliberately examining the more general case of lists \nrather than only registers. This helps establish ways of talking and thinking about \ndifferent kinds of registers as well as allowing us to consider which of these \nproperties are required for us to think of a list as a register. \n \nWe have grouped the features of registers into three related categories: type of list, \nauthority and stewardship. We created the categories by examining a sample of 20 \nregisters of varying forms, how they were similar, how they differed and by noting \nhow people discuss them and describe their key features. The results of this analysis \nare available as open data. When people make lists, they often create hybrid forms \n4\nthat cross these boundaries. \n \n1. Type of list  \nThe first group of features allows us to understand what kind of list we are \ndealing with. \nPrimary or secondary source  \nSome lists will be the primary source of that information; others will not. These are \nusually an aggregation of smaller lists. The register of attendance taken by a teacher \nis the primary source of data about which pupils attended class that day. The \nschool-wide attendance register is an aggregated, secondary list made up of all the \nother (smaller) lists of who attended class. \n \nIn this report, we refer to source lists as primary and aggregated/derived ones \nas secondary. \n \nPrimary:​ the main source of the information contained in it, like a class attendance \nregister, or the register of companies provided by Companies House. \n5\n \nSecondary:​ A list created from other sources of information, like the school-wide \nattendance register, or the database of companies provided by Open Corporates. \n6\n \nContents: ​The level of detail that we include about each entry in a list, e.g. its name, \nidentifier and other attributes, is a central aspect of how we think and talk about the \nlist. When designing a list, we have to decide how much information to record for \n4\n David Miller, Sym Roe (2018), ‘Registers and collaboration - describing lists’, \nhttps://docs.google.com/spreadsheets/d/1eLmRzkKFJx6hpOcVTv6WmcFYOULCB0\nSWSOCN0lF5O1Q/edit \n5\n Companies House, ‘Search the register’, ​https://beta.companieshouse.gov.uk/ \n6\n ‘Open Corporates’, ​http://opencorporates.com/​. Open Corporates reuse data \npublished by Companies House and other sources. \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust9 \n\n \neach entry. This decision sits on a sliding scale, ranging from least possible amount \nof information to most possible amount of information.  \n \nThis scale is highly subjective. Trying to define hard criteria or subsections of the \nscale is not particularly useful. Instead, think about lists in terms of which end of the \nscale they are closest to, from minimum viable dataset to what we call a \nbroad dataset.  \n \nMinimum viable dataset:​ tightly defined, minimal schema of what makes up a \ndataset. There is an expectation that all or at least most attributes will be completed \nfor each entry. The FCO Country register or the register of pupils are closer to this \n7\nend of the scale.  \n \nBroad dataset:​ everything we know or might know about one entry on the list, \nincluding attributes that might be based on opinion rather than objective facts. The \nShipping Register or Open Street Map are both close to this end of the scale.  \n8\nTechnology \nThe technology the list uses to record or publish information can change how we use \nor think about the list. List technologies fall into the following broad categories: \n \nPaper: the list is stored and published on paper, like the first few hundred years of \nthe Lloyd’s Register of Ships. \n \nDigital paper: the list is stored or published electronically, but it uses a format where \nindividual entries are not addressable and the contents not machine-readable. Legal \nnotices published as PDFs or Microsoft Word documents on council websites fall \ninto this broad category. \n \nMachine-readable: the list is published in a machine-readable, computable format, \nprobably online. For example AddressBase, the Ordnance Survey’s address \ndata product. \n9\n \nLinkable: the list is published in a machine-readable, computable format, with \nindividual entries addressable via URIs, such as the government’s service register. \n10\n \nMany lists are published in ways that span more than one of these list technologies; \nthe Companies House register can be downloaded in a machine-readable format \n11\nand is also published as a linkable online service. \n12\n \n \n  \n7\n Gov.uk, ‘Country register’, ​https://country.register.gov.uk/ \n8\n OpenStreetmap, ​https://www.openstreetmap.org \n9\n Ordnance Survey, ‘AddressBase’, \nhttps://www.ordnancesurvey.co.uk/business-and-government/products/addressbase\n.html \n10\n Gov.uk, ‘Government service register’, ​https://government-service.register.gov.uk/ \n11\n Companies House, ‘Free Companies Data Product’, \nhttp://download.companieshouse.gov.uk/en_output.html \n12\n Companies House, ‘Open Data Institute’, \nhttps://beta.companieshouse.gov.uk/company/08030289 \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust10 \n\n \n \nList Source Contents Technology \nRegister of \nShipping \nPrimary Broad dataset Paper \nDemocracy Club \nCandidates \nSecondary Broad dataset Linkable \nCouncil candidate \nnotifications \nPrimary Minimum viable \ndataset \nDigital paper \nCountries Register Primary Minimum viable \ndataset \nLinkable \nRegister of pupils \nin a class \nPrimary Minimum viable \ndataset \nPaper \nHistoric England’s \nlist of listed \nbuildings  \nPrimary Broad dataset Machine- \nreadable \n \nTaken from the full list of lists used to make this system. \n13\n2. Authority \nThe second group of features are reasons to consider a list authoritative.  \nSources of authority \nWhen talking about complete and accurate lists (or even lists that are complete and \naccurate enough to use) we also need to be able to understand how we are able to \nmake that assertion. There are various sources of authority: \n \nFrom first principles:​ Some lists are provably complete and accurate. A list of the \nfirst thousand prime numbers could be proved complete through calculating those \nnumbers and then checking the register against the list you computed. \n \nVerifiable observation:​ When the list contains things that have been observed in the \nworld, it is often possible for someone else to recreate the list from scratch. \nWikipedia’s list of rivers longer than 1000 km could be verified by someone else \n14\nmeasuring the lengths of rivers. \n \nObservation might introduce bias or opinion in the register in ways that make it a \nmatter of opinion whether or not it can be reproduced. This is a form of \nobserver-expectancy effect. In the classroom register, a teacher might make \n15\nallowances for a child who is not actually in the room when the register is taken, \n13\n David Miller, Sym Roe (2018), ‘Registers and collaboration - describing lists’, \nhttps://docs.google.com/spreadsheets/d/1eLmRzkKFJx6hpOcVTv6WmcFYOULCB0\nSWSOCN0lF5O1Q/edit \n14\n Wikipedia, ‘List of rivers longer than 1000 km’, \nhttps://en.wikipedia.org/wiki/List_of_rivers_by_length#List_of_rivers_longer_than_100\n0_km \n15\n Wikipedia, ‘Observer-expectancy effect’, \nhttps://en.wikipedia.org/wiki/Observer-expectancy_effect \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust11 \n\n \nbecause the pupil is known to be running late. Other teachers might decide to only \nnote pupils who are actually in the room at the exact time when they are taking \nthe register. \n \nTrusted observation:​ For some lists that are created by observing the world, \nre-observation or verification is not possible. This is often because they take a \nsnapshot of a point in time. While the school class attendance register can be \nvalidated at the time by a third party, a week later it is impossible to verify that it is \ncorrect. There might, however, be ways to detect whether there are likely to be errors \nin a list. In the class attendance register example, if an attendance register on a given \nday was very different from the same register on surrounding days, you might expect \nit to be wrong. \n \nThe authority of such lists depends on how much we trust both whoever made the \nobservation, and the integrity of the processes for storing, retrieving and publishing \nthe list. \n \nInstitutional authority:​ The contents of the list are the responsibility of a single \ninstitution, which asserts that the list is complete and accurate. When Microsoft \npublishes the list of software it provides support for, we believe this list to be \n16\ncomplete. Microsoft can be trusted to say that this is a complete list because it is in a \nposition to unilaterally decide the contents of the list. \n \nLegislative custodian:​ A subset of institutional responsibility where the institution \nresponsible for maintaining the list is doing so due to a statutory obligation. For \ninstance, the register of UK political parties is maintained by the Electoral \nCommission due to their obligations in, for example, the Political Parties, Elections \n17\nand Referendums Act 2000. \n18\n \nTrusted custodian:​ Often, where a list is does not fall under any of these categories, \nits sources of authority are highly subjective, based upon how much you trust the \norganisation that has put together the list to have done a good job. This makes it \ndifficult to have a truly authoritative list. When looking for a list of companies to use, \nsome people might trust Open Corporates. Others might choose to trust Thomson \n19\nReuters or Dun and Bradstreet. \n2021\n \nNone of these categories are mutually exclusive, and there are occasions where a \nregister draws on multiple sources of authority. \nReality \nLists have different relationships to how we understand the world. While the \nclassroom attendance register describes which children were in the room, being on \nthe register of children in class 1B makes a child a member of class 1B. \n16\n Microsoft, ‘Search product lifecycle’, \nhttps://support.microsoft.com/en-gb/lifecycle/search?ts=-3 \n17\n Electoral Commission, ‘Where can I find a list of all political parties?’, \nhttp://www.electoralcommission.org.uk/faq/donations-to-political-parties/where-can-\ni-find-a-list-of-all-political-parties \n18\n Legislation.gov.uk (2018), ‘Political Parties, Elections and Referendums Act 2000’, \nhttps://www.legislation.gov.uk/ukpga/2000/41/contents \n19\n Opencorporates, ​http://opencorporates.com/ \n20\n Thomson Reuters (2018), ‘Company Data’, \nhttps://financial.thomsonreuters.com/en/products/data-analytics/company-data.html  \n21\n Dun and Bradstreet (2018), ‘Our data differentiators’, \nhttps://www.dnb.co.uk/about-us/our-data.html \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust12 \n\n \n \nWe refer to these types of lists as being reality describing or reality defining. \n \nReality defining:​ a list that somehow changes reality when an entry appears on it, \nlike the register of pupils in a class or the register of medical practitioners. This is \n22\ngenerally associated with the source of authority of the list. The source of authority is \nusually either institutional authority, or the legislation that mandates the existence of \nthe list. \n \nStrictly speaking, these lists do not define reality. They are generally ones that a large \nnumber of people have agreed to recognise as authoritative. Therefore, they are also \nassociated with rigorous controls and processes around adding, deleting or \nchanging entries. \n \nReality describing:​ a list that describes the world as observed, like a class register \nof attendance or the Wikidata list of countries in the European Union. \n23\n \nThere may also be hybrids of these two types, where authoritative, canonical \ninformation (reality defining) is supplemented by additional descriptive information \n(reality describing) in a list.  \nCompleteness \nA list may be either complete or incomplete. An incomplete list does not contain all \nthe entities that should be on it, like a list of pupils in a class that is missing some of \nthe children that are meant to be there. A list can also be incomplete because it is \nmissing information about each entity, like a list of contact details which contains a \nrow of names, but is missing the email and phone number of each person. \n \nThe register of pupils is complete because it contains all the pupils who are meant to \nbe in the class at that time. By contrast, the Wikipedia list of deaths in 1989 is \n24\nincomplete (at the time of writing) because it does not include everyone who died in \n1989 and the register of government cats is similarly incomplete as it only contains \n25\nthose cats that have been reported. \nAccuracy \nA list may be entirely accurate, have some known errors, or contain some elements \nwhere the provenance and accuracy are unknown. \n \nWe would expect a list of numbers between 1 and 100 to be entirely accurate \nbecause it is easy to compile and check. A list of every postal address in the country \nis likely to be inaccurate as new houses are built and other changes occur. \nInaccuracy is particularly an issue for reality describing lists. \n \nA list may also be accurate only at a certain granularity. For instance, Open Street \nMap will be less accurate than the Ordnance Survey MasterMap product, because \n2627\n22\n General Medical Council (2018), ‘Registration and licensing’, \nhttps://www.gmc-uk.org/doctors/medical_register.asp \n23\n Wikipedia (2018), ‘Member state of the European Union’, \nhttps://en.wikipedia.org/wiki/Member_state_of_the_European_Union \n24\n Wikipedia (2018), ‘1989’, ​https://en.wikipedia.org/wiki/1989#Deaths \n25\n Peter Wells (2016), ‘UK Government cats’, \nhttps://peterkwells.github.io/uk-government-cats/  \n26\n OpenStreetMap, ​https://www.openstreetmap.org \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust13 \n\n \nthe process of recording information is not performed to the same level of accuracy \nas that required of the surveyors that Ordnance Survey use. \n \nSome lists contain pieces of information which can never really be entirely accurate; \nthey are opinions rather than facts, as in the Shipping Register example \ndiscussed above. \n \n \nList Source of \nauthority \nReality Completeness Accuracy \nPupils in a \nclass \nInstitutional Defining Complete  Accurate \nOpen Street \nMap \nVerifiable \nObservations \nDescribing Incomplete Sometimes \naccurate \nGeneral \nMedical \nCouncil \nRegister \nLegislative Defining Complete Accurate \nFirst \nthousand \nprime \nnumbers \nA priori Describing Provably complete Provably \naccurate \n3. Stewardship \nStewarding is the process of maintaining a register. This includes how and when \ninformation is added, removed or changed and who does those things. \nUnderstanding how a custodian stewards a register allows people who are thinking \nabout using it to make informed decisions about how much they can trust it. \n \nWe explore the stewardship of a register by asking a series of questions. \nWho will maintain this register? \nWhich individual, group or institution is responsible for making sure it remains \naccurate and complete? \nHow is an entry added to the register? \nWhat is the process for adding an entry to the list? Who is involved? Will there be an \napproval step or can something be added directly? \nHow are errors in the register corrected? \nRegisters often contain errors, omissions or inaccuracies. This is particularly true of \nregisters which are reality describing, as they are susceptible to external changes \nwhich are not yet reflected in the register. When this happens, there needs to be a \nway to make changes to that register.  \n27\n Ordnance Survey (2018), ‘OS MasterMap’, \nhttps://www.ordnancesurvey.co.uk/business-and-government/products/mastermap-\nproducts.html \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust14 \n\n \nWho is paying to maintain this register? \nMaking sure a register remains accurate and complete over time requires work. This \noften requires funding to pay for staff or infrastructure. \n \nUnderstanding who pays for such work is important. Funders often maintain a \nregister for a particular purpose. Considering what these purposes are can help us \nunderstand how the register is governed. \nCommunicating stewarding \nThe benefits of register infrastructure come from the contents of the register being \nused by many people, systems or institutions. To achieve this, the register needs to \nbe seen as trustworthy and authoritative by those users. \n \nIf users cannot see the how the register is governed, compiled and quality-assured, \nthey cannot know if it is appropriate to reuse. Such assessments include whether it \nwill be accurate enough, whether the precise and detailed definition of what \nconstitutes an entry on the register is suitable for a specific purpose, as well other \nquestions based on the details of the particular subject matter. \n \nMost registers and lists that claim to be authoritative and suitable for reuse do not \nexplain the process of stewarding, or only partially explain it. While it is often possible \nto find out how the stewarding works, doing so can often require extensive research. \nThis increases the cost of reuse, which in turn probably reduces how often it \nis reused.  \n \nFor example, at the time of writing the Foreign and Commonwealth Office’s register \nof countries recognised by the UK does not give a clear picture of its stewarding. \n28\n \nThe register website simply lists the government department responsible for \nmaintaining it. This leaves many questions unanswered. The register provides no way \nof contacting the department to report errors or suggest changes to the register. It \nhas no information about how decisions have been made about which countries are \nincluded. The register is not simply “British English-language names and descriptive \nterms for countries” (the description given in the register itself); it contains countries \nformally recognised by the government of the United Kingdom. \n \n28\n Gov.uk, ‘Country register’, ​https://country.register.gov.uk  \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust15 \n\n \n \n \nWithout understanding how and why a country has an entry on the register, it is hard \nfor anyone considering using the register to decide whether or not it is an appropriate \nlist of countries. This in turn influences what you can use the register for. A service in \nthe UK recognising passports or visas could not use this country register as it does \nnot include Taiwan (the UK does not formally recognise Taiwan). However, Taiwan \nissues passports to citizens, and they are granted visas to travel to the UK. \n \nObscure stewarding is currently a feature of all government registers; there is no \npublicly available information about how such lists are maintained. \nCollaboration makes better registers \nRegisters are established for a purpose, but it is rare for even the main intended \nusers to be involved in the specific design of the register – let alone people who may \nnot be the intended users but who have a legitimate use for the information \nit contains. \n \nIn government, failing to consult or co-design with users to make the register as easy \nand relevant as possible adds to the costs of the nation and reduces its efficiency. \n \nIn commerce, the design of a register should follow the processes and procedures \nrecommended for other kinds of open standards for data, and for the same reasons\n29\n. \n \n29\n Open Data Institute (2018), ‘Exploring the development and impact of open \nstandards for data report’, \nhttps://docs.google.com/document/d/1Sab5YMVj4PVqLjZD35hX8FTnMeeP6gLGG0\nxszuRMIaM/edit#heading=h.zenpzz1aus85  \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust16 \n\n \nThis call for transparency and collaboration does not mean that registers should \naccept every suggestion made, or that they should attempt to meet the needs \nof everyone.  \n \nMeeting the needs of data users is not the same as meeting the needs of the users of \nthe services built using data.  \n \nFor registers designed as data infrastructure (due to the social, legal. environmental \nor economic value of their contents), explaining their governance processes is a \nminimum requirement.  \n \nThose processes should be both transparent and open. The criteria by which \ndecisions are made, and also individual decisions, should be public. Where such \ndecisions are potentially controversial, or subjective, someone who disagrees with \nthe contents of a register should at least be able to understand why the maintainer of \na register has made their decision. \nFunding registers \nUnderstanding who is paying for something can give valuable insights into the \ngovernance of the register. It also may help us to make judgements about things like \n \n●is it likely to be sustained in the long term? \n●is it likely to be unbiased? \n●are its operations likely to be well-funded? \n \nTraditionally, registers were often maintained by an organ of the state (although there \nare exceptions, such as Lloyd’s Register). This is related to practical aspects of \nrunning a state, such as taxation, and to the fact that the creating and maintaining \nregisters was both time-consuming and expensive. \n \nToday, there are three main ways that registers are funded: \n \n●State funding \n●Service revenue \n●Grants and donations \n \nRegisters are often funded through a mixture of these. \nState funding \nRegisters operated by the state are generally funded by taxation. This These registers \nmay be directly operated by a government body or by a third party the state enlists to \nmaintain the register. \nService revenue \nSometimes, institutions who are paid to maintain a register are also instructed to \ngenerate revenues from the register. This supports the maintenance of the register \nitself and sometimes provides additional revenue for the Treasury. There are three \ntypical models: charging for updates to the register, selling commercial services \nrelated to that data, or charging for access to the data within the register. These \nthree models have differing levels of tension between providing that data for free at \nthe point of use and revenue maximisation. \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust17 \n\n \nCase study: HM Land Registry \nHM Land Registry was established by the Land Registry Act 1862. Since then, it \n30\nhas registered ownership of land and property in England and Wales. Registration of \nland was initially voluntary, though compulsory registration was gradually introduced \nand extended over the next 140 years. \n \nThe registry took from 1862 to 1963 to collect two million entries. Since then, the \n31\nprocess has accelerated. The Land Registry now contains more than 24 million titles. \nThis acceleration has been driven by the increasing speed of processing introduced \nby digitisation of the registry. \n \nRegistration is only compulsory under certain circumstances, such as when property \nis sold or inherited. Therefore, the registry still does not cover the whole of England \nand Wales. As of 2017,  HM Land Registry claimed to have 85% coverage. \n32\n \nWhile HM Land Registry is a department of the government of the United Kingdom, \nthey do not receive public funding. Rather, they generate revenue by charging people \nwho want to change or query the register. In 2016, this revenue was more than \n£300M. This allows HM Land Registry to operate in the way that it does: backed by \nlegislative authority without needing to be funded by taxes. \n30\n Legislation.gov.uk, ‘Land Registry Act 1862’, \nhttp://www.legislation.gov.uk/ukpga/Vict/25-26/53 \n31\n Land Registry (2000), ‘A Short History of Land Registration in England and Wales’, \nhttp://webarchive.nationalarchives.gov.uk/20101213175739/http:/www.landreg.gov.u\nk/assets/library/documents/bhist-lr.pdf \n32\n Land Registry (2018), ‘About Us’, \nhttps://www.gov.uk/government/organisations/land-registry/about \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust18 \n\n \n \nGrants and donations \nAs the internet has made new modes of collaboration and coordination possible, \nincreasingly there are institutions capable of creating reasonably authoritative, \ncomplete registers without requiring state funding and complex organisational \noverheads. Non-governmental organisations (NGOs) and charities are seeking to \n3334\neffect change by maintaining digital registers that are in the public interest. That said, \nkeeping such registers accurate and complete still requires some organisation and, \noften, funding. They are often funded in the same way as other work that charities \nand NGOs perform, such as grant funding, donations and through volunteers \ndonating their time and skills.  \n \n33\n Eg Democracy Club, ‘Democracy Club Candidates’, \nhttps://candidates.democracyclub.org.uk/ \n34\n Eg MySociety, ‘TheyWorkForYou All MPs’, ​https://www.theyworkforyou.com/mps/ \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust19 \n\n \nWe even see instances where complete and accurate lists of things can be \nmaintained by groups of volunteers without paid-for central professional stewarding. \nA simple example is the list of US State birds on Wikipedia. This is particularly true \n35\nof lists which can be verified by observation and comprise a tightly constrained \nminimum viable dataset. For some such registers (often small to moderate in size) the \noverheads relating to publishing them online, such as server space, bandwidth and \nsimilar costs, are close to zero. \n  \n35\n Wikipedia (2018), ‘List of U.S. state birds’, \nhttps://en.wikipedia.org/wiki/List_of_U.S._state_birds \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust20 \n\n \nBuilding trust when \nstewarding \nTrust in registers is built by high-quality stewarding practices. While much of the \nactivity of a custodian comprises ensuring that the register is kept up to date and \naccurate, it is also the responsibility of the custodian to make the governance \nprocesses surrounding a register transparent and open. \n \nFor many registers, improving the transparency that surrounds these processes is \nthe simplest and cheapest way to increase trust in their contents. \n \nTransparent stewarding means being clear and open about the answers to the four \nkey governance questions outlined above: \n \n●Who maintains this register and why? \n●How are entries added to the register? \n●How are errors in the register corrected? \n●Who pays for the maintenance of this register? \n  \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust21 \n\n \nModels for collaboration \nWe have observed a number of different models for collaboration to make and \nmaintain registers. We have examined some of these models as case studies to \nshow how they work. \nCustodial discretion \nFor some registers, there is an obvious decision maker who unilaterally operates \nupon the contents of the register. They most commonly have either legislative \ncustodians or institutional authority. \n \nWhen Google decides to stop supporting the Mosaic web browser and removes it \nfrom their list of supported web browsers, this is a decision taken by a person or \ngroup that has been given executive power. This results in Mosaic being removed \nfrom the list of supported web browsers. \n \nIf you are not the custodian, your ability to collaborate on maintaining the register is \ndramatically reduced. There may be a clear mechanism to provide feedback, which \nprovides some opportunity to request additions or corrections, but this is not always \nthe case. Registers that operate a custodial discretion model may just as easily \noperate on a whim, or make decisions in a smoke-filled room with minimal \ntransparency of the precise process. \n \nWhile errors can be fixed or changes made by the right person in the organisation \nthat created the list, finding that person is often difficult. Formal error-reporting \nmechanisms may not exist. This model is poorly suited to reality-describing registers, \nwhich often need to change rapidly, as the custodian acts as a bottleneck for \nchanges. This in turn makes it hard for the register to be kept up to date; it also \nincreases the resources required because they cannot take advantage of other \npeople who might be able to observe reality. Depending on the level of transparency, \na lack of timeliness and an unclear process can diminish trust in the register itself.  \n \nCustodial discretion can be effective for reality-altering registers when the custodian \nis always involved in the change process. \nSuggested edits \nIn other cases, the custodian will make suggested alterations to the register when \nanother another party requests them, but the custodian verifies the accuracy of the \nalteration before making a change. This allows a broad collaboration with many \nparties on the contents of the register, while maintaining a narrow responsibility for \naccuracy. \n \nOne of the benefits of publishing registers in an online, linkable way is that it makes it \nfar easier to examine their contents for inaccuracies. When tightly coupled with error \nreporting or change suggestion tools, this way of publishing registers unlocks \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust22 \n\n \npotential for collaboration and error detection at a rate that was unavailable to \nregisters either on paper or held electronically without being linkable. \nFood Standards Agency Sampling Classification Register \nThe Food Standards Agency (FSA) is responsible for the Food Surveillance System. \n36\nThis is a national database of analytical results for food samples as part of official \ncontrols. Sampling is conducted by enforcement authorities, local authorities and \nport health authorities. \n \nThe sample programme requires that samples are rigorously classified in a consistent \nmanner across the country. This classification was previously managed as part of the \ndata capture software product. Any change to the classification involved a complex \ngathering of requirements and a change management process. Once this was \ncompleted, and a new release of the data capture software made, each of the \nindividual laboratories testing samples on behalf of local authorities would have to \nmanage and implement the relevant software update – an expensive and \ntime-consuming process. \n \nSince becoming a register on the FSA registry this food sampling classification has \n37\nbecome easier. The online register provides a place where any lab that conducts \nsampling, or indeed any party interested in food sampling, can download the full list. \nThe sampling classification has become a data product, instead of being a small \nfeature hidden in a broader product with unrelated change processes, priorities \nand incentives. \n \nThis shift has enabled the FSA to make engaging with laboratories to improve the \nsampling classification register much easier. They have taken direct ownership of the \nchange processes by which food sampling is classified, making it possible to change \nthings much more dynamically. \n \nThe publication technology makes the complete register available online. It also \nprovides a path for people to submit and track changes. As a result, the FSA has \nbeen able to gather improvements to the information from experts in the field, \nwithout having to pay someone to travel across the country and speak to them in \nperson. \n \nBy abstracting the data standards required in the surveillance system as their own \nregister, the FSA has decoupled data collection from classification. This gives \nsampling laboratories a choice of how to collect and submit this data. They can \ninnovate with their own data collection and audit processes without being tied to a \nsingle monopoly supplier. \nDirect access \nThe most permissive change model is providing tools that allow anyone to directly \nalter the register. These tools may be an online service (like a wiki), or paper forms \nthat anyone can fill in and send to the custodian responsible for compiling or \npublishing the register. \n \n36\n Food Standards Agency (2018), ‘UK Food Surveillance System’, \nhttps://www.food.gov.uk/enforcement/sampling/fss \n37\n Food Standards Agency (2018), ‘Register: Classifications’, \nhttps://data.food.gov.uk/codes/enforcement-monitoring/sampling/_classifications \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust23 \n\n \nWith a direct access model, there is no validation of the information provided. There \nmay, however, be validation that the information is within certain parameters. There \nmight not be any examination of the dates on birth certificates or passports, as long \nthese are dates that could be correct (for example, not in the future).  \n \nDirect access models have to provide some mechanism to deal with vandalism or \nfraudulent entries. They frequently include a proactive mechanism for reporting \ninaccuracies. Especially with official registers that carry some legal weight, there may \nbe penalties for deliberately entering misinformation. \n \nWhere registers using this model have been created in the recent past, they have \ntended be created and evolved alongside tools explicitly designed to facilitate these \ngovernance processes. This frequently includes some sort of version control system \nthat allows other users (sometimes only with special permissions) to roll back \nmalicious or accidental changes. The penalty for vandalism in such cases is often a \nban on a particular user. \n \nIn such cases, the governance model and the register are so closely linked that they \nare almost the same thing; Wikidata as a product is both an editing platform and a \ndata platform. \nRegistering a company in England and Wales \nCompanies House is the public body responsible for incorporating and dissolving \ncompanies. They register company information and make it available to the public. \n38\nThe Companies House register of companies is a reality-altering register with \nlegislative authority. \n \nIf a person wants to start a company, they are required to register that company. To \ndo so they must provide some information. For a private limited company, you have \nto submit the company name, address, directors and details of shareholders to \nCompanies House, using either an online service or by post with a paper form. \n3940\nSome of these details must comply with certain rules. For example, you cannot \n41\nchoose a company name that implies a connection with government or local \nauthorities without permission. \n \nHowever, Companies House do not perform any additional checks on the accuracy \nof the information that is being submitted. If a mistake is made on a form, Companies \nHouse will add the incorrect information to the register. At times this happens even \nwhen the information is known to be inaccurate at the time of submission: “Where a \nform is completed incorrectly we are not able to reject the form”. \n42\n \nAs it is an official register, the company directors have a duty to correct any errors or \ninconsistencies they are aware of. In some cases, the registrar may mark a record as \n“inconsistent”. In extreme cases, where no correction has been made, these can \nconstitute an offence and may be liable for a fine. \n38\n Companies House (2018), ‘About Us’, \nhttps://www.gov.uk/government/organisations/companies-house/about \n39\n Companies House (2018), ‘Web Filing’, \nhttps://ewf.companieshouse.gov.uk//seclogin?tc=1  \n40\n Gov.uk (2018), ‘Register a private or public company (IN01)’, \nhttps://www.gov.uk/government/publications/register-a-private-or-public-company-i\nn01 \n41\n Gov.uk (2018), ‘Set up a private limited company’, \nhttps://www.gov.uk/limited-company-formation/choose-company-name \n42\n Companies House (2015), ‘Inconsistencies on the public register’, \nhttps://www.gov.uk/government/publications/inconsistencies-on-the-public-register/i\nnconsistencies-on-the-public-register \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust24 \n\n \n \nCompanies House receives no taxpayer funds. It is mostly funded by charging fees \nfor company registrations. A smaller proportion of its income comes from charging \npeople who want to receive information from the register on paper or in bulk, and by \nrenting buildings they own to other parts of government. \n43\n \nAggregator registers \n \nA secondary register is a list created from other registers, for instance by aggregating \nother smaller registers. \n \nIn this model, many different custodians create separate registers, which are then be \nmerged into a single authoritative register. This often occurs where, due to either \nhistorical accident or design, there is strong institutional authority or legislative \ncustodian of lists of data that are a subset of a larger group of identical or very \nsimilar things. \n \nSeparate custodians are also found where there is a division of the register that \nmakes logical or administrative sense. In this manner, book publishers are allocated \nblocks of ISBN numbers to use for books they publish. \n44\n \n \n \nDue to the range of ways in which current custodians make registers they are \nresponsible for available, automating the transformation of primary registers into a \nsecondary register is not always possible.  \nUK Postal Addresses \nThe UK does not have a single authoritative list of geographic postal addresses. It \ndoes have a de facto ‘most-trusted’ list called AddressBase, which is one of a range \nof commercial products sold by Ordnance Survey, a government agency. The \nprocess of creating the address list is complex. There are many sources, each with \nslightly different reasons for contributing to it. As a result, it is difficult to talk about \naddresses as a single list. It is possible that it would be better understood as a set of \nsmaller lists that could be combined depending on use.  \n \nEach organisation’s section of the list comes with slightly different properties. For \nexample, the Royal Mail contributes postcodes. These are designed to help someone \n43\n Companies House (2016), ‘Annual Report and Accounts 2015/16’, \nhttps://www.gov.uk/government/uploads/system/uploads/attachment_data/file/5404\n43/AnnualReport_201516.pdf \n44\n Wikipedia (2018), ‘International Standard Book Number’, \nhttps://en.wikipedia.org/wiki/International_Standard_Book_Number#Registrant_elem\nent \n \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust25 \n\n \ndeliver letters to groups of buildings. Carrying a letter to a house has different \nrequirements than knowing the exact latitude and longitude of each property, or \nknowing whether the property is a hostel with multiple residents, or the exact name \nof each street. \n \nEach local authority contributes to the list via the Local Land and Property Gazetteer,\n which is in turn made by street naming and house numbering officers, and which \n45\nalso has other planning and tax functions. \n \nMost other elements of the list are reality describing. There are some exceptions like \npostcodes, which are assigned to groups of houses, thus altering reality. New \naddresses at the local authority level are a form of reality-altering register as they \ndefine what the street names and house numbers will be. They quickly become \nreality-describing registers as people rename houses, split them into flats and so on. \n \nAll these sublists, with each of their different governance and custodial patterns, are \naggregated by GeoPlace LLP. They are then turned into AddressBase, sold by \nOrdnance Survey (OS), and PAF, sold by Royal Mail. \n \nOS charges for access to AddressBase and has a form of error reporting governance \nmodel. However, because it is a secondary register, error reports might target the \nwrong institution, leaving Ordnance Survey or GeoPlace to relay error reports on to \nupstream data maintainers. \n \nThe list technology is machine-readable but not linkable. The data is only published \nas snapshots, roughly every six weeks. \n \nAs a reality-describing, secondary register that is very large with a lot of updates, \nAddressBase is prone to a lot of errors and will always be slightly out of date. It is \npossible that no list of this sort could ever be considered “complete”. Because of \nthis, easy and transparent error reporting is vital. \nCrowdsourcing \nCrowdsourcing is a model of sourcing information from a community.  Typically, the \ntotal information required has been broken down into small chunks. These can be \nsourced with relatively little effort and then the results are aggregated. This model \ntypically employs a strategy to prevent abuse or vandalism. One such strategy is \npost-moderation or version control combined with tooling like the ability to monitor \ncontent changes or being notified of edits, allowing the community to maintain \nquality. Another is to require multiple people to provide the same information – \nproviding a form of verified observation. \n \nAn editable Google Document is a simple tool for crowdsourcing, with fewer features \nfor community maintenance, while Wikipedia and OpenStreetMap are example of \nlarge scale, moderation-based crowdsourcing platforms.  \nDemocracy Club Candidates \nDemocracy Club Candidates is a database of candidates for elected office in \n46\nthe UK. \n45\n Wikipedia (2018), ‘Local Land and Property Gazetteer’, \nhttps://en.wikipedia.org/wiki/Local_Land_and_Property_Gazetteer \n46\n DemocracyClub (2018), ‘Candidates’, ​http://candidates.democracyclub.org.uk/ \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust26 \n\n \n \nFor elections covered in the past, the list of candidate names and parties are \nverifiably complete because each local authority publishes this data as an \nauthoritative, reality-describing, non-machine-readable, hard-to-discover list called a \nStatement Of Persons Nominated (SOPN).  \n \nDemocracy Club provides a data service on top of these siloed lists through both \ncombining them and making them machine-readable. Adding a person to Democracy \nClub’s list does not mean they are nominated, but being on a SOPN does. \n \nFor future elections Democracy Club maintains a sourced, non-canonical, patchy list \nof people they think will end up on the verifiably complete list. The number of \nattributes in this speculative list is greater than the official registers (for example, it \nincludes Twitter handles). \n \nEach field is crowdsourced and each edit requires a source. This means the \ndatabase is reproducible by anyone willing to invest the same effort. Particularly for \ninformation that is not sourced from official publications of formal notices from local \nauthorities, observations may not be verifiable after the fact. For instance, candidates \noften take down websites they operated for their campaign. \n \nThis means that the list of all candidates Democracy Club knows about contains \nmany different types of list with content that varies from verifiably complete, to \nrough assertions.  \n \nSome fields in the list use different patterns for contributions depending on the type \nof field and its importance when converted from a sourced, non-canonical, patchy list \nto a verifiably complete list. Photos are pre-moderated to avoid malicious, \nhard-to-automatically-select uploads. Once a candidate has been verified against a \nSOPN – by at least two people checking – you cannot edit the candidate’s name and \nparty. This “locking” is represented in the API and CSV downloads, which allows data \nconsumers to decide on the type of list they want. \norg-id.guide \nOrg-id.guide is a register where each entry is itself a list of organisations. It allows \n47\npeople to locate unique identifiers for organisations around the world by providing a \ncanonical form to refer to the many different sources of unique identifier for \norganisations that we find in the world. \n \nIt does not store unique identifiers itself as an aggregator. Instead, it provides an \nindex of external primary registers where such things may be found. It also provides \nadditional metadata to enable the use of open, interoperable, unambiguous \norganisation identifiers. \n \nOrg-id.guide provides a contributor’s handbook for people looking to suggest an \n48\naddition or change.  \n \nThis is an official way for people to either request or propose changes. The process \nis managed using the Github platform. Requests are raised as Github issues, and \nproposals as Github pull requests. There are clear guidelines for the details a request \nor proposal should include. The results of decisions made in reaction to requests or \nproposals are made in the open, and can be reviewed by anyone. Each decision has \n47\n Open Data Services Co-op, ‘Org-id.guide’, ​http://org-id.guide/ \n48\n Open Data Services Co-op, ‘Org-id.guide contributors handbook’, \nhttp://docs.org-id.guide/en/latest/contribute/ \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust27 \n\n \na URI. This also contains information about the discussions that contributed to any \ngiven decision, and the names of the people who eventually accepted or rejected a \nsuggested alteration. The decision-making process is also outlined, clarifying whose \nresponsibility it is to decide on a proposal, how to provide feedback on somebody \nelse’s proposal and the amount of time available to provide such feedback. \nMixed models \nIn practice, many registers operate a mixed governance model. This allows \nmaintainers to spend their time more effectively. They can focus on particularly \ncomplex or sensitive tasks or areas, while allowing other people to effectively \ncollaborate on maintaining the register. \n \nCompanies House allows essentially direct access to many operations, such as \ncreating a new company or recording a share issue. A form is submitted, but not \nchecked beyond minimal validation. However, when a company is struck off the \nregister, only the custodian can record that piece of information.  \n \nSimilarly, with crowdsourced lists such as Wikidata, the default is direct access to \ncreate, alter or remove entries, but certain pages are protected. This changes the \n49\ndefault behaviour, as only administrators can make changes to those pages. \n  \n49\n Wikidata (2018), ‘Wikidata:Page protection policy’, \nhttps://www.wikidata.org/wiki/Wikidata:Page_protection_policy \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust28 \n\n \nAppendix 1: report \nmethodology \nThis report has been written by David Miller and Sym Roe in partnership with the \nODI.  \n \nIt draws on their experience of open data and registers, desk research into current \nand historical practices around registers, and interviews with people who self-identify \nas being interested in registers and open data. \n \nThe interviews were semi-structured, with questions designed to stimulate \nconversation. Interviewees were asked questions such as “can you define a \nregister?”, “can you give an example of a list that isn’t a register?” and “what would \nyou change if you had a magic wand?”  \n \nInterviewees were recruited through David, Sym and the ODI’s existing network of \nexperts. Interviews included representatives from Epimorphics, Food Standards \nAgency, Government Digital Service, Office of National Statistics, Ordnance Survey, \nOpen Data Services, Thomson Reuters and Wikidata. \n \nTime constraints meant that no wider recruitment or quantitative research was done. \nThis report should be considered a starting point for debate rather than evidence of \nany point or argument. \n \nAll internet links in footnotes were active at the time of publication. Should the reader \nfind that links are no longer active we would recommend consulting the \nInternet Archive. \n50\n \n50\n Internet Archive, ​https://archive.org \nOpen Data Institute 2018 Registers and collaboration: making lists we can trust29 ","version":"1.10.100"}